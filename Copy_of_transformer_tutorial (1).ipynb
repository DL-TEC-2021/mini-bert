{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "Copy_of_transformer_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "616f1eabadab4a8c9e6d4216c5979495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b5bf127ff56244b690f74931b32bf117",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3b32ef67da6c47fc80ca0b2c816ad36d",
              "IPY_MODEL_41e4bc41bcf44407b0b7dbc7504c3d18",
              "IPY_MODEL_d58b5ec0ab2a48abb982682d381b8d2c"
            ]
          }
        },
        "b5bf127ff56244b690f74931b32bf117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b32ef67da6c47fc80ca0b2c816ad36d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e015b79872774622a1434d6ddb7b4f29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc36558937e846f8b32763791cb0d0f3"
          }
        },
        "41e4bc41bcf44407b0b7dbc7504c3d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e2b85d31b08a437d8782cf529b9bb63b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24b313a614c74a6fbce9a608216cb36c"
          }
        },
        "d58b5ec0ab2a48abb982682d381b8d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_abd31eccfc2644b88a70f0512ddf5ee7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 226k/226k [00:00&lt;00:00, 659kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04d0b459b043416f952728664da38d2f"
          }
        },
        "e015b79872774622a1434d6ddb7b4f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc36558937e846f8b32763791cb0d0f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2b85d31b08a437d8782cf529b9bb63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24b313a614c74a6fbce9a608216cb36c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "abd31eccfc2644b88a70f0512ddf5ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04d0b459b043416f952728664da38d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3eecebe612774989acef1e9c171aa0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5484de01ca624125915af46f0990668e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bf896a18c8784ed881e1425768f2bfb7",
              "IPY_MODEL_602e3469104e4da1af9257ca9a44e736",
              "IPY_MODEL_8bfa6236596c40859ae2cf5d62b5322b"
            ]
          }
        },
        "5484de01ca624125915af46f0990668e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf896a18c8784ed881e1425768f2bfb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_37db539f1aaf4e50826c050b3f15d24a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad4a469e825f40b9bcc6eb99eec98053"
          }
        },
        "602e3469104e4da1af9257ca9a44e736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_255b9a25e745475bb6d4c9188e867dc0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_146a492160c04ad182b0eb7241d5bb93"
          }
        },
        "8bfa6236596c40859ae2cf5d62b5322b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_05863db73ed04635af01a4767b1e5c4a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.13kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28e9eacbc0bd43198c68558414ca6d56"
          }
        },
        "37db539f1aaf4e50826c050b3f15d24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad4a469e825f40b9bcc6eb99eec98053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "255b9a25e745475bb6d4c9188e867dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "146a492160c04ad182b0eb7241d5bb93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05863db73ed04635af01a4767b1e5c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28e9eacbc0bd43198c68558414ca6d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f8eb1baf2c44800b4fd0945d276d628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f497ce42790400bb4fc2ab0e9f90d0e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e86da445ef9d4968a0c646c79a6b09e7",
              "IPY_MODEL_317885ebe433471e8292e1c029fa9d84",
              "IPY_MODEL_822fbc52c33c420c9e23d2b77f6d849f"
            ]
          }
        },
        "0f497ce42790400bb4fc2ab0e9f90d0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e86da445ef9d4968a0c646c79a6b09e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_15beffc1233f45f8940a3dbabb0d21df",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a3758ab38c9e44c7b65c8b38ffbdc42c"
          }
        },
        "317885ebe433471e8292e1c029fa9d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1844cd9068194696ac93e7d7d9d7807d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b4fc676dc724f4b90d030acb417413d"
          }
        },
        "822fbc52c33c420c9e23d2b77f6d849f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d2e98c37f324b67a8bc7cde2aeef7ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 455k/455k [00:00&lt;00:00, 1.05MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af074ee38901464388e24a306c87bc4d"
          }
        },
        "15beffc1233f45f8940a3dbabb0d21df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a3758ab38c9e44c7b65c8b38ffbdc42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1844cd9068194696ac93e7d7d9d7807d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b4fc676dc724f4b90d030acb417413d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d2e98c37f324b67a8bc7cde2aeef7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af074ee38901464388e24a306c87bc4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad290de7de6645bca4353a50a63d8345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_477636ae92054402a67b94681db95cfc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e432fd175a07481b908a3a557453adb9",
              "IPY_MODEL_8863ccfc09ad45d89e8a579dc8eb1047",
              "IPY_MODEL_1e1f7e247a6a41f28bd1ae85f42196cb"
            ]
          }
        },
        "477636ae92054402a67b94681db95cfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e432fd175a07481b908a3a557453adb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2b0039494b4045bbb6dae36b06d8942c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17a3876387854618b688ee17e0258012"
          }
        },
        "8863ccfc09ad45d89e8a579dc8eb1047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_965d515c9701441f8a58a428ff8bcd03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b6a383133754651bc2d7447f6e63e20"
          }
        },
        "1e1f7e247a6a41f28bd1ae85f42196cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f19d06ee3d0f4f85a5a24bac5f091f89",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 24.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9603dfa4d7684384a5550d6264fd37a8"
          }
        },
        "2b0039494b4045bbb6dae36b06d8942c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17a3876387854618b688ee17e0258012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "965d515c9701441f8a58a428ff8bcd03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b6a383133754651bc2d7447f6e63e20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f19d06ee3d0f4f85a5a24bac5f091f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9603dfa4d7684384a5550d6264fd37a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQiqyuRreYAo"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L-F_yd1eYAu"
      },
      "source": [
        "\n",
        "Language Modeling with nn.Transformer and TorchText\n",
        "===============================================================\n",
        "\n",
        "This is a tutorial on training a sequence-to-sequence model that uses the\n",
        "`nn.Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ module.\n",
        "\n",
        "The PyTorch 1.2 release includes a standard transformer module based on the\n",
        "paper `Attention is All You Need <https://arxiv.org/pdf/1706.03762.pdf>`__.\n",
        "Compared to Recurrent Neural Networks (RNNs), the transformer model has proven\n",
        "to be superior in quality for many sequence-to-sequence tasks while being more\n",
        "parallelizable. The ``nn.Transformer`` module relies entirely on an attention\n",
        "mechanism (implemented as\n",
        "`nn.MultiheadAttention <https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html>`__)\n",
        "to draw global dependencies between input and output. The ``nn.Transformer``\n",
        "module is highly modularized such that a single component (e.g.,\n",
        "`nn.TransformerEncoder <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html>`__)\n",
        "can be easily adapted/composed.\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_architecture.jpg?raw=1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUmUhzS5eYAw"
      },
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsI6UPtkeYAx"
      },
      "source": [
        "In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "`nn.TransformerEncoderLayer <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html>`__.\n",
        "Along with the input sequence, a square attention mask is required because the\n",
        "self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend\n",
        "the earlier positions in the sequence. For the language modeling task, any\n",
        "tokens on the future positions should be masked. To produce a probability\n",
        "distribution over output words, the output of the ``nn.TransformerEncoder``\n",
        "model is passed through a linear layer followed by a log-softmax function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMyU29XqeYAy"
      },
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBt3X-LIeYA0"
      },
      "source": [
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtjoxHrPeYA1"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tq0YVV1eYA2"
      },
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eae3A9-8eYA3"
      },
      "source": [
        "This tutorial uses ``torchtext`` to generate Wikitext-2 dataset. The\n",
        "vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Wikitext-2 represents rare tokens as `<unk>`.\n",
        "\n",
        "Given a 1-D vector of sequential data, ``batchify()`` arranges the data\n",
        "into ``batch_size`` columns. If the data does not divide evenly into\n",
        "``batch_size`` columns, then the data is trimmed to fit. For instance, with\n",
        "the alphabet as the data (total length of 26) and ``batch_size=4``, we would\n",
        "divide the alphabet into 4 sequences of length 6:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "Batching enables more parallelizable processing. However, batching means that\n",
        "the model treats each column independently; for example, the dependence of\n",
        "``G`` and ``F`` can not be learned in the example above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8PvFZY0MyE-",
        "outputId": "fb3dfab0-d1a6-49f8-bc33-d581880ab4d7"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 63.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_VsQVkqhrxn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "616f1eabadab4a8c9e6d4216c5979495",
            "b5bf127ff56244b690f74931b32bf117",
            "3b32ef67da6c47fc80ca0b2c816ad36d",
            "41e4bc41bcf44407b0b7dbc7504c3d18",
            "d58b5ec0ab2a48abb982682d381b8d2c",
            "e015b79872774622a1434d6ddb7b4f29",
            "bc36558937e846f8b32763791cb0d0f3",
            "e2b85d31b08a437d8782cf529b9bb63b",
            "24b313a614c74a6fbce9a608216cb36c",
            "abd31eccfc2644b88a70f0512ddf5ee7",
            "04d0b459b043416f952728664da38d2f",
            "3eecebe612774989acef1e9c171aa0b5",
            "5484de01ca624125915af46f0990668e",
            "bf896a18c8784ed881e1425768f2bfb7",
            "602e3469104e4da1af9257ca9a44e736",
            "8bfa6236596c40859ae2cf5d62b5322b",
            "37db539f1aaf4e50826c050b3f15d24a",
            "ad4a469e825f40b9bcc6eb99eec98053",
            "255b9a25e745475bb6d4c9188e867dc0",
            "146a492160c04ad182b0eb7241d5bb93",
            "05863db73ed04635af01a4767b1e5c4a",
            "28e9eacbc0bd43198c68558414ca6d56",
            "0f8eb1baf2c44800b4fd0945d276d628",
            "0f497ce42790400bb4fc2ab0e9f90d0e",
            "e86da445ef9d4968a0c646c79a6b09e7",
            "317885ebe433471e8292e1c029fa9d84",
            "822fbc52c33c420c9e23d2b77f6d849f",
            "15beffc1233f45f8940a3dbabb0d21df",
            "a3758ab38c9e44c7b65c8b38ffbdc42c",
            "1844cd9068194696ac93e7d7d9d7807d",
            "0b4fc676dc724f4b90d030acb417413d",
            "6d2e98c37f324b67a8bc7cde2aeef7ce",
            "af074ee38901464388e24a306c87bc4d",
            "ad290de7de6645bca4353a50a63d8345",
            "477636ae92054402a67b94681db95cfc",
            "e432fd175a07481b908a3a557453adb9",
            "8863ccfc09ad45d89e8a579dc8eb1047",
            "1e1f7e247a6a41f28bd1ae85f42196cb",
            "2b0039494b4045bbb6dae36b06d8942c",
            "17a3876387854618b688ee17e0258012",
            "965d515c9701441f8a58a428ff8bcd03",
            "9b6a383133754651bc2d7447f6e63e20",
            "f19d06ee3d0f4f85a5a24bac5f091f89",
            "9603dfa4d7684384a5550d6264fd37a8"
          ]
        },
        "outputId": "12b9174e-1a76-4e3c-b171-23b12eb6f4c8"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#13649397 2924871 2924871\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\"\"\"\n",
        "class SentenceIterator:\n",
        "\n",
        "    def __init__(self, file_path, batch_size, begin_index, total_lines):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.total_lines = total_lines\n",
        "        self.begin_index = begin_index\n",
        "        self.offset = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.n = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.n <= self.total_lines:\n",
        "            line_return = []\n",
        "            \n",
        "            #with open(self.file_path, \"r\") as f:\n",
        "            #  for i, line in enumerate(f):\n",
        "            #    if len(line_return) == self.batch_size:\n",
        "            #      break\n",
        "            #    elif i >= self.begin_index + self.n and i < self.begin_index + self.n + self.batch_size:\n",
        "            #      line_return.append(line.replace('\\n',''))\n",
        "            #  self.n += self.batch_size\n",
        "            #return tokenizer(\n",
        "            #          line_return,\n",
        "            #          padding=True, \n",
        "            #          truncation=True,\n",
        "            #          add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "            #         return_token_type_ids=False,\n",
        "            #          return_attention_mask=False,\n",
        "            #          return_tensors='pt',  # Return PyTorch tensors\n",
        "            #        )[\"input_ids\"]\n",
        "            \n",
        "            with open(self.file_path, \"r\") as f:\n",
        "              f.seek(self.offset)\n",
        "              for i in range(self.begin_index + self.n, self.begin_index + self.n + self.batch_size):\n",
        "                line = f.readline()\n",
        "                print(str(i),\"---\",line,\"---\",str(self.offset))\n",
        "                line_without_newline = line.replace('\\n','')\n",
        "                line_return.append(line_without_newline)\n",
        "                difference = len(line) - len(line_without_newline)\n",
        "                self.offset += len(line) + difference\n",
        "              self.n += self.batch_size\n",
        "            return tokenizer(\n",
        "                      line_return,\n",
        "                      padding=True, \n",
        "                      truncation=True,\n",
        "                      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "                      return_token_type_ids=False,\n",
        "                      return_attention_mask=False,\n",
        "                      return_tensors='pt',  # Return PyTorch tensors\n",
        "                    )[\"input_ids\"]\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "train_iterable = SentenceIterator(file_path = \"/content/drive/MyDrive/Project2/wikipedia16.txt\", batch_size=35, begin_index=0, total_lines=13649397)\n",
        "train_iterator = iter(train_iterable)\n",
        "\"\"\"\n",
        "\n",
        "class SentenceIterator:\n",
        "\n",
        "    def __init__(self, dataset, batch_size, device):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.total_lines = len(dataset)\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.n = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.n <= self.total_lines:\n",
        "            batch = tokenizer(\n",
        "                      self.dataset[self.n : self.n + self.batch_size],\n",
        "                      padding=True, \n",
        "                      truncation=True,\n",
        "                      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "                      return_token_type_ids=False,\n",
        "                      return_attention_mask=False,\n",
        "                      return_tensors='pt',  # Return PyTorch tensors\n",
        "                    )[\"input_ids\"].to('cpu')\n",
        "            self.n += self.batch_size\n",
        "            return torch.tensor([np.concatenate((item[0:np.where(item == 102)[0][0]-1],item[np.where(item == 102)[0][0]:len(item)]),axis=0) for item in batch.numpy()], dtype=torch.long).to(self.device), torch.tensor([np.concatenate(([item[0]],item[2:len(item)]),axis=0) for item in batch.numpy()], dtype=torch.long).reshape(-1).to(self.device)\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "\n",
        "def get_data(file_path, train_size):\n",
        "  with open(file_path, \"r\") as f:\n",
        "    sentences = f.readlines()\n",
        "    random.shuffle(sentences)\n",
        "    len_sentences = len(sentences)\n",
        "    len_train_sentences = int(len_sentences * train_size)\n",
        "    len_val_sentences = int((len_sentences - len_train_sentences) // 2)\n",
        "    train_sentences = sentences[0:len_train_sentences]\n",
        "    val_sentences = sentences[len_train_sentences:len_train_sentences + len_val_sentences]\n",
        "    test_sentences = sentences[len_train_sentences + len_val_sentences:len_sentences]\n",
        "    del sentences\n",
        "    return train_sentences, val_sentences, test_sentences\n",
        "\n",
        "train_data, val_data, test_data = get_data(file_path = \"/content/drive/MyDrive/Project2/wikipedia16-large.txt\", train_size = 0.7)\n",
        "\n",
        "#train_iterable = SentenceIterator(dataset = train_data, batch_size=35, device = device)\n",
        "#train_iterator = iter(train_iterable)\n",
        "\n",
        "#batch_input, batch_target = next(train_iterator)\n",
        "#print(batch_input.shape, batch_target.shape)\n",
        "#for batch_input, batch_target in train_iterator:\n",
        "#  print(batch_input.shape, batch_target.shape)\n",
        "#  break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "616f1eabadab4a8c9e6d4216c5979495",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eecebe612774989acef1e9c171aa0b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f8eb1baf2c44800b4fd0945d276d628",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad290de7de6645bca4353a50a63d8345",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K5ev6eweYA5"
      },
      "source": [
        "Functions to generate input and target sequence\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGOMtFTgeYA5"
      },
      "source": [
        "``get_batch()`` generates a pair of input-target sequences for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "we’d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_input_target.png?raw=1)\n",
        "\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent\n",
        "with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "``N`` is along dimension 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_GKaFV3eYA6"
      },
      "source": [
        "#This is not necessary\n",
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsULpXcVeYA7"
      },
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKTx899PeYA7"
      },
      "source": [
        "The model hyperparameters are defined below. The vocab size is\n",
        "equal to the length of the vocab object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x7fWe59eYA8"
      },
      "source": [
        "ntokens = 30592  # size of vocabulary in BERT\n",
        "batch_size = 35\n",
        "emsize = 768  # embedding dimension\n",
        "d_hid = emsize * 4  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm_vjbzZeYA8"
      },
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCO1inZVeYA9"
      },
      "source": [
        "We use `CrossEntropyLoss <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__\n",
        "with the `SGD <https://pytorch.org/docs/stable/generated/torch.optim.SGD.html>`__\n",
        "(stochastic gradient descent) optimizer. The learning rate is initially set to\n",
        "5.0 and follows a `StepLR <https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html>`__\n",
        "schedule. During training, we use `nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html>`__\n",
        "to prevent gradients from exploding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nmbKzROeYA9"
      },
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(batch_size).to(device)\n",
        "    num_batches = len(train_data) // batch_size\n",
        "    batch = 0\n",
        "    train_iterable = SentenceIterator(dataset = train_data, batch_size=batch_size, device = device)\n",
        "    train_iterator = iter(train_iterable)\n",
        "    for batch_input, batch_target in train_iterator:\n",
        "        current_batch_size = batch_input.size(0)\n",
        "        if batch_size != current_batch_size:  # only on last batch\n",
        "            src_mask = src_mask[:current_batch_size, :current_batch_size]\n",
        "        output = model(batch_input, src_mask)\n",
        "        #print(output.shape, batch_input.shape, batch_target.shape)\n",
        "        loss = criterion(output.view(-1, ntokens), batch_target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if batch % 2000 == 0 and batch > 0:\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Project2/transformer3.pt\")\n",
        "        batch += 1\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(batch_size).to(device)\n",
        "    with torch.no_grad():\n",
        "        val_iterable = SentenceIterator(dataset = eval_data, batch_size=batch_size, device = device)\n",
        "        val_iterator = iter(val_iterable)\n",
        "        for batch_input, batch_target in val_iterator:\n",
        "            current_batch_size = batch_input.size(0)\n",
        "            if batch_size != current_batch_size:  # only on last batch\n",
        "                src_mask = src_mask[:current_batch_size, :current_batch_size]\n",
        "            output = model(batch_input, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, batch_target).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLKi4r4teYA-"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rZPQ147eYA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7d5999-a45c-4fd1-cb8d-bc44b9ead81d"
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 1\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/389982 batches | lr 5.00 | ms/batch 70.34 | loss  7.29 | ppl  1469.73\n",
            "| epoch   1 |   400/389982 batches | lr 5.00 | ms/batch 68.41 | loss  4.63 | ppl   102.02\n",
            "| epoch   1 |   600/389982 batches | lr 5.00 | ms/batch 68.91 | loss  4.35 | ppl    77.35\n",
            "| epoch   1 |   800/389982 batches | lr 5.00 | ms/batch 68.05 | loss  4.16 | ppl    63.80\n",
            "| epoch   1 |  1000/389982 batches | lr 5.00 | ms/batch 67.44 | loss  4.12 | ppl    61.44\n",
            "| epoch   1 |  1200/389982 batches | lr 5.00 | ms/batch 67.66 | loss  4.09 | ppl    59.86\n",
            "| epoch   1 |  1400/389982 batches | lr 5.00 | ms/batch 68.89 | loss  3.96 | ppl    52.31\n",
            "| epoch   1 |  1600/389982 batches | lr 5.00 | ms/batch 68.23 | loss  3.92 | ppl    50.41\n",
            "| epoch   1 |  1800/389982 batches | lr 5.00 | ms/batch 67.80 | loss  3.96 | ppl    52.64\n",
            "| epoch   1 |  2000/389982 batches | lr 5.00 | ms/batch 67.43 | loss  3.95 | ppl    51.96\n",
            "| epoch   1 |  2200/389982 batches | lr 5.00 | ms/batch 73.03 | loss  3.84 | ppl    46.41\n",
            "| epoch   1 |  2400/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.89 | ppl    49.07\n",
            "| epoch   1 |  2600/389982 batches | lr 5.00 | ms/batch 67.88 | loss  3.85 | ppl    46.88\n",
            "| epoch   1 |  2800/389982 batches | lr 5.00 | ms/batch 69.15 | loss  3.77 | ppl    43.44\n",
            "| epoch   1 |  3000/389982 batches | lr 5.00 | ms/batch 66.74 | loss  3.93 | ppl    50.88\n",
            "| epoch   1 |  3200/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.86 | ppl    47.69\n",
            "| epoch   1 |  3400/389982 batches | lr 5.00 | ms/batch 69.76 | loss  3.76 | ppl    42.75\n",
            "| epoch   1 |  3600/389982 batches | lr 5.00 | ms/batch 66.76 | loss  3.85 | ppl    46.94\n",
            "| epoch   1 |  3800/389982 batches | lr 5.00 | ms/batch 68.08 | loss  3.78 | ppl    43.95\n",
            "| epoch   1 |  4000/389982 batches | lr 5.00 | ms/batch 67.54 | loss  4.02 | ppl    55.62\n",
            "| epoch   1 |  4200/389982 batches | lr 5.00 | ms/batch 72.80 | loss  3.76 | ppl    43.14\n",
            "| epoch   1 |  4400/389982 batches | lr 5.00 | ms/batch 68.33 | loss  3.76 | ppl    42.84\n",
            "| epoch   1 |  4600/389982 batches | lr 5.00 | ms/batch 66.36 | loss  3.86 | ppl    47.28\n",
            "| epoch   1 |  4800/389982 batches | lr 5.00 | ms/batch 68.24 | loss  3.76 | ppl    43.08\n",
            "| epoch   1 |  5000/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.78 | ppl    43.70\n",
            "| epoch   1 |  5200/389982 batches | lr 5.00 | ms/batch 68.72 | loss  3.72 | ppl    41.39\n",
            "| epoch   1 |  5400/389982 batches | lr 5.00 | ms/batch 68.77 | loss  3.71 | ppl    41.03\n",
            "| epoch   1 |  5600/389982 batches | lr 5.00 | ms/batch 68.34 | loss  3.83 | ppl    46.02\n",
            "| epoch   1 |  5800/389982 batches | lr 5.00 | ms/batch 69.09 | loss  3.75 | ppl    42.41\n",
            "| epoch   1 |  6000/389982 batches | lr 5.00 | ms/batch 68.77 | loss  3.71 | ppl    41.04\n",
            "| epoch   1 |  6200/389982 batches | lr 5.00 | ms/batch 75.06 | loss  3.81 | ppl    45.23\n",
            "| epoch   1 |  6400/389982 batches | lr 5.00 | ms/batch 67.79 | loss  3.77 | ppl    43.24\n",
            "| epoch   1 |  6600/389982 batches | lr 5.00 | ms/batch 66.55 | loss  3.85 | ppl    46.90\n",
            "| epoch   1 |  6800/389982 batches | lr 5.00 | ms/batch 68.39 | loss  3.68 | ppl    39.66\n",
            "| epoch   1 |  7000/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.72 | ppl    41.36\n",
            "| epoch   1 |  7200/389982 batches | lr 5.00 | ms/batch 66.64 | loss  3.82 | ppl    45.53\n",
            "| epoch   1 |  7400/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.72 | ppl    41.14\n",
            "| epoch   1 |  7600/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.69 | ppl    40.07\n",
            "| epoch   1 |  7800/389982 batches | lr 5.00 | ms/batch 67.75 | loss  3.75 | ppl    42.46\n",
            "| epoch   1 |  8000/389982 batches | lr 5.00 | ms/batch 67.12 | loss  3.74 | ppl    42.13\n",
            "| epoch   1 |  8200/389982 batches | lr 5.00 | ms/batch 73.52 | loss  3.69 | ppl    39.88\n",
            "| epoch   1 |  8400/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.77 | ppl    43.59\n",
            "| epoch   1 |  8600/389982 batches | lr 5.00 | ms/batch 68.60 | loss  3.69 | ppl    39.88\n",
            "| epoch   1 |  8800/389982 batches | lr 5.00 | ms/batch 68.12 | loss  3.69 | ppl    40.10\n",
            "| epoch   1 |  9000/389982 batches | lr 5.00 | ms/batch 68.88 | loss  3.64 | ppl    38.15\n",
            "| epoch   1 |  9200/389982 batches | lr 5.00 | ms/batch 67.76 | loss  3.87 | ppl    47.86\n",
            "| epoch   1 |  9400/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.86 | ppl    47.56\n",
            "| epoch   1 |  9600/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.86 | ppl    47.27\n",
            "| epoch   1 |  9800/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.74 | ppl    42.26\n",
            "| epoch   1 | 10000/389982 batches | lr 5.00 | ms/batch 68.25 | loss  3.68 | ppl    39.63\n",
            "| epoch   1 | 10200/389982 batches | lr 5.00 | ms/batch 73.14 | loss  3.67 | ppl    39.37\n",
            "| epoch   1 | 10400/389982 batches | lr 5.00 | ms/batch 68.33 | loss  4.01 | ppl    55.33\n",
            "| epoch   1 | 10600/389982 batches | lr 5.00 | ms/batch 68.64 | loss  3.69 | ppl    40.00\n",
            "| epoch   1 | 10800/389982 batches | lr 5.00 | ms/batch 67.84 | loss  3.67 | ppl    39.42\n",
            "| epoch   1 | 11000/389982 batches | lr 5.00 | ms/batch 68.76 | loss  3.67 | ppl    39.18\n",
            "| epoch   1 | 11200/389982 batches | lr 5.00 | ms/batch 68.65 | loss  3.67 | ppl    39.07\n",
            "| epoch   1 | 11400/389982 batches | lr 5.00 | ms/batch 68.82 | loss  3.62 | ppl    37.46\n",
            "| epoch   1 | 11600/389982 batches | lr 5.00 | ms/batch 68.17 | loss  3.66 | ppl    38.68\n",
            "| epoch   1 | 11800/389982 batches | lr 5.00 | ms/batch 66.76 | loss  3.72 | ppl    41.40\n",
            "| epoch   1 | 12000/389982 batches | lr 5.00 | ms/batch 67.59 | loss  3.70 | ppl    40.62\n",
            "| epoch   1 | 12200/389982 batches | lr 5.00 | ms/batch 73.55 | loss  3.71 | ppl    40.88\n",
            "| epoch   1 | 12400/389982 batches | lr 5.00 | ms/batch 66.06 | loss  3.77 | ppl    43.21\n",
            "| epoch   1 | 12600/389982 batches | lr 5.00 | ms/batch 66.92 | loss  3.75 | ppl    42.36\n",
            "| epoch   1 | 12800/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.78 | ppl    43.86\n",
            "| epoch   1 | 13000/389982 batches | lr 5.00 | ms/batch 67.11 | loss  3.73 | ppl    41.83\n",
            "| epoch   1 | 13200/389982 batches | lr 5.00 | ms/batch 66.55 | loss  3.71 | ppl    40.94\n",
            "| epoch   1 | 13400/389982 batches | lr 5.00 | ms/batch 69.03 | loss  3.72 | ppl    41.41\n",
            "| epoch   1 | 13600/389982 batches | lr 5.00 | ms/batch 68.63 | loss  3.67 | ppl    39.39\n",
            "| epoch   1 | 13800/389982 batches | lr 5.00 | ms/batch 68.34 | loss  3.66 | ppl    38.75\n",
            "| epoch   1 | 14000/389982 batches | lr 5.00 | ms/batch 68.60 | loss  3.67 | ppl    39.08\n",
            "| epoch   1 | 14200/389982 batches | lr 5.00 | ms/batch 78.40 | loss  3.65 | ppl    38.44\n",
            "| epoch   1 | 14400/389982 batches | lr 5.00 | ms/batch 67.85 | loss  3.63 | ppl    37.55\n",
            "| epoch   1 | 14600/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.85 | ppl    46.95\n",
            "| epoch   1 | 14800/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.67 | ppl    39.18\n",
            "| epoch   1 | 15000/389982 batches | lr 5.00 | ms/batch 67.62 | loss  3.64 | ppl    37.94\n",
            "| epoch   1 | 15200/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.60 | ppl    36.72\n",
            "| epoch   1 | 15400/389982 batches | lr 5.00 | ms/batch 67.17 | loss  3.73 | ppl    41.84\n",
            "| epoch   1 | 15600/389982 batches | lr 5.00 | ms/batch 66.19 | loss  3.71 | ppl    40.97\n",
            "| epoch   1 | 15800/389982 batches | lr 5.00 | ms/batch 68.42 | loss  3.61 | ppl    37.02\n",
            "| epoch   1 | 16000/389982 batches | lr 5.00 | ms/batch 67.05 | loss  3.70 | ppl    40.53\n",
            "| epoch   1 | 16200/389982 batches | lr 5.00 | ms/batch 72.30 | loss  3.64 | ppl    38.00\n",
            "| epoch   1 | 16400/389982 batches | lr 5.00 | ms/batch 68.11 | loss  3.62 | ppl    37.38\n",
            "| epoch   1 | 16600/389982 batches | lr 5.00 | ms/batch 66.95 | loss  3.66 | ppl    39.05\n",
            "| epoch   1 | 16800/389982 batches | lr 5.00 | ms/batch 67.67 | loss  3.68 | ppl    39.67\n",
            "| epoch   1 | 17000/389982 batches | lr 5.00 | ms/batch 67.05 | loss  3.74 | ppl    42.06\n",
            "| epoch   1 | 17200/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.70 | ppl    40.37\n",
            "| epoch   1 | 17400/389982 batches | lr 5.00 | ms/batch 68.76 | loss  3.65 | ppl    38.61\n",
            "| epoch   1 | 17600/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.62 | ppl    37.22\n",
            "| epoch   1 | 17800/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.69 | ppl    39.97\n",
            "| epoch   1 | 18000/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.62 | ppl    37.32\n",
            "| epoch   1 | 18200/389982 batches | lr 5.00 | ms/batch 72.26 | loss  3.64 | ppl    37.97\n",
            "| epoch   1 | 18400/389982 batches | lr 5.00 | ms/batch 67.76 | loss  3.72 | ppl    41.43\n",
            "| epoch   1 | 18600/389982 batches | lr 5.00 | ms/batch 68.68 | loss  3.57 | ppl    35.52\n",
            "| epoch   1 | 18800/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.67 | ppl    39.06\n",
            "| epoch   1 | 19000/389982 batches | lr 5.00 | ms/batch 68.21 | loss  3.67 | ppl    39.39\n",
            "| epoch   1 | 19200/389982 batches | lr 5.00 | ms/batch 69.08 | loss  3.59 | ppl    36.23\n",
            "| epoch   1 | 19400/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.62 | ppl    37.41\n",
            "| epoch   1 | 19600/389982 batches | lr 5.00 | ms/batch 68.04 | loss  3.63 | ppl    37.83\n",
            "| epoch   1 | 19800/389982 batches | lr 5.00 | ms/batch 68.24 | loss  3.60 | ppl    36.63\n",
            "| epoch   1 | 20000/389982 batches | lr 5.00 | ms/batch 69.19 | loss  3.55 | ppl    34.76\n",
            "| epoch   1 | 20200/389982 batches | lr 5.00 | ms/batch 72.90 | loss  3.68 | ppl    39.63\n",
            "| epoch   1 | 20400/389982 batches | lr 5.00 | ms/batch 68.74 | loss  3.66 | ppl    38.79\n",
            "| epoch   1 | 20600/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.65 | ppl    38.61\n",
            "| epoch   1 | 20800/389982 batches | lr 5.00 | ms/batch 68.52 | loss  3.68 | ppl    39.82\n",
            "| epoch   1 | 21000/389982 batches | lr 5.00 | ms/batch 68.88 | loss  3.59 | ppl    36.31\n",
            "| epoch   1 | 21200/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.74 | ppl    42.05\n",
            "| epoch   1 | 21400/389982 batches | lr 5.00 | ms/batch 68.57 | loss  3.63 | ppl    37.81\n",
            "| epoch   1 | 21600/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.61 | ppl    36.92\n",
            "| epoch   1 | 21800/389982 batches | lr 5.00 | ms/batch 68.26 | loss  3.65 | ppl    38.28\n",
            "| epoch   1 | 22000/389982 batches | lr 5.00 | ms/batch 68.45 | loss  3.70 | ppl    40.54\n",
            "| epoch   1 | 22200/389982 batches | lr 5.00 | ms/batch 72.53 | loss  3.64 | ppl    38.28\n",
            "| epoch   1 | 22400/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.68 | ppl    39.61\n",
            "| epoch   1 | 22600/389982 batches | lr 5.00 | ms/batch 67.64 | loss  3.62 | ppl    37.34\n",
            "| epoch   1 | 22800/389982 batches | lr 5.00 | ms/batch 68.35 | loss  3.57 | ppl    35.66\n",
            "| epoch   1 | 23000/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.59 | ppl    36.13\n",
            "| epoch   1 | 23200/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.66 | ppl    39.05\n",
            "| epoch   1 | 23400/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.61 | ppl    37.08\n",
            "| epoch   1 | 23600/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.63 | ppl    37.82\n",
            "| epoch   1 | 23800/389982 batches | lr 5.00 | ms/batch 67.55 | loss  3.79 | ppl    44.31\n",
            "| epoch   1 | 24000/389982 batches | lr 5.00 | ms/batch 67.67 | loss  3.62 | ppl    37.24\n",
            "| epoch   1 | 24200/389982 batches | lr 5.00 | ms/batch 73.51 | loss  3.61 | ppl    37.07\n",
            "| epoch   1 | 24400/389982 batches | lr 5.00 | ms/batch 67.89 | loss  3.59 | ppl    36.16\n",
            "| epoch   1 | 24600/389982 batches | lr 5.00 | ms/batch 68.85 | loss  3.66 | ppl    38.86\n",
            "| epoch   1 | 24800/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.77 | ppl    43.21\n",
            "| epoch   1 | 25000/389982 batches | lr 5.00 | ms/batch 68.11 | loss  3.58 | ppl    35.72\n",
            "| epoch   1 | 25200/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.58 | ppl    35.94\n",
            "| epoch   1 | 25400/389982 batches | lr 5.00 | ms/batch 69.01 | loss  3.55 | ppl    34.91\n",
            "| epoch   1 | 25600/389982 batches | lr 5.00 | ms/batch 68.60 | loss  3.55 | ppl    34.84\n",
            "| epoch   1 | 25800/389982 batches | lr 5.00 | ms/batch 68.37 | loss  3.66 | ppl    38.68\n",
            "| epoch   1 | 26000/389982 batches | lr 5.00 | ms/batch 67.21 | loss  3.73 | ppl    41.64\n",
            "| epoch   1 | 26200/389982 batches | lr 5.00 | ms/batch 72.91 | loss  3.65 | ppl    38.45\n",
            "| epoch   1 | 26400/389982 batches | lr 5.00 | ms/batch 68.41 | loss  3.55 | ppl    34.80\n",
            "| epoch   1 | 26600/389982 batches | lr 5.00 | ms/batch 69.48 | loss  3.59 | ppl    36.33\n",
            "| epoch   1 | 26800/389982 batches | lr 5.00 | ms/batch 68.54 | loss  3.58 | ppl    36.04\n",
            "| epoch   1 | 27000/389982 batches | lr 5.00 | ms/batch 68.89 | loss  3.59 | ppl    36.14\n",
            "| epoch   1 | 27200/389982 batches | lr 5.00 | ms/batch 67.14 | loss  3.65 | ppl    38.46\n",
            "| epoch   1 | 27400/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.68 | ppl    39.45\n",
            "| epoch   1 | 27600/389982 batches | lr 5.00 | ms/batch 67.33 | loss  3.78 | ppl    43.67\n",
            "| epoch   1 | 27800/389982 batches | lr 5.00 | ms/batch 69.84 | loss  3.57 | ppl    35.62\n",
            "| epoch   1 | 28000/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.60 | ppl    36.77\n",
            "| epoch   1 | 28200/389982 batches | lr 5.00 | ms/batch 77.93 | loss  3.62 | ppl    37.29\n",
            "| epoch   1 | 28400/389982 batches | lr 5.00 | ms/batch 68.82 | loss  3.55 | ppl    34.92\n",
            "| epoch   1 | 28600/389982 batches | lr 5.00 | ms/batch 67.08 | loss  3.61 | ppl    37.15\n",
            "| epoch   1 | 28800/389982 batches | lr 5.00 | ms/batch 67.05 | loss  3.69 | ppl    39.96\n",
            "| epoch   1 | 29000/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.68 | ppl    39.53\n",
            "| epoch   1 | 29200/389982 batches | lr 5.00 | ms/batch 69.00 | loss  3.54 | ppl    34.49\n",
            "| epoch   1 | 29400/389982 batches | lr 5.00 | ms/batch 69.39 | loss  3.58 | ppl    35.70\n",
            "| epoch   1 | 29600/389982 batches | lr 5.00 | ms/batch 68.53 | loss  3.56 | ppl    35.13\n",
            "| epoch   1 | 29800/389982 batches | lr 5.00 | ms/batch 68.81 | loss  3.68 | ppl    39.49\n",
            "| epoch   1 | 30000/389982 batches | lr 5.00 | ms/batch 66.42 | loss  3.76 | ppl    42.86\n",
            "| epoch   1 | 30200/389982 batches | lr 5.00 | ms/batch 73.05 | loss  3.57 | ppl    35.50\n",
            "| epoch   1 | 30400/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.81 | ppl    45.31\n",
            "| epoch   1 | 30600/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.60 | ppl    36.71\n",
            "| epoch   1 | 30800/389982 batches | lr 5.00 | ms/batch 68.47 | loss  3.51 | ppl    33.57\n",
            "| epoch   1 | 31000/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.70 | ppl    40.35\n",
            "| epoch   1 | 31200/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.62 | ppl    37.26\n",
            "| epoch   1 | 31400/389982 batches | lr 5.00 | ms/batch 66.50 | loss  3.77 | ppl    43.48\n",
            "| epoch   1 | 31600/389982 batches | lr 5.00 | ms/batch 67.84 | loss  3.59 | ppl    36.10\n",
            "| epoch   1 | 31800/389982 batches | lr 5.00 | ms/batch 68.57 | loss  3.58 | ppl    35.86\n",
            "| epoch   1 | 32000/389982 batches | lr 5.00 | ms/batch 68.79 | loss  3.56 | ppl    35.17\n",
            "| epoch   1 | 32200/389982 batches | lr 5.00 | ms/batch 72.20 | loss  3.56 | ppl    35.24\n",
            "| epoch   1 | 32400/389982 batches | lr 5.00 | ms/batch 67.47 | loss  3.67 | ppl    39.16\n",
            "| epoch   1 | 32600/389982 batches | lr 5.00 | ms/batch 68.17 | loss  3.55 | ppl    34.81\n",
            "| epoch   1 | 32800/389982 batches | lr 5.00 | ms/batch 67.96 | loss  3.57 | ppl    35.53\n",
            "| epoch   1 | 33000/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.57 | ppl    35.38\n",
            "| epoch   1 | 33200/389982 batches | lr 5.00 | ms/batch 68.78 | loss  3.57 | ppl    35.65\n",
            "| epoch   1 | 33400/389982 batches | lr 5.00 | ms/batch 67.85 | loss  3.69 | ppl    40.14\n",
            "| epoch   1 | 33600/389982 batches | lr 5.00 | ms/batch 67.75 | loss  3.62 | ppl    37.34\n",
            "| epoch   1 | 33800/389982 batches | lr 5.00 | ms/batch 67.46 | loss  3.61 | ppl    36.81\n",
            "| epoch   1 | 34000/389982 batches | lr 5.00 | ms/batch 67.97 | loss  3.63 | ppl    37.78\n",
            "| epoch   1 | 34200/389982 batches | lr 5.00 | ms/batch 72.71 | loss  3.69 | ppl    40.16\n",
            "| epoch   1 | 34400/389982 batches | lr 5.00 | ms/batch 68.68 | loss  3.83 | ppl    45.97\n",
            "| epoch   1 | 34600/389982 batches | lr 5.00 | ms/batch 67.36 | loss  3.78 | ppl    43.99\n",
            "| epoch   1 | 34800/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.59 | ppl    36.13\n",
            "| epoch   1 | 35000/389982 batches | lr 5.00 | ms/batch 69.53 | loss  3.53 | ppl    34.16\n",
            "| epoch   1 | 35200/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.67 | ppl    39.29\n",
            "| epoch   1 | 35400/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.62 | ppl    37.25\n",
            "| epoch   1 | 35600/389982 batches | lr 5.00 | ms/batch 68.36 | loss  3.57 | ppl    35.47\n",
            "| epoch   1 | 35800/389982 batches | lr 5.00 | ms/batch 68.29 | loss  3.59 | ppl    36.08\n",
            "| epoch   1 | 36000/389982 batches | lr 5.00 | ms/batch 67.21 | loss  3.60 | ppl    36.68\n",
            "| epoch   1 | 36200/389982 batches | lr 5.00 | ms/batch 72.90 | loss  3.69 | ppl    39.95\n",
            "| epoch   1 | 36400/389982 batches | lr 5.00 | ms/batch 68.35 | loss  3.52 | ppl    33.80\n",
            "| epoch   1 | 36600/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.61 | ppl    37.05\n",
            "| epoch   1 | 36800/389982 batches | lr 5.00 | ms/batch 67.06 | loss  3.59 | ppl    36.10\n",
            "| epoch   1 | 37000/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.66 | ppl    38.88\n",
            "| epoch   1 | 37200/389982 batches | lr 5.00 | ms/batch 68.07 | loss  3.63 | ppl    37.53\n",
            "| epoch   1 | 37400/389982 batches | lr 5.00 | ms/batch 68.24 | loss  3.60 | ppl    36.78\n",
            "| epoch   1 | 37600/389982 batches | lr 5.00 | ms/batch 68.32 | loss  3.55 | ppl    34.70\n",
            "| epoch   1 | 37800/389982 batches | lr 5.00 | ms/batch 67.75 | loss  3.56 | ppl    35.34\n",
            "| epoch   1 | 38000/389982 batches | lr 5.00 | ms/batch 68.63 | loss  3.53 | ppl    34.01\n",
            "| epoch   1 | 38200/389982 batches | lr 5.00 | ms/batch 73.66 | loss  3.51 | ppl    33.55\n",
            "| epoch   1 | 38400/389982 batches | lr 5.00 | ms/batch 66.73 | loss  3.66 | ppl    38.91\n",
            "| epoch   1 | 38600/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.56 | ppl    35.04\n",
            "| epoch   1 | 38800/389982 batches | lr 5.00 | ms/batch 67.78 | loss  3.62 | ppl    37.49\n",
            "| epoch   1 | 39000/389982 batches | lr 5.00 | ms/batch 67.92 | loss  3.70 | ppl    40.57\n",
            "| epoch   1 | 39200/389982 batches | lr 5.00 | ms/batch 67.36 | loss  3.59 | ppl    36.22\n",
            "| epoch   1 | 39400/389982 batches | lr 5.00 | ms/batch 68.27 | loss  3.53 | ppl    34.01\n",
            "| epoch   1 | 39600/389982 batches | lr 5.00 | ms/batch 67.91 | loss  3.57 | ppl    35.57\n",
            "| epoch   1 | 39800/389982 batches | lr 5.00 | ms/batch 67.75 | loss  3.59 | ppl    36.22\n",
            "| epoch   1 | 40000/389982 batches | lr 5.00 | ms/batch 67.97 | loss  3.55 | ppl    34.77\n",
            "| epoch   1 | 40200/389982 batches | lr 5.00 | ms/batch 72.97 | loss  3.60 | ppl    36.69\n",
            "| epoch   1 | 40400/389982 batches | lr 5.00 | ms/batch 68.70 | loss  3.52 | ppl    33.75\n",
            "| epoch   1 | 40600/389982 batches | lr 5.00 | ms/batch 67.17 | loss  3.59 | ppl    36.16\n",
            "| epoch   1 | 40800/389982 batches | lr 5.00 | ms/batch 68.86 | loss  3.56 | ppl    35.20\n",
            "| epoch   1 | 41000/389982 batches | lr 5.00 | ms/batch 68.76 | loss  3.53 | ppl    34.29\n",
            "| epoch   1 | 41200/389982 batches | lr 5.00 | ms/batch 66.80 | loss  3.70 | ppl    40.26\n",
            "| epoch   1 | 41400/389982 batches | lr 5.00 | ms/batch 68.61 | loss  3.85 | ppl    47.16\n",
            "| epoch   1 | 41600/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.81 | ppl    45.19\n",
            "| epoch   1 | 41800/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.88 | ppl    48.49\n",
            "| epoch   1 | 42000/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.83 | ppl    45.97\n",
            "| epoch   1 | 42200/389982 batches | lr 5.00 | ms/batch 72.58 | loss  3.70 | ppl    40.34\n",
            "| epoch   1 | 42400/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.74 | ppl    42.25\n",
            "| epoch   1 | 42600/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.62 | ppl    37.47\n",
            "| epoch   1 | 42800/389982 batches | lr 5.00 | ms/batch 67.46 | loss  3.62 | ppl    37.18\n",
            "| epoch   1 | 43000/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.56 | ppl    35.27\n",
            "| epoch   1 | 43200/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.59 | ppl    36.24\n",
            "| epoch   1 | 43400/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.51 | ppl    33.47\n",
            "| epoch   1 | 43600/389982 batches | lr 5.00 | ms/batch 68.77 | loss  3.53 | ppl    34.13\n",
            "| epoch   1 | 43800/389982 batches | lr 5.00 | ms/batch 68.25 | loss  3.78 | ppl    43.76\n",
            "| epoch   1 | 44000/389982 batches | lr 5.00 | ms/batch 68.08 | loss  3.66 | ppl    38.75\n",
            "| epoch   1 | 44200/389982 batches | lr 5.00 | ms/batch 74.19 | loss  3.51 | ppl    33.44\n",
            "| epoch   1 | 44400/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.73 | ppl    41.85\n",
            "| epoch   1 | 44600/389982 batches | lr 5.00 | ms/batch 68.48 | loss  3.51 | ppl    33.33\n",
            "| epoch   1 | 44800/389982 batches | lr 5.00 | ms/batch 68.16 | loss  3.55 | ppl    34.74\n",
            "| epoch   1 | 45000/389982 batches | lr 5.00 | ms/batch 67.01 | loss  3.63 | ppl    37.79\n",
            "| epoch   1 | 45200/389982 batches | lr 5.00 | ms/batch 67.65 | loss  3.61 | ppl    36.80\n",
            "| epoch   1 | 45400/389982 batches | lr 5.00 | ms/batch 67.65 | loss  3.56 | ppl    35.26\n",
            "| epoch   1 | 45600/389982 batches | lr 5.00 | ms/batch 67.78 | loss  3.66 | ppl    38.92\n",
            "| epoch   1 | 45800/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.55 | ppl    34.84\n",
            "| epoch   1 | 46000/389982 batches | lr 5.00 | ms/batch 67.32 | loss  3.59 | ppl    36.41\n",
            "| epoch   1 | 46200/389982 batches | lr 5.00 | ms/batch 72.24 | loss  3.69 | ppl    40.02\n",
            "| epoch   1 | 46400/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.58 | ppl    35.95\n",
            "| epoch   1 | 46600/389982 batches | lr 5.00 | ms/batch 67.98 | loss  3.69 | ppl    39.96\n",
            "| epoch   1 | 46800/389982 batches | lr 5.00 | ms/batch 67.60 | loss  3.56 | ppl    35.28\n",
            "| epoch   1 | 47000/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.54 | ppl    34.30\n",
            "| epoch   1 | 47200/389982 batches | lr 5.00 | ms/batch 67.07 | loss  3.57 | ppl    35.58\n",
            "| epoch   1 | 47400/389982 batches | lr 5.00 | ms/batch 67.85 | loss  3.58 | ppl    35.99\n",
            "| epoch   1 | 47600/389982 batches | lr 5.00 | ms/batch 67.24 | loss  3.57 | ppl    35.53\n",
            "| epoch   1 | 47800/389982 batches | lr 5.00 | ms/batch 69.22 | loss  3.54 | ppl    34.43\n",
            "| epoch   1 | 48000/389982 batches | lr 5.00 | ms/batch 66.85 | loss  3.59 | ppl    36.27\n",
            "| epoch   1 | 48200/389982 batches | lr 5.00 | ms/batch 72.74 | loss  3.54 | ppl    34.58\n",
            "| epoch   1 | 48400/389982 batches | lr 5.00 | ms/batch 65.63 | loss  3.68 | ppl    39.79\n",
            "| epoch   1 | 48600/389982 batches | lr 5.00 | ms/batch 67.27 | loss  3.53 | ppl    34.20\n",
            "| epoch   1 | 48800/389982 batches | lr 5.00 | ms/batch 67.00 | loss  3.66 | ppl    38.71\n",
            "| epoch   1 | 49000/389982 batches | lr 5.00 | ms/batch 67.34 | loss  3.78 | ppl    43.75\n",
            "| epoch   1 | 49200/389982 batches | lr 5.00 | ms/batch 67.65 | loss  3.56 | ppl    35.02\n",
            "| epoch   1 | 49400/389982 batches | lr 5.00 | ms/batch 67.32 | loss  3.56 | ppl    34.99\n",
            "| epoch   1 | 49600/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.53 | ppl    34.26\n",
            "| epoch   1 | 49800/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.57 | ppl    35.34\n",
            "| epoch   1 | 50000/389982 batches | lr 5.00 | ms/batch 68.11 | loss  3.57 | ppl    35.67\n",
            "| epoch   1 | 50200/389982 batches | lr 5.00 | ms/batch 73.24 | loss  3.54 | ppl    34.37\n",
            "| epoch   1 | 50400/389982 batches | lr 5.00 | ms/batch 68.38 | loss  3.88 | ppl    48.65\n",
            "| epoch   1 | 50600/389982 batches | lr 5.00 | ms/batch 67.38 | loss  3.59 | ppl    36.06\n",
            "| epoch   1 | 50800/389982 batches | lr 5.00 | ms/batch 68.33 | loss  3.91 | ppl    49.83\n",
            "| epoch   1 | 51000/389982 batches | lr 5.00 | ms/batch 69.57 | loss  3.70 | ppl    40.49\n",
            "| epoch   1 | 51200/389982 batches | lr 5.00 | ms/batch 68.05 | loss  3.56 | ppl    35.00\n",
            "| epoch   1 | 51400/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.64 | ppl    38.12\n",
            "| epoch   1 | 51600/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.55 | ppl    34.87\n",
            "| epoch   1 | 51800/389982 batches | lr 5.00 | ms/batch 66.79 | loss  3.64 | ppl    37.96\n",
            "| epoch   1 | 52000/389982 batches | lr 5.00 | ms/batch 68.73 | loss  3.51 | ppl    33.43\n",
            "| epoch   1 | 52200/389982 batches | lr 5.00 | ms/batch 73.41 | loss  3.53 | ppl    33.97\n",
            "| epoch   1 | 52400/389982 batches | lr 5.00 | ms/batch 66.69 | loss  3.58 | ppl    35.78\n",
            "| epoch   1 | 52600/389982 batches | lr 5.00 | ms/batch 68.33 | loss  3.54 | ppl    34.58\n",
            "| epoch   1 | 52800/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.57 | ppl    35.36\n",
            "| epoch   1 | 53000/389982 batches | lr 5.00 | ms/batch 68.89 | loss  3.57 | ppl    35.49\n",
            "| epoch   1 | 53200/389982 batches | lr 5.00 | ms/batch 67.91 | loss  3.55 | ppl    34.72\n",
            "| epoch   1 | 53400/389982 batches | lr 5.00 | ms/batch 67.68 | loss  3.57 | ppl    35.51\n",
            "| epoch   1 | 53600/389982 batches | lr 5.00 | ms/batch 66.71 | loss  3.61 | ppl    36.99\n",
            "| epoch   1 | 53800/389982 batches | lr 5.00 | ms/batch 68.63 | loss  3.53 | ppl    34.05\n",
            "| epoch   1 | 54000/389982 batches | lr 5.00 | ms/batch 67.09 | loss  3.66 | ppl    38.98\n",
            "| epoch   1 | 54200/389982 batches | lr 5.00 | ms/batch 72.17 | loss  3.54 | ppl    34.41\n",
            "| epoch   1 | 54400/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.51 | ppl    33.58\n",
            "| epoch   1 | 54600/389982 batches | lr 5.00 | ms/batch 67.53 | loss  3.55 | ppl    34.72\n",
            "| epoch   1 | 54800/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.56 | ppl    35.29\n",
            "| epoch   1 | 55000/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.53 | ppl    34.29\n",
            "| epoch   1 | 55200/389982 batches | lr 5.00 | ms/batch 67.57 | loss  3.58 | ppl    35.74\n",
            "| epoch   1 | 55400/389982 batches | lr 5.00 | ms/batch 68.53 | loss  3.52 | ppl    33.93\n",
            "| epoch   1 | 55600/389982 batches | lr 5.00 | ms/batch 68.36 | loss  3.47 | ppl    32.29\n",
            "| epoch   1 | 55800/389982 batches | lr 5.00 | ms/batch 68.50 | loss  3.57 | ppl    35.39\n",
            "| epoch   1 | 56000/389982 batches | lr 5.00 | ms/batch 66.94 | loss  3.59 | ppl    36.09\n",
            "| epoch   1 | 56200/389982 batches | lr 5.00 | ms/batch 71.72 | loss  3.59 | ppl    36.33\n",
            "| epoch   1 | 56400/389982 batches | lr 5.00 | ms/batch 68.16 | loss  3.54 | ppl    34.44\n",
            "| epoch   1 | 56600/389982 batches | lr 5.00 | ms/batch 67.48 | loss  3.82 | ppl    45.39\n",
            "| epoch   1 | 56800/389982 batches | lr 5.00 | ms/batch 67.37 | loss  3.58 | ppl    35.70\n",
            "| epoch   1 | 57000/389982 batches | lr 5.00 | ms/batch 68.77 | loss  3.47 | ppl    31.98\n",
            "| epoch   1 | 57200/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.51 | ppl    33.50\n",
            "| epoch   1 | 57400/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.54 | ppl    34.37\n",
            "| epoch   1 | 57600/389982 batches | lr 5.00 | ms/batch 67.59 | loss  3.70 | ppl    40.49\n",
            "| epoch   1 | 57800/389982 batches | lr 5.00 | ms/batch 70.71 | loss  3.61 | ppl    37.11\n",
            "| epoch   1 | 58000/389982 batches | lr 5.00 | ms/batch 67.62 | loss  3.52 | ppl    33.70\n",
            "| epoch   1 | 58200/389982 batches | lr 5.00 | ms/batch 73.54 | loss  3.51 | ppl    33.40\n",
            "| epoch   1 | 58400/389982 batches | lr 5.00 | ms/batch 68.34 | loss  3.61 | ppl    37.12\n",
            "| epoch   1 | 58600/389982 batches | lr 5.00 | ms/batch 68.90 | loss  3.57 | ppl    35.54\n",
            "| epoch   1 | 58800/389982 batches | lr 5.00 | ms/batch 67.68 | loss  3.56 | ppl    35.29\n",
            "| epoch   1 | 59000/389982 batches | lr 5.00 | ms/batch 67.23 | loss  3.55 | ppl    34.82\n",
            "| epoch   1 | 59200/389982 batches | lr 5.00 | ms/batch 66.87 | loss  3.63 | ppl    37.77\n",
            "| epoch   1 | 59400/389982 batches | lr 5.00 | ms/batch 67.08 | loss  3.59 | ppl    36.22\n",
            "| epoch   1 | 59600/389982 batches | lr 5.00 | ms/batch 70.08 | loss  3.48 | ppl    32.57\n",
            "| epoch   1 | 59800/389982 batches | lr 5.00 | ms/batch 67.20 | loss  3.57 | ppl    35.34\n",
            "| epoch   1 | 60000/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.63 | ppl    37.83\n",
            "| epoch   1 | 60200/389982 batches | lr 5.00 | ms/batch 72.02 | loss  3.59 | ppl    36.06\n",
            "| epoch   1 | 60400/389982 batches | lr 5.00 | ms/batch 66.09 | loss  3.62 | ppl    37.23\n",
            "| epoch   1 | 60600/389982 batches | lr 5.00 | ms/batch 67.50 | loss  3.60 | ppl    36.51\n",
            "| epoch   1 | 60800/389982 batches | lr 5.00 | ms/batch 68.09 | loss  3.58 | ppl    35.79\n",
            "| epoch   1 | 61000/389982 batches | lr 5.00 | ms/batch 66.32 | loss  3.63 | ppl    37.85\n",
            "| epoch   1 | 61200/389982 batches | lr 5.00 | ms/batch 66.77 | loss  3.66 | ppl    39.04\n",
            "| epoch   1 | 61400/389982 batches | lr 5.00 | ms/batch 68.38 | loss  3.85 | ppl    47.08\n",
            "| epoch   1 | 61600/389982 batches | lr 5.00 | ms/batch 66.85 | loss  3.64 | ppl    38.03\n",
            "| epoch   1 | 61800/389982 batches | lr 5.00 | ms/batch 68.39 | loss  3.52 | ppl    33.85\n",
            "| epoch   1 | 62000/389982 batches | lr 5.00 | ms/batch 68.49 | loss  3.72 | ppl    41.37\n",
            "| epoch   1 | 62200/389982 batches | lr 5.00 | ms/batch 72.44 | loss  3.53 | ppl    34.21\n",
            "| epoch   1 | 62400/389982 batches | lr 5.00 | ms/batch 67.33 | loss  3.60 | ppl    36.46\n",
            "| epoch   1 | 62600/389982 batches | lr 5.00 | ms/batch 68.16 | loss  3.58 | ppl    35.93\n",
            "| epoch   1 | 62800/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.69 | ppl    40.11\n",
            "| epoch   1 | 63000/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.54 | ppl    34.64\n",
            "| epoch   1 | 63200/389982 batches | lr 5.00 | ms/batch 67.62 | loss  3.55 | ppl    34.82\n",
            "| epoch   1 | 63400/389982 batches | lr 5.00 | ms/batch 67.38 | loss  3.55 | ppl    34.72\n",
            "| epoch   1 | 63600/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.62 | ppl    37.52\n",
            "| epoch   1 | 63800/389982 batches | lr 5.00 | ms/batch 67.89 | loss  3.57 | ppl    35.53\n",
            "| epoch   1 | 64000/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.56 | ppl    35.23\n",
            "| epoch   1 | 64200/389982 batches | lr 5.00 | ms/batch 71.22 | loss  3.59 | ppl    36.30\n",
            "| epoch   1 | 64400/389982 batches | lr 5.00 | ms/batch 66.90 | loss  3.57 | ppl    35.57\n",
            "| epoch   1 | 64600/389982 batches | lr 5.00 | ms/batch 70.27 | loss  3.54 | ppl    34.51\n",
            "| epoch   1 | 64800/389982 batches | lr 5.00 | ms/batch 66.49 | loss  3.63 | ppl    37.64\n",
            "| epoch   1 | 65000/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.52 | ppl    33.88\n",
            "| epoch   1 | 65200/389982 batches | lr 5.00 | ms/batch 67.60 | loss  3.54 | ppl    34.30\n",
            "| epoch   1 | 65400/389982 batches | lr 5.00 | ms/batch 68.26 | loss  3.61 | ppl    37.12\n",
            "| epoch   1 | 65600/389982 batches | lr 5.00 | ms/batch 68.36 | loss  3.55 | ppl    34.77\n",
            "| epoch   1 | 65800/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.52 | ppl    33.89\n",
            "| epoch   1 | 66000/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.56 | ppl    35.02\n",
            "| epoch   1 | 66200/389982 batches | lr 5.00 | ms/batch 72.21 | loss  3.61 | ppl    37.12\n",
            "| epoch   1 | 66400/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.51 | ppl    33.49\n",
            "| epoch   1 | 66600/389982 batches | lr 5.00 | ms/batch 68.92 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 66800/389982 batches | lr 5.00 | ms/batch 67.16 | loss  3.53 | ppl    33.95\n",
            "| epoch   1 | 67000/389982 batches | lr 5.00 | ms/batch 67.90 | loss  3.54 | ppl    34.59\n",
            "| epoch   1 | 67200/389982 batches | lr 5.00 | ms/batch 68.38 | loss  3.54 | ppl    34.40\n",
            "| epoch   1 | 67400/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.53 | ppl    34.16\n",
            "| epoch   1 | 67600/389982 batches | lr 5.00 | ms/batch 67.28 | loss  3.59 | ppl    36.07\n",
            "| epoch   1 | 67800/389982 batches | lr 5.00 | ms/batch 66.96 | loss  3.61 | ppl    36.79\n",
            "| epoch   1 | 68000/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.51 | ppl    33.58\n",
            "| epoch   1 | 68200/389982 batches | lr 5.00 | ms/batch 72.50 | loss  3.52 | ppl    33.82\n",
            "| epoch   1 | 68400/389982 batches | lr 5.00 | ms/batch 68.53 | loss  3.45 | ppl    31.62\n",
            "| epoch   1 | 68600/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.60 | ppl    36.43\n",
            "| epoch   1 | 68800/389982 batches | lr 5.00 | ms/batch 68.12 | loss  3.54 | ppl    34.56\n",
            "| epoch   1 | 69000/389982 batches | lr 5.00 | ms/batch 68.37 | loss  3.51 | ppl    33.37\n",
            "| epoch   1 | 69200/389982 batches | lr 5.00 | ms/batch 68.36 | loss  3.48 | ppl    32.60\n",
            "| epoch   1 | 69400/389982 batches | lr 5.00 | ms/batch 68.01 | loss  3.62 | ppl    37.24\n",
            "| epoch   1 | 69600/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.65 | ppl    38.35\n",
            "| epoch   1 | 69800/389982 batches | lr 5.00 | ms/batch 68.25 | loss  3.47 | ppl    32.24\n",
            "| epoch   1 | 70000/389982 batches | lr 5.00 | ms/batch 67.84 | loss  3.60 | ppl    36.52\n",
            "| epoch   1 | 70200/389982 batches | lr 5.00 | ms/batch 71.82 | loss  3.56 | ppl    35.31\n",
            "| epoch   1 | 70400/389982 batches | lr 5.00 | ms/batch 67.24 | loss  3.62 | ppl    37.44\n",
            "| epoch   1 | 70600/389982 batches | lr 5.00 | ms/batch 68.76 | loss  3.52 | ppl    33.67\n",
            "| epoch   1 | 70800/389982 batches | lr 5.00 | ms/batch 68.98 | loss  3.50 | ppl    33.24\n",
            "| epoch   1 | 71000/389982 batches | lr 5.00 | ms/batch 69.27 | loss  3.47 | ppl    32.15\n",
            "| epoch   1 | 71200/389982 batches | lr 5.00 | ms/batch 68.60 | loss  3.52 | ppl    33.79\n",
            "| epoch   1 | 71400/389982 batches | lr 5.00 | ms/batch 68.19 | loss  3.57 | ppl    35.68\n",
            "| epoch   1 | 71600/389982 batches | lr 5.00 | ms/batch 72.73 | loss  3.44 | ppl    31.33\n",
            "| epoch   1 | 71800/389982 batches | lr 5.00 | ms/batch 67.18 | loss  3.58 | ppl    35.80\n",
            "| epoch   1 | 72000/389982 batches | lr 5.00 | ms/batch 67.01 | loss  3.55 | ppl    34.91\n",
            "| epoch   1 | 72200/389982 batches | lr 5.00 | ms/batch 70.68 | loss  3.70 | ppl    40.44\n",
            "| epoch   1 | 72400/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.51 | ppl    33.55\n",
            "| epoch   1 | 72600/389982 batches | lr 5.00 | ms/batch 68.09 | loss  3.55 | ppl    34.79\n",
            "| epoch   1 | 72800/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.49 | ppl    32.94\n",
            "| epoch   1 | 73000/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.57 | ppl    35.66\n",
            "| epoch   1 | 73200/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.53 | ppl    33.99\n",
            "| epoch   1 | 73400/389982 batches | lr 5.00 | ms/batch 68.72 | loss  3.60 | ppl    36.71\n",
            "| epoch   1 | 73600/389982 batches | lr 5.00 | ms/batch 68.53 | loss  3.71 | ppl    40.92\n",
            "| epoch   1 | 73800/389982 batches | lr 5.00 | ms/batch 67.58 | loss  3.61 | ppl    36.79\n",
            "| epoch   1 | 74000/389982 batches | lr 5.00 | ms/batch 68.66 | loss  3.66 | ppl    38.71\n",
            "| epoch   1 | 74200/389982 batches | lr 5.00 | ms/batch 72.78 | loss  3.51 | ppl    33.34\n",
            "| epoch   1 | 74400/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.52 | ppl    33.71\n",
            "| epoch   1 | 74600/389982 batches | lr 5.00 | ms/batch 71.30 | loss  3.48 | ppl    32.52\n",
            "| epoch   1 | 74800/389982 batches | lr 5.00 | ms/batch 67.72 | loss  3.52 | ppl    33.78\n",
            "| epoch   1 | 75000/389982 batches | lr 5.00 | ms/batch 68.14 | loss  3.50 | ppl    33.18\n",
            "| epoch   1 | 75200/389982 batches | lr 5.00 | ms/batch 67.17 | loss  3.54 | ppl    34.36\n",
            "| epoch   1 | 75400/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.53 | ppl    34.04\n",
            "| epoch   1 | 75600/389982 batches | lr 5.00 | ms/batch 68.40 | loss  3.47 | ppl    32.11\n",
            "| epoch   1 | 75800/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.58 | ppl    35.98\n",
            "| epoch   1 | 76000/389982 batches | lr 5.00 | ms/batch 67.21 | loss  3.66 | ppl    38.68\n",
            "| epoch   1 | 76200/389982 batches | lr 5.00 | ms/batch 71.96 | loss  3.56 | ppl    35.08\n",
            "| epoch   1 | 76400/389982 batches | lr 5.00 | ms/batch 68.40 | loss  3.48 | ppl    32.62\n",
            "| epoch   1 | 76600/389982 batches | lr 5.00 | ms/batch 67.79 | loss  3.53 | ppl    34.09\n",
            "| epoch   1 | 76800/389982 batches | lr 5.00 | ms/batch 68.14 | loss  3.57 | ppl    35.44\n",
            "| epoch   1 | 77000/389982 batches | lr 5.00 | ms/batch 68.29 | loss  3.54 | ppl    34.33\n",
            "| epoch   1 | 77200/389982 batches | lr 5.00 | ms/batch 69.06 | loss  3.55 | ppl    34.80\n",
            "| epoch   1 | 77400/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.56 | ppl    35.31\n",
            "| epoch   1 | 77600/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.50 | ppl    32.98\n",
            "| epoch   1 | 77800/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.65 | ppl    38.37\n",
            "| epoch   1 | 78000/389982 batches | lr 5.00 | ms/batch 68.09 | loss  3.53 | ppl    34.04\n",
            "| epoch   1 | 78200/389982 batches | lr 5.00 | ms/batch 73.39 | loss  3.56 | ppl    35.08\n",
            "| epoch   1 | 78400/389982 batches | lr 5.00 | ms/batch 66.76 | loss  3.53 | ppl    34.19\n",
            "| epoch   1 | 78600/389982 batches | lr 5.00 | ms/batch 67.07 | loss  3.55 | ppl    34.80\n",
            "| epoch   1 | 78800/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.48 | ppl    32.61\n",
            "| epoch   1 | 79000/389982 batches | lr 5.00 | ms/batch 67.19 | loss  3.52 | ppl    33.78\n",
            "| epoch   1 | 79200/389982 batches | lr 5.00 | ms/batch 67.88 | loss  3.50 | ppl    33.26\n",
            "| epoch   1 | 79400/389982 batches | lr 5.00 | ms/batch 67.19 | loss  3.52 | ppl    33.71\n",
            "| epoch   1 | 79600/389982 batches | lr 5.00 | ms/batch 66.71 | loss  3.64 | ppl    37.91\n",
            "| epoch   1 | 79800/389982 batches | lr 5.00 | ms/batch 67.10 | loss  3.54 | ppl    34.34\n",
            "| epoch   1 | 80000/389982 batches | lr 5.00 | ms/batch 68.11 | loss  3.54 | ppl    34.38\n",
            "| epoch   1 | 80200/389982 batches | lr 5.00 | ms/batch 72.63 | loss  3.61 | ppl    36.94\n",
            "| epoch   1 | 80400/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.62 | ppl    37.48\n",
            "| epoch   1 | 80600/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.52 | ppl    33.73\n",
            "| epoch   1 | 80800/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.55 | ppl    34.74\n",
            "| epoch   1 | 81000/389982 batches | lr 5.00 | ms/batch 68.25 | loss  3.51 | ppl    33.46\n",
            "| epoch   1 | 81200/389982 batches | lr 5.00 | ms/batch 67.38 | loss  3.53 | ppl    34.19\n",
            "| epoch   1 | 81400/389982 batches | lr 5.00 | ms/batch 69.53 | loss  3.47 | ppl    31.99\n",
            "| epoch   1 | 81600/389982 batches | lr 5.00 | ms/batch 66.53 | loss  3.57 | ppl    35.66\n",
            "| epoch   1 | 81800/389982 batches | lr 5.00 | ms/batch 67.05 | loss  3.55 | ppl    34.73\n",
            "| epoch   1 | 82000/389982 batches | lr 5.00 | ms/batch 67.21 | loss  3.52 | ppl    33.83\n",
            "| epoch   1 | 82200/389982 batches | lr 5.00 | ms/batch 73.18 | loss  3.48 | ppl    32.32\n",
            "| epoch   1 | 82400/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.64 | ppl    38.14\n",
            "| epoch   1 | 82600/389982 batches | lr 5.00 | ms/batch 68.09 | loss  3.66 | ppl    39.04\n",
            "| epoch   1 | 82800/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.51 | ppl    33.55\n",
            "| epoch   1 | 83000/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.59 | ppl    36.23\n",
            "| epoch   1 | 83200/389982 batches | lr 5.00 | ms/batch 69.02 | loss  3.44 | ppl    31.08\n",
            "| epoch   1 | 83400/389982 batches | lr 5.00 | ms/batch 70.41 | loss  3.47 | ppl    32.05\n",
            "| epoch   1 | 83600/389982 batches | lr 5.00 | ms/batch 67.44 | loss  3.51 | ppl    33.46\n",
            "| epoch   1 | 83800/389982 batches | lr 5.00 | ms/batch 68.04 | loss  3.48 | ppl    32.38\n",
            "| epoch   1 | 84000/389982 batches | lr 5.00 | ms/batch 69.12 | loss  3.47 | ppl    32.09\n",
            "| epoch   1 | 84200/389982 batches | lr 5.00 | ms/batch 71.93 | loss  3.51 | ppl    33.53\n",
            "| epoch   1 | 84400/389982 batches | lr 5.00 | ms/batch 68.32 | loss  3.50 | ppl    33.26\n",
            "| epoch   1 | 84600/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.51 | ppl    33.53\n",
            "| epoch   1 | 84800/389982 batches | lr 5.00 | ms/batch 67.54 | loss  3.62 | ppl    37.42\n",
            "| epoch   1 | 85000/389982 batches | lr 5.00 | ms/batch 69.55 | loss  3.47 | ppl    32.25\n",
            "| epoch   1 | 85200/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.55 | ppl    34.93\n",
            "| epoch   1 | 85400/389982 batches | lr 5.00 | ms/batch 68.16 | loss  3.66 | ppl    38.92\n",
            "| epoch   1 | 85600/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.62 | ppl    37.39\n",
            "| epoch   1 | 85800/389982 batches | lr 5.00 | ms/batch 68.76 | loss  3.49 | ppl    32.82\n",
            "| epoch   1 | 86000/389982 batches | lr 5.00 | ms/batch 67.32 | loss  3.49 | ppl    32.93\n",
            "| epoch   1 | 86200/389982 batches | lr 5.00 | ms/batch 71.14 | loss  3.68 | ppl    39.54\n",
            "| epoch   1 | 86400/389982 batches | lr 5.00 | ms/batch 68.04 | loss  3.52 | ppl    33.85\n",
            "| epoch   1 | 86600/389982 batches | lr 5.00 | ms/batch 68.84 | loss  3.51 | ppl    33.47\n",
            "| epoch   1 | 86800/389982 batches | lr 5.00 | ms/batch 69.78 | loss  3.54 | ppl    34.56\n",
            "| epoch   1 | 87000/389982 batches | lr 5.00 | ms/batch 68.38 | loss  3.54 | ppl    34.31\n",
            "| epoch   1 | 87200/389982 batches | lr 5.00 | ms/batch 68.64 | loss  3.79 | ppl    44.45\n",
            "| epoch   1 | 87400/389982 batches | lr 5.00 | ms/batch 67.48 | loss  3.62 | ppl    37.43\n",
            "| epoch   1 | 87600/389982 batches | lr 5.00 | ms/batch 68.17 | loss  3.47 | ppl    32.13\n",
            "| epoch   1 | 87800/389982 batches | lr 5.00 | ms/batch 66.99 | loss  3.69 | ppl    40.24\n",
            "| epoch   1 | 88000/389982 batches | lr 5.00 | ms/batch 67.28 | loss  3.56 | ppl    35.30\n",
            "| epoch   1 | 88200/389982 batches | lr 5.00 | ms/batch 73.59 | loss  3.58 | ppl    35.78\n",
            "| epoch   1 | 88400/389982 batches | lr 5.00 | ms/batch 67.68 | loss  3.56 | ppl    35.25\n",
            "| epoch   1 | 88600/389982 batches | lr 5.00 | ms/batch 68.80 | loss  3.56 | ppl    35.32\n",
            "| epoch   1 | 88800/389982 batches | lr 5.00 | ms/batch 68.98 | loss  3.49 | ppl    32.67\n",
            "| epoch   1 | 89000/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.57 | ppl    35.64\n",
            "| epoch   1 | 89200/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.59 | ppl    36.23\n",
            "| epoch   1 | 89400/389982 batches | lr 5.00 | ms/batch 66.92 | loss  3.58 | ppl    35.97\n",
            "| epoch   1 | 89600/389982 batches | lr 5.00 | ms/batch 68.27 | loss  3.53 | ppl    34.07\n",
            "| epoch   1 | 89800/389982 batches | lr 5.00 | ms/batch 66.33 | loss  3.63 | ppl    37.84\n",
            "| epoch   1 | 90000/389982 batches | lr 5.00 | ms/batch 67.58 | loss  3.51 | ppl    33.39\n",
            "| epoch   1 | 90200/389982 batches | lr 5.00 | ms/batch 73.08 | loss  3.49 | ppl    32.81\n",
            "| epoch   1 | 90400/389982 batches | lr 5.00 | ms/batch 67.91 | loss  3.49 | ppl    32.93\n",
            "| epoch   1 | 90600/389982 batches | lr 5.00 | ms/batch 66.82 | loss  3.53 | ppl    34.21\n",
            "| epoch   1 | 90800/389982 batches | lr 5.00 | ms/batch 68.93 | loss  3.41 | ppl    30.30\n",
            "| epoch   1 | 91000/389982 batches | lr 5.00 | ms/batch 67.93 | loss  3.48 | ppl    32.52\n",
            "| epoch   1 | 91200/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.56 | ppl    35.17\n",
            "| epoch   1 | 91400/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.51 | ppl    33.59\n",
            "| epoch   1 | 91600/389982 batches | lr 5.00 | ms/batch 66.93 | loss  3.56 | ppl    35.22\n",
            "| epoch   1 | 91800/389982 batches | lr 5.00 | ms/batch 66.46 | loss  3.56 | ppl    35.25\n",
            "| epoch   1 | 92000/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.52 | ppl    33.84\n",
            "| epoch   1 | 92200/389982 batches | lr 5.00 | ms/batch 72.31 | loss  3.54 | ppl    34.47\n",
            "| epoch   1 | 92400/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.52 | ppl    33.78\n",
            "| epoch   1 | 92600/389982 batches | lr 5.00 | ms/batch 68.53 | loss  3.65 | ppl    38.63\n",
            "| epoch   1 | 92800/389982 batches | lr 5.00 | ms/batch 67.97 | loss  3.65 | ppl    38.48\n",
            "| epoch   1 | 93000/389982 batches | lr 5.00 | ms/batch 68.38 | loss  3.56 | ppl    35.25\n",
            "| epoch   1 | 93200/389982 batches | lr 5.00 | ms/batch 67.17 | loss  3.76 | ppl    43.02\n",
            "| epoch   1 | 93400/389982 batches | lr 5.00 | ms/batch 69.90 | loss  3.57 | ppl    35.42\n",
            "| epoch   1 | 93600/389982 batches | lr 5.00 | ms/batch 69.57 | loss  3.42 | ppl    30.56\n",
            "| epoch   1 | 93800/389982 batches | lr 5.00 | ms/batch 67.65 | loss  3.54 | ppl    34.49\n",
            "| epoch   1 | 94000/389982 batches | lr 5.00 | ms/batch 67.00 | loss  3.54 | ppl    34.45\n",
            "| epoch   1 | 94200/389982 batches | lr 5.00 | ms/batch 71.87 | loss  3.50 | ppl    33.26\n",
            "| epoch   1 | 94400/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.57 | ppl    35.67\n",
            "| epoch   1 | 94600/389982 batches | lr 5.00 | ms/batch 68.86 | loss  3.49 | ppl    32.87\n",
            "| epoch   1 | 94800/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.46 | ppl    31.78\n",
            "| epoch   1 | 95000/389982 batches | lr 5.00 | ms/batch 66.39 | loss  3.68 | ppl    39.51\n",
            "| epoch   1 | 95200/389982 batches | lr 5.00 | ms/batch 68.13 | loss  3.47 | ppl    32.29\n",
            "| epoch   1 | 95400/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.47 | ppl    32.27\n",
            "| epoch   1 | 95600/389982 batches | lr 5.00 | ms/batch 67.29 | loss  3.49 | ppl    32.69\n",
            "| epoch   1 | 95800/389982 batches | lr 5.00 | ms/batch 66.06 | loss  3.60 | ppl    36.49\n",
            "| epoch   1 | 96000/389982 batches | lr 5.00 | ms/batch 68.25 | loss  3.48 | ppl    32.46\n",
            "| epoch   1 | 96200/389982 batches | lr 5.00 | ms/batch 72.17 | loss  3.64 | ppl    37.98\n",
            "| epoch   1 | 96400/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.58 | ppl    35.99\n",
            "| epoch   1 | 96600/389982 batches | lr 5.00 | ms/batch 68.35 | loss  3.47 | ppl    32.28\n",
            "| epoch   1 | 96800/389982 batches | lr 5.00 | ms/batch 67.23 | loss  3.48 | ppl    32.43\n",
            "| epoch   1 | 97000/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.46 | ppl    31.73\n",
            "| epoch   1 | 97200/389982 batches | lr 5.00 | ms/batch 66.61 | loss  3.60 | ppl    36.45\n",
            "| epoch   1 | 97400/389982 batches | lr 5.00 | ms/batch 68.57 | loss  3.51 | ppl    33.43\n",
            "| epoch   1 | 97600/389982 batches | lr 5.00 | ms/batch 69.53 | loss  3.45 | ppl    31.51\n",
            "| epoch   1 | 97800/389982 batches | lr 5.00 | ms/batch 67.48 | loss  3.55 | ppl    34.88\n",
            "| epoch   1 | 98000/389982 batches | lr 5.00 | ms/batch 67.60 | loss  3.50 | ppl    33.00\n",
            "| epoch   1 | 98200/389982 batches | lr 5.00 | ms/batch 72.58 | loss  3.53 | ppl    33.99\n",
            "| epoch   1 | 98400/389982 batches | lr 5.00 | ms/batch 69.05 | loss  3.44 | ppl    31.10\n",
            "| epoch   1 | 98600/389982 batches | lr 5.00 | ms/batch 68.13 | loss  3.47 | ppl    32.16\n",
            "| epoch   1 | 98800/389982 batches | lr 5.00 | ms/batch 69.34 | loss  3.51 | ppl    33.31\n",
            "| epoch   1 | 99000/389982 batches | lr 5.00 | ms/batch 68.82 | loss  3.54 | ppl    34.46\n",
            "| epoch   1 | 99200/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.47 | ppl    32.15\n",
            "| epoch   1 | 99400/389982 batches | lr 5.00 | ms/batch 67.15 | loss  3.56 | ppl    35.11\n",
            "| epoch   1 | 99600/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.43 | ppl    31.00\n",
            "| epoch   1 | 99800/389982 batches | lr 5.00 | ms/batch 67.80 | loss  3.50 | ppl    33.01\n",
            "| epoch   1 | 100000/389982 batches | lr 5.00 | ms/batch 68.51 | loss  3.43 | ppl    30.82\n",
            "| epoch   1 | 100200/389982 batches | lr 5.00 | ms/batch 74.23 | loss  3.46 | ppl    31.68\n",
            "| epoch   1 | 100400/389982 batches | lr 5.00 | ms/batch 69.08 | loss  3.49 | ppl    32.69\n",
            "| epoch   1 | 100600/389982 batches | lr 5.00 | ms/batch 67.64 | loss  3.50 | ppl    33.12\n",
            "| epoch   1 | 100800/389982 batches | lr 5.00 | ms/batch 67.32 | loss  3.51 | ppl    33.29\n",
            "| epoch   1 | 101000/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.53 | ppl    34.08\n",
            "| epoch   1 | 101200/389982 batches | lr 5.00 | ms/batch 68.33 | loss  3.51 | ppl    33.29\n",
            "| epoch   1 | 101400/389982 batches | lr 5.00 | ms/batch 68.19 | loss  3.51 | ppl    33.39\n",
            "| epoch   1 | 101600/389982 batches | lr 5.00 | ms/batch 68.56 | loss  3.66 | ppl    38.67\n",
            "| epoch   1 | 101800/389982 batches | lr 5.00 | ms/batch 67.19 | loss  3.60 | ppl    36.68\n",
            "| epoch   1 | 102000/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.57 | ppl    35.61\n",
            "| epoch   1 | 102200/389982 batches | lr 5.00 | ms/batch 72.59 | loss  3.53 | ppl    34.06\n",
            "| epoch   1 | 102400/389982 batches | lr 5.00 | ms/batch 69.55 | loss  3.46 | ppl    31.70\n",
            "| epoch   1 | 102600/389982 batches | lr 5.00 | ms/batch 68.11 | loss  3.57 | ppl    35.55\n",
            "| epoch   1 | 102800/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.51 | ppl    33.49\n",
            "| epoch   1 | 103000/389982 batches | lr 5.00 | ms/batch 69.60 | loss  3.43 | ppl    30.81\n",
            "| epoch   1 | 103200/389982 batches | lr 5.00 | ms/batch 66.44 | loss  3.61 | ppl    36.99\n",
            "| epoch   1 | 103400/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.57 | ppl    35.40\n",
            "| epoch   1 | 103600/389982 batches | lr 5.00 | ms/batch 68.53 | loss  3.49 | ppl    32.92\n",
            "| epoch   1 | 103800/389982 batches | lr 5.00 | ms/batch 68.43 | loss  3.47 | ppl    32.20\n",
            "| epoch   1 | 104000/389982 batches | lr 5.00 | ms/batch 67.17 | loss  3.58 | ppl    35.78\n",
            "| epoch   1 | 104200/389982 batches | lr 5.00 | ms/batch 73.96 | loss  3.47 | ppl    32.08\n",
            "| epoch   1 | 104400/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.49 | ppl    32.71\n",
            "| epoch   1 | 104600/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.57 | ppl    35.36\n",
            "| epoch   1 | 104800/389982 batches | lr 5.00 | ms/batch 66.21 | loss  3.61 | ppl    36.96\n",
            "| epoch   1 | 105000/389982 batches | lr 5.00 | ms/batch 68.10 | loss  3.57 | ppl    35.43\n",
            "| epoch   1 | 105200/389982 batches | lr 5.00 | ms/batch 69.07 | loss  3.63 | ppl    37.71\n",
            "| epoch   1 | 105400/389982 batches | lr 5.00 | ms/batch 67.60 | loss  3.50 | ppl    33.15\n",
            "| epoch   1 | 105600/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.53 | ppl    34.06\n",
            "| epoch   1 | 105800/389982 batches | lr 5.00 | ms/batch 66.81 | loss  3.63 | ppl    37.82\n",
            "| epoch   1 | 106000/389982 batches | lr 5.00 | ms/batch 68.17 | loss  3.45 | ppl    31.64\n",
            "| epoch   1 | 106200/389982 batches | lr 5.00 | ms/batch 73.35 | loss  3.45 | ppl    31.51\n",
            "| epoch   1 | 106400/389982 batches | lr 5.00 | ms/batch 67.31 | loss  3.53 | ppl    34.17\n",
            "| epoch   1 | 106600/389982 batches | lr 5.00 | ms/batch 68.26 | loss  3.50 | ppl    33.25\n",
            "| epoch   1 | 106800/389982 batches | lr 5.00 | ms/batch 69.28 | loss  3.49 | ppl    32.81\n",
            "| epoch   1 | 107000/389982 batches | lr 5.00 | ms/batch 66.99 | loss  3.54 | ppl    34.31\n",
            "| epoch   1 | 107200/389982 batches | lr 5.00 | ms/batch 66.67 | loss  3.54 | ppl    34.32\n",
            "| epoch   1 | 107400/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.49 | ppl    32.71\n",
            "| epoch   1 | 107600/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.47 | ppl    32.08\n",
            "| epoch   1 | 107800/389982 batches | lr 5.00 | ms/batch 66.46 | loss  3.59 | ppl    36.34\n",
            "| epoch   1 | 108000/389982 batches | lr 5.00 | ms/batch 66.75 | loss  3.52 | ppl    33.73\n",
            "| epoch   1 | 108200/389982 batches | lr 5.00 | ms/batch 73.02 | loss  3.58 | ppl    36.03\n",
            "| epoch   1 | 108400/389982 batches | lr 5.00 | ms/batch 66.58 | loss  3.59 | ppl    36.07\n",
            "| epoch   1 | 108600/389982 batches | lr 5.00 | ms/batch 68.91 | loss  3.53 | ppl    34.11\n",
            "| epoch   1 | 108800/389982 batches | lr 5.00 | ms/batch 68.21 | loss  3.50 | ppl    33.22\n",
            "| epoch   1 | 109000/389982 batches | lr 5.00 | ms/batch 67.57 | loss  3.59 | ppl    36.13\n",
            "| epoch   1 | 109200/389982 batches | lr 5.00 | ms/batch 69.08 | loss  3.48 | ppl    32.41\n",
            "| epoch   1 | 109400/389982 batches | lr 5.00 | ms/batch 67.21 | loss  3.49 | ppl    32.94\n",
            "| epoch   1 | 109600/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.49 | ppl    32.93\n",
            "| epoch   1 | 109800/389982 batches | lr 5.00 | ms/batch 66.81 | loss  3.55 | ppl    34.89\n",
            "| epoch   1 | 110000/389982 batches | lr 5.00 | ms/batch 68.75 | loss  3.49 | ppl    32.94\n",
            "| epoch   1 | 110200/389982 batches | lr 5.00 | ms/batch 73.60 | loss  3.49 | ppl    32.85\n",
            "| epoch   1 | 110400/389982 batches | lr 5.00 | ms/batch 68.62 | loss  3.45 | ppl    31.48\n",
            "| epoch   1 | 110600/389982 batches | lr 5.00 | ms/batch 67.34 | loss  3.52 | ppl    33.85\n",
            "| epoch   1 | 110800/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.48 | ppl    32.46\n",
            "| epoch   1 | 111000/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.47 | ppl    31.98\n",
            "| epoch   1 | 111200/389982 batches | lr 5.00 | ms/batch 66.88 | loss  3.56 | ppl    35.19\n",
            "| epoch   1 | 111400/389982 batches | lr 5.00 | ms/batch 67.68 | loss  3.48 | ppl    32.50\n",
            "| epoch   1 | 111600/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.50 | ppl    33.22\n",
            "| epoch   1 | 111800/389982 batches | lr 5.00 | ms/batch 67.54 | loss  3.51 | ppl    33.51\n",
            "| epoch   1 | 112000/389982 batches | lr 5.00 | ms/batch 66.48 | loss  3.54 | ppl    34.63\n",
            "| epoch   1 | 112200/389982 batches | lr 5.00 | ms/batch 74.05 | loss  3.48 | ppl    32.61\n",
            "| epoch   1 | 112400/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.57 | ppl    35.48\n",
            "| epoch   1 | 112600/389982 batches | lr 5.00 | ms/batch 68.44 | loss  3.51 | ppl    33.61\n",
            "| epoch   1 | 112800/389982 batches | lr 5.00 | ms/batch 68.32 | loss  3.52 | ppl    33.95\n",
            "| epoch   1 | 113000/389982 batches | lr 5.00 | ms/batch 68.55 | loss  3.54 | ppl    34.42\n",
            "| epoch   1 | 113200/389982 batches | lr 5.00 | ms/batch 67.01 | loss  3.54 | ppl    34.55\n",
            "| epoch   1 | 113400/389982 batches | lr 5.00 | ms/batch 67.47 | loss  3.49 | ppl    32.93\n",
            "| epoch   1 | 113600/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.47 | ppl    32.05\n",
            "| epoch   1 | 113800/389982 batches | lr 5.00 | ms/batch 68.66 | loss  3.70 | ppl    40.36\n",
            "| epoch   1 | 114000/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.46 | ppl    31.68\n",
            "| epoch   1 | 114200/389982 batches | lr 5.00 | ms/batch 72.66 | loss  3.51 | ppl    33.39\n",
            "| epoch   1 | 114400/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.62 | ppl    37.37\n",
            "| epoch   1 | 114600/389982 batches | lr 5.00 | ms/batch 67.43 | loss  3.53 | ppl    34.18\n",
            "| epoch   1 | 114800/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.49 | ppl    32.72\n",
            "| epoch   1 | 115000/389982 batches | lr 5.00 | ms/batch 69.46 | loss  3.44 | ppl    31.30\n",
            "| epoch   1 | 115200/389982 batches | lr 5.00 | ms/batch 69.63 | loss  3.49 | ppl    32.80\n",
            "| epoch   1 | 115400/389982 batches | lr 5.00 | ms/batch 68.05 | loss  3.62 | ppl    37.34\n",
            "| epoch   1 | 115600/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.60 | ppl    36.44\n",
            "| epoch   1 | 115800/389982 batches | lr 5.00 | ms/batch 67.44 | loss  3.54 | ppl    34.60\n",
            "| epoch   1 | 116000/389982 batches | lr 5.00 | ms/batch 68.09 | loss  3.54 | ppl    34.35\n",
            "| epoch   1 | 116200/389982 batches | lr 5.00 | ms/batch 71.98 | loss  3.52 | ppl    33.74\n",
            "| epoch   1 | 116400/389982 batches | lr 5.00 | ms/batch 68.59 | loss  3.49 | ppl    32.79\n",
            "| epoch   1 | 116600/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.47 | ppl    32.27\n",
            "| epoch   1 | 116800/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.50 | ppl    33.06\n",
            "| epoch   1 | 117000/389982 batches | lr 5.00 | ms/batch 67.15 | loss  3.52 | ppl    33.79\n",
            "| epoch   1 | 117200/389982 batches | lr 5.00 | ms/batch 68.05 | loss  3.48 | ppl    32.38\n",
            "| epoch   1 | 117400/389982 batches | lr 5.00 | ms/batch 68.95 | loss  3.54 | ppl    34.53\n",
            "| epoch   1 | 117600/389982 batches | lr 5.00 | ms/batch 68.74 | loss  3.48 | ppl    32.52\n",
            "| epoch   1 | 117800/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.49 | ppl    32.87\n",
            "| epoch   1 | 118000/389982 batches | lr 5.00 | ms/batch 68.65 | loss  3.46 | ppl    31.78\n",
            "| epoch   1 | 118200/389982 batches | lr 5.00 | ms/batch 73.73 | loss  3.44 | ppl    31.29\n",
            "| epoch   1 | 118400/389982 batches | lr 5.00 | ms/batch 70.43 | loss  3.51 | ppl    33.51\n",
            "| epoch   1 | 118600/389982 batches | lr 5.00 | ms/batch 66.75 | loss  3.53 | ppl    34.15\n",
            "| epoch   1 | 118800/389982 batches | lr 5.00 | ms/batch 68.63 | loss  3.50 | ppl    33.03\n",
            "| epoch   1 | 119000/389982 batches | lr 5.00 | ms/batch 67.16 | loss  3.53 | ppl    34.07\n",
            "| epoch   1 | 119200/389982 batches | lr 5.00 | ms/batch 68.66 | loss  3.54 | ppl    34.36\n",
            "| epoch   1 | 119400/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.55 | ppl    34.91\n",
            "| epoch   1 | 119600/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.64 | ppl    37.99\n",
            "| epoch   1 | 119800/389982 batches | lr 5.00 | ms/batch 67.55 | loss  3.59 | ppl    36.28\n",
            "| epoch   1 | 120000/389982 batches | lr 5.00 | ms/batch 68.13 | loss  3.50 | ppl    33.11\n",
            "| epoch   1 | 120200/389982 batches | lr 5.00 | ms/batch 72.97 | loss  3.54 | ppl    34.40\n",
            "| epoch   1 | 120400/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.50 | ppl    33.04\n",
            "| epoch   1 | 120600/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.56 | ppl    35.18\n",
            "| epoch   1 | 120800/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.68 | ppl    39.55\n",
            "| epoch   1 | 121000/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.48 | ppl    32.53\n",
            "| epoch   1 | 121200/389982 batches | lr 5.00 | ms/batch 67.10 | loss  3.51 | ppl    33.52\n",
            "| epoch   1 | 121400/389982 batches | lr 5.00 | ms/batch 68.83 | loss  3.49 | ppl    32.72\n",
            "| epoch   1 | 121600/389982 batches | lr 5.00 | ms/batch 68.98 | loss  3.51 | ppl    33.36\n",
            "| epoch   1 | 121800/389982 batches | lr 5.00 | ms/batch 68.93 | loss  3.51 | ppl    33.59\n",
            "| epoch   1 | 122000/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.52 | ppl    33.65\n",
            "| epoch   1 | 122200/389982 batches | lr 5.00 | ms/batch 73.08 | loss  3.47 | ppl    32.01\n",
            "| epoch   1 | 122400/389982 batches | lr 5.00 | ms/batch 66.57 | loss  3.66 | ppl    38.90\n",
            "| epoch   1 | 122600/389982 batches | lr 5.00 | ms/batch 67.28 | loss  3.52 | ppl    33.80\n",
            "| epoch   1 | 122800/389982 batches | lr 5.00 | ms/batch 67.88 | loss  3.48 | ppl    32.38\n",
            "| epoch   1 | 123000/389982 batches | lr 5.00 | ms/batch 67.23 | loss  3.52 | ppl    33.71\n",
            "| epoch   1 | 123200/389982 batches | lr 5.00 | ms/batch 67.67 | loss  3.54 | ppl    34.38\n",
            "| epoch   1 | 123400/389982 batches | lr 5.00 | ms/batch 68.49 | loss  3.45 | ppl    31.47\n",
            "| epoch   1 | 123600/389982 batches | lr 5.00 | ms/batch 68.65 | loss  3.58 | ppl    35.98\n",
            "| epoch   1 | 123800/389982 batches | lr 5.00 | ms/batch 68.31 | loss  3.53 | ppl    34.13\n",
            "| epoch   1 | 124000/389982 batches | lr 5.00 | ms/batch 66.14 | loss  3.58 | ppl    35.95\n",
            "| epoch   1 | 124200/389982 batches | lr 5.00 | ms/batch 72.37 | loss  3.53 | ppl    34.10\n",
            "| epoch   1 | 124400/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.50 | ppl    33.03\n",
            "| epoch   1 | 124600/389982 batches | lr 5.00 | ms/batch 67.80 | loss  3.58 | ppl    35.70\n",
            "| epoch   1 | 124800/389982 batches | lr 5.00 | ms/batch 66.81 | loss  3.50 | ppl    33.20\n",
            "| epoch   1 | 125000/389982 batches | lr 5.00 | ms/batch 67.07 | loss  3.51 | ppl    33.55\n",
            "| epoch   1 | 125200/389982 batches | lr 5.00 | ms/batch 67.19 | loss  3.60 | ppl    36.61\n",
            "| epoch   1 | 125400/389982 batches | lr 5.00 | ms/batch 66.57 | loss  3.53 | ppl    34.04\n",
            "| epoch   1 | 125600/389982 batches | lr 5.00 | ms/batch 67.39 | loss  3.49 | ppl    32.80\n",
            "| epoch   1 | 125800/389982 batches | lr 5.00 | ms/batch 67.15 | loss  3.53 | ppl    34.03\n",
            "| epoch   1 | 126000/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.62 | ppl    37.29\n",
            "| epoch   1 | 126200/389982 batches | lr 5.00 | ms/batch 72.60 | loss  3.48 | ppl    32.54\n",
            "| epoch   1 | 126400/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 126600/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.51 | ppl    33.31\n",
            "| epoch   1 | 126800/389982 batches | lr 5.00 | ms/batch 67.31 | loss  3.53 | ppl    33.97\n",
            "| epoch   1 | 127000/389982 batches | lr 5.00 | ms/batch 66.15 | loss  3.62 | ppl    37.45\n",
            "| epoch   1 | 127200/389982 batches | lr 5.00 | ms/batch 66.86 | loss  3.52 | ppl    33.86\n",
            "| epoch   1 | 127400/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.49 | ppl    32.72\n",
            "| epoch   1 | 127600/389982 batches | lr 5.00 | ms/batch 68.48 | loss  3.47 | ppl    32.07\n",
            "| epoch   1 | 127800/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.64 | ppl    37.97\n",
            "| epoch   1 | 128000/389982 batches | lr 5.00 | ms/batch 69.55 | loss  3.52 | ppl    33.80\n",
            "| epoch   1 | 128200/389982 batches | lr 5.00 | ms/batch 72.78 | loss  3.55 | ppl    34.75\n",
            "| epoch   1 | 128400/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.68 | ppl    39.47\n",
            "| epoch   1 | 128600/389982 batches | lr 5.00 | ms/batch 67.34 | loss  3.46 | ppl    31.70\n",
            "| epoch   1 | 128800/389982 batches | lr 5.00 | ms/batch 66.57 | loss  3.54 | ppl    34.39\n",
            "| epoch   1 | 129000/389982 batches | lr 5.00 | ms/batch 71.08 | loss  3.49 | ppl    32.73\n",
            "| epoch   1 | 129200/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.73 | ppl    41.66\n",
            "| epoch   1 | 129400/389982 batches | lr 5.00 | ms/batch 67.81 | loss  3.51 | ppl    33.39\n",
            "| epoch   1 | 129600/389982 batches | lr 5.00 | ms/batch 67.43 | loss  3.48 | ppl    32.47\n",
            "| epoch   1 | 129800/389982 batches | lr 5.00 | ms/batch 67.72 | loss  3.54 | ppl    34.36\n",
            "| epoch   1 | 130000/389982 batches | lr 5.00 | ms/batch 66.73 | loss  3.55 | ppl    34.86\n",
            "| epoch   1 | 130200/389982 batches | lr 5.00 | ms/batch 73.59 | loss  3.49 | ppl    32.81\n",
            "| epoch   1 | 130400/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.47 | ppl    32.14\n",
            "| epoch   1 | 130600/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.51 | ppl    33.38\n",
            "| epoch   1 | 130800/389982 batches | lr 5.00 | ms/batch 67.43 | loss  3.57 | ppl    35.65\n",
            "| epoch   1 | 131000/389982 batches | lr 5.00 | ms/batch 67.16 | loss  3.52 | ppl    33.77\n",
            "| epoch   1 | 131200/389982 batches | lr 5.00 | ms/batch 70.39 | loss  3.37 | ppl    29.22\n",
            "| epoch   1 | 131400/389982 batches | lr 5.00 | ms/batch 67.30 | loss  3.54 | ppl    34.43\n",
            "| epoch   1 | 131600/389982 batches | lr 5.00 | ms/batch 69.03 | loss  3.42 | ppl    30.62\n",
            "| epoch   1 | 131800/389982 batches | lr 5.00 | ms/batch 67.55 | loss  3.48 | ppl    32.61\n",
            "| epoch   1 | 132000/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.48 | ppl    32.44\n",
            "| epoch   1 | 132200/389982 batches | lr 5.00 | ms/batch 72.36 | loss  3.60 | ppl    36.69\n",
            "| epoch   1 | 132400/389982 batches | lr 5.00 | ms/batch 68.68 | loss  3.46 | ppl    31.72\n",
            "| epoch   1 | 132600/389982 batches | lr 5.00 | ms/batch 67.02 | loss  3.56 | ppl    35.03\n",
            "| epoch   1 | 132800/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.59 | ppl    36.28\n",
            "| epoch   1 | 133000/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.47 | ppl    32.14\n",
            "| epoch   1 | 133200/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.50 | ppl    33.19\n",
            "| epoch   1 | 133400/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.44 | ppl    31.13\n",
            "| epoch   1 | 133600/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.60 | ppl    36.67\n",
            "| epoch   1 | 133800/389982 batches | lr 5.00 | ms/batch 68.89 | loss  3.56 | ppl    35.08\n",
            "| epoch   1 | 134000/389982 batches | lr 5.00 | ms/batch 66.67 | loss  3.60 | ppl    36.59\n",
            "| epoch   1 | 134200/389982 batches | lr 5.00 | ms/batch 72.39 | loss  3.56 | ppl    35.08\n",
            "| epoch   1 | 134400/389982 batches | lr 5.00 | ms/batch 66.78 | loss  3.52 | ppl    33.92\n",
            "| epoch   1 | 134600/389982 batches | lr 5.00 | ms/batch 67.27 | loss  3.50 | ppl    33.17\n",
            "| epoch   1 | 134800/389982 batches | lr 5.00 | ms/batch 67.44 | loss  3.54 | ppl    34.33\n",
            "| epoch   1 | 135000/389982 batches | lr 5.00 | ms/batch 70.30 | loss  3.46 | ppl    31.89\n",
            "| epoch   1 | 135200/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.59 | ppl    36.25\n",
            "| epoch   1 | 135400/389982 batches | lr 5.00 | ms/batch 69.20 | loss  3.38 | ppl    29.31\n",
            "| epoch   1 | 135600/389982 batches | lr 5.00 | ms/batch 66.79 | loss  3.51 | ppl    33.61\n",
            "| epoch   1 | 135800/389982 batches | lr 5.00 | ms/batch 67.20 | loss  3.51 | ppl    33.41\n",
            "| epoch   1 | 136000/389982 batches | lr 5.00 | ms/batch 68.09 | loss  3.42 | ppl    30.56\n",
            "| epoch   1 | 136200/389982 batches | lr 5.00 | ms/batch 72.79 | loss  3.49 | ppl    32.68\n",
            "| epoch   1 | 136400/389982 batches | lr 5.00 | ms/batch 67.89 | loss  3.51 | ppl    33.29\n",
            "| epoch   1 | 136600/389982 batches | lr 5.00 | ms/batch 67.98 | loss  3.58 | ppl    35.85\n",
            "| epoch   1 | 136800/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.53 | ppl    34.07\n",
            "| epoch   1 | 137000/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.48 | ppl    32.57\n",
            "| epoch   1 | 137200/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.48 | ppl    32.60\n",
            "| epoch   1 | 137400/389982 batches | lr 5.00 | ms/batch 66.65 | loss  3.52 | ppl    33.87\n",
            "| epoch   1 | 137600/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.46 | ppl    31.75\n",
            "| epoch   1 | 137800/389982 batches | lr 5.00 | ms/batch 68.66 | loss  3.44 | ppl    31.26\n",
            "| epoch   1 | 138000/389982 batches | lr 5.00 | ms/batch 66.90 | loss  3.53 | ppl    34.03\n",
            "| epoch   1 | 138200/389982 batches | lr 5.00 | ms/batch 74.72 | loss  3.40 | ppl    29.89\n",
            "| epoch   1 | 138400/389982 batches | lr 5.00 | ms/batch 66.95 | loss  3.63 | ppl    37.66\n",
            "| epoch   1 | 138600/389982 batches | lr 5.00 | ms/batch 69.03 | loss  3.58 | ppl    35.91\n",
            "| epoch   1 | 138800/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.51 | ppl    33.56\n",
            "| epoch   1 | 139000/389982 batches | lr 5.00 | ms/batch 68.48 | loss  3.44 | ppl    31.13\n",
            "| epoch   1 | 139200/389982 batches | lr 5.00 | ms/batch 68.71 | loss  3.48 | ppl    32.44\n",
            "| epoch   1 | 139400/389982 batches | lr 5.00 | ms/batch 68.91 | loss  3.46 | ppl    31.73\n",
            "| epoch   1 | 139600/389982 batches | lr 5.00 | ms/batch 66.66 | loss  3.53 | ppl    34.26\n",
            "| epoch   1 | 139800/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.59 | ppl    36.33\n",
            "| epoch   1 | 140000/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.53 | ppl    34.27\n",
            "| epoch   1 | 140200/389982 batches | lr 5.00 | ms/batch 73.37 | loss  3.45 | ppl    31.56\n",
            "| epoch   1 | 140400/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.56 | ppl    35.25\n",
            "| epoch   1 | 140600/389982 batches | lr 5.00 | ms/batch 68.45 | loss  3.48 | ppl    32.31\n",
            "| epoch   1 | 140800/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.46 | ppl    31.81\n",
            "| epoch   1 | 141000/389982 batches | lr 5.00 | ms/batch 68.95 | loss  3.47 | ppl    32.01\n",
            "| epoch   1 | 141200/389982 batches | lr 5.00 | ms/batch 69.25 | loss  3.44 | ppl    31.21\n",
            "| epoch   1 | 141400/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.63 | ppl    37.60\n",
            "| epoch   1 | 141600/389982 batches | lr 5.00 | ms/batch 67.90 | loss  3.52 | ppl    33.92\n",
            "| epoch   1 | 141800/389982 batches | lr 5.00 | ms/batch 67.19 | loss  3.54 | ppl    34.41\n",
            "| epoch   1 | 142000/389982 batches | lr 5.00 | ms/batch 69.15 | loss  3.61 | ppl    37.14\n",
            "| epoch   1 | 142200/389982 batches | lr 5.00 | ms/batch 72.99 | loss  3.47 | ppl    32.25\n",
            "| epoch   1 | 142400/389982 batches | lr 5.00 | ms/batch 68.32 | loss  3.61 | ppl    36.93\n",
            "| epoch   1 | 142600/389982 batches | lr 5.00 | ms/batch 66.86 | loss  3.51 | ppl    33.31\n",
            "| epoch   1 | 142800/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.54 | ppl    34.55\n",
            "| epoch   1 | 143000/389982 batches | lr 5.00 | ms/batch 67.91 | loss  3.49 | ppl    32.73\n",
            "| epoch   1 | 143200/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.47 | ppl    32.17\n",
            "| epoch   1 | 143400/389982 batches | lr 5.00 | ms/batch 66.90 | loss  3.55 | ppl    34.87\n",
            "| epoch   1 | 143600/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.48 | ppl    32.44\n",
            "| epoch   1 | 143800/389982 batches | lr 5.00 | ms/batch 67.00 | loss  3.54 | ppl    34.40\n",
            "| epoch   1 | 144000/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.50 | ppl    33.07\n",
            "| epoch   1 | 144200/389982 batches | lr 5.00 | ms/batch 73.38 | loss  3.47 | ppl    32.18\n",
            "| epoch   1 | 144400/389982 batches | lr 5.00 | ms/batch 68.52 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 144600/389982 batches | lr 5.00 | ms/batch 67.08 | loss  3.47 | ppl    32.29\n",
            "| epoch   1 | 144800/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.52 | ppl    33.86\n",
            "| epoch   1 | 145000/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.54 | ppl    34.59\n",
            "| epoch   1 | 145200/389982 batches | lr 5.00 | ms/batch 67.75 | loss  3.48 | ppl    32.45\n",
            "| epoch   1 | 145400/389982 batches | lr 5.00 | ms/batch 67.27 | loss  3.53 | ppl    34.09\n",
            "| epoch   1 | 145600/389982 batches | lr 5.00 | ms/batch 70.19 | loss  3.40 | ppl    29.89\n",
            "| epoch   1 | 145800/389982 batches | lr 5.00 | ms/batch 67.29 | loss  3.52 | ppl    33.79\n",
            "| epoch   1 | 146000/389982 batches | lr 5.00 | ms/batch 66.89 | loss  3.52 | ppl    33.78\n",
            "| epoch   1 | 146200/389982 batches | lr 5.00 | ms/batch 72.28 | loss  3.51 | ppl    33.42\n",
            "| epoch   1 | 146400/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.55 | ppl    34.71\n",
            "| epoch   1 | 146600/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.51 | ppl    33.57\n",
            "| epoch   1 | 146800/389982 batches | lr 5.00 | ms/batch 70.00 | loss  3.65 | ppl    38.52\n",
            "| epoch   1 | 147000/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.54 | ppl    34.58\n",
            "| epoch   1 | 147200/389982 batches | lr 5.00 | ms/batch 67.91 | loss  3.49 | ppl    32.89\n",
            "| epoch   1 | 147400/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.57 | ppl    35.46\n",
            "| epoch   1 | 147600/389982 batches | lr 5.00 | ms/batch 67.84 | loss  3.75 | ppl    42.33\n",
            "| epoch   1 | 147800/389982 batches | lr 5.00 | ms/batch 69.38 | loss  3.64 | ppl    38.14\n",
            "| epoch   1 | 148000/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.48 | ppl    32.39\n",
            "| epoch   1 | 148200/389982 batches | lr 5.00 | ms/batch 72.43 | loss  3.51 | ppl    33.39\n",
            "| epoch   1 | 148400/389982 batches | lr 5.00 | ms/batch 66.72 | loss  3.54 | ppl    34.55\n",
            "| epoch   1 | 148600/389982 batches | lr 5.00 | ms/batch 66.82 | loss  3.58 | ppl    35.72\n",
            "| epoch   1 | 148800/389982 batches | lr 5.00 | ms/batch 69.11 | loss  3.39 | ppl    29.74\n",
            "| epoch   1 | 149000/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.49 | ppl    32.90\n",
            "| epoch   1 | 149200/389982 batches | lr 5.00 | ms/batch 68.10 | loss  3.50 | ppl    32.98\n",
            "| epoch   1 | 149400/389982 batches | lr 5.00 | ms/batch 66.65 | loss  3.52 | ppl    33.79\n",
            "| epoch   1 | 149600/389982 batches | lr 5.00 | ms/batch 68.44 | loss  3.45 | ppl    31.52\n",
            "| epoch   1 | 149800/389982 batches | lr 5.00 | ms/batch 68.44 | loss  3.70 | ppl    40.29\n",
            "| epoch   1 | 150000/389982 batches | lr 5.00 | ms/batch 68.13 | loss  3.59 | ppl    36.07\n",
            "| epoch   1 | 150200/389982 batches | lr 5.00 | ms/batch 77.66 | loss  3.51 | ppl    33.42\n",
            "| epoch   1 | 150400/389982 batches | lr 5.00 | ms/batch 68.88 | loss  3.40 | ppl    29.92\n",
            "| epoch   1 | 150600/389982 batches | lr 5.00 | ms/batch 68.10 | loss  3.47 | ppl    32.25\n",
            "| epoch   1 | 150800/389982 batches | lr 5.00 | ms/batch 70.20 | loss  3.41 | ppl    30.17\n",
            "| epoch   1 | 151000/389982 batches | lr 5.00 | ms/batch 66.87 | loss  3.51 | ppl    33.50\n",
            "| epoch   1 | 151200/389982 batches | lr 5.00 | ms/batch 68.83 | loss  3.48 | ppl    32.57\n",
            "| epoch   1 | 151400/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.50 | ppl    33.05\n",
            "| epoch   1 | 151600/389982 batches | lr 5.00 | ms/batch 67.08 | loss  3.48 | ppl    32.59\n",
            "| epoch   1 | 151800/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.45 | ppl    31.63\n",
            "| epoch   1 | 152000/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.52 | ppl    33.72\n",
            "| epoch   1 | 152200/389982 batches | lr 5.00 | ms/batch 71.86 | loss  3.54 | ppl    34.34\n",
            "| epoch   1 | 152400/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.48 | ppl    32.40\n",
            "| epoch   1 | 152600/389982 batches | lr 5.00 | ms/batch 68.68 | loss  3.47 | ppl    32.04\n",
            "| epoch   1 | 152800/389982 batches | lr 5.00 | ms/batch 68.43 | loss  3.49 | ppl    32.92\n",
            "| epoch   1 | 153000/389982 batches | lr 5.00 | ms/batch 67.37 | loss  3.53 | ppl    34.12\n",
            "| epoch   1 | 153200/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.47 | ppl    32.19\n",
            "| epoch   1 | 153400/389982 batches | lr 5.00 | ms/batch 69.52 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 153600/389982 batches | lr 5.00 | ms/batch 67.78 | loss  3.47 | ppl    31.99\n",
            "| epoch   1 | 153800/389982 batches | lr 5.00 | ms/batch 67.48 | loss  3.51 | ppl    33.51\n",
            "| epoch   1 | 154000/389982 batches | lr 5.00 | ms/batch 69.45 | loss  3.42 | ppl    30.48\n",
            "| epoch   1 | 154200/389982 batches | lr 5.00 | ms/batch 72.25 | loss  3.51 | ppl    33.46\n",
            "| epoch   1 | 154400/389982 batches | lr 5.00 | ms/batch 68.92 | loss  3.46 | ppl    31.93\n",
            "| epoch   1 | 154600/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.48 | ppl    32.32\n",
            "| epoch   1 | 154800/389982 batches | lr 5.00 | ms/batch 68.64 | loss  3.39 | ppl    29.81\n",
            "| epoch   1 | 155000/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.46 | ppl    31.72\n",
            "| epoch   1 | 155200/389982 batches | lr 5.00 | ms/batch 68.72 | loss  3.51 | ppl    33.33\n",
            "| epoch   1 | 155400/389982 batches | lr 5.00 | ms/batch 67.87 | loss  3.49 | ppl    32.81\n",
            "| epoch   1 | 155600/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.47 | ppl    31.98\n",
            "| epoch   1 | 155800/389982 batches | lr 5.00 | ms/batch 68.34 | loss  3.48 | ppl    32.42\n",
            "| epoch   1 | 156000/389982 batches | lr 5.00 | ms/batch 69.48 | loss  3.40 | ppl    30.09\n",
            "| epoch   1 | 156200/389982 batches | lr 5.00 | ms/batch 73.08 | loss  3.46 | ppl    31.75\n",
            "| epoch   1 | 156400/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.55 | ppl    34.90\n",
            "| epoch   1 | 156600/389982 batches | lr 5.00 | ms/batch 67.27 | loss  3.48 | ppl    32.30\n",
            "| epoch   1 | 156800/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.52 | ppl    33.64\n",
            "| epoch   1 | 157000/389982 batches | lr 5.00 | ms/batch 67.62 | loss  3.58 | ppl    35.94\n",
            "| epoch   1 | 157200/389982 batches | lr 5.00 | ms/batch 67.53 | loss  3.64 | ppl    38.28\n",
            "| epoch   1 | 157400/389982 batches | lr 5.00 | ms/batch 68.68 | loss  3.48 | ppl    32.43\n",
            "| epoch   1 | 157600/389982 batches | lr 5.00 | ms/batch 69.20 | loss  3.40 | ppl    30.09\n",
            "| epoch   1 | 157800/389982 batches | lr 5.00 | ms/batch 68.83 | loss  3.55 | ppl    34.80\n",
            "| epoch   1 | 158000/389982 batches | lr 5.00 | ms/batch 68.98 | loss  3.45 | ppl    31.54\n",
            "| epoch   1 | 158200/389982 batches | lr 5.00 | ms/batch 73.14 | loss  3.44 | ppl    31.15\n",
            "| epoch   1 | 158400/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.55 | ppl    34.66\n",
            "| epoch   1 | 158600/389982 batches | lr 5.00 | ms/batch 67.05 | loss  3.53 | ppl    34.02\n",
            "| epoch   1 | 158800/389982 batches | lr 5.00 | ms/batch 68.75 | loss  3.46 | ppl    31.75\n",
            "| epoch   1 | 159000/389982 batches | lr 5.00 | ms/batch 67.81 | loss  3.47 | ppl    32.19\n",
            "| epoch   1 | 159200/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.46 | ppl    31.97\n",
            "| epoch   1 | 159400/389982 batches | lr 5.00 | ms/batch 68.73 | loss  3.42 | ppl    30.61\n",
            "| epoch   1 | 159600/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.50 | ppl    33.20\n",
            "| epoch   1 | 159800/389982 batches | lr 5.00 | ms/batch 68.16 | loss  3.48 | ppl    32.62\n",
            "| epoch   1 | 160000/389982 batches | lr 5.00 | ms/batch 66.77 | loss  3.47 | ppl    32.29\n",
            "| epoch   1 | 160200/389982 batches | lr 5.00 | ms/batch 72.06 | loss  3.52 | ppl    33.81\n",
            "| epoch   1 | 160400/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.46 | ppl    31.96\n",
            "| epoch   1 | 160600/389982 batches | lr 5.00 | ms/batch 70.41 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 160800/389982 batches | lr 5.00 | ms/batch 68.54 | loss  3.46 | ppl    31.67\n",
            "| epoch   1 | 161000/389982 batches | lr 5.00 | ms/batch 67.09 | loss  3.51 | ppl    33.61\n",
            "| epoch   1 | 161200/389982 batches | lr 5.00 | ms/batch 66.69 | loss  3.49 | ppl    32.88\n",
            "| epoch   1 | 161400/389982 batches | lr 5.00 | ms/batch 67.01 | loss  3.63 | ppl    37.70\n",
            "| epoch   1 | 161600/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.50 | ppl    33.07\n",
            "| epoch   1 | 161800/389982 batches | lr 5.00 | ms/batch 68.01 | loss  3.79 | ppl    44.07\n",
            "| epoch   1 | 162000/389982 batches | lr 5.00 | ms/batch 68.68 | loss  3.69 | ppl    40.13\n",
            "| epoch   1 | 162200/389982 batches | lr 5.00 | ms/batch 72.10 | loss  3.71 | ppl    40.95\n",
            "| epoch   1 | 162400/389982 batches | lr 5.00 | ms/batch 67.10 | loss  3.47 | ppl    32.04\n",
            "| epoch   1 | 162600/389982 batches | lr 5.00 | ms/batch 68.23 | loss  3.52 | ppl    33.66\n",
            "| epoch   1 | 162800/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.57 | ppl    35.36\n",
            "| epoch   1 | 163000/389982 batches | lr 5.00 | ms/batch 68.92 | loss  3.51 | ppl    33.46\n",
            "| epoch   1 | 163200/389982 batches | lr 5.00 | ms/batch 67.02 | loss  3.51 | ppl    33.58\n",
            "| epoch   1 | 163400/389982 batches | lr 5.00 | ms/batch 68.73 | loss  3.45 | ppl    31.58\n",
            "| epoch   1 | 163600/389982 batches | lr 5.00 | ms/batch 67.87 | loss  3.47 | ppl    32.16\n",
            "| epoch   1 | 163800/389982 batches | lr 5.00 | ms/batch 68.05 | loss  3.45 | ppl    31.59\n",
            "| epoch   1 | 164000/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.46 | ppl    31.79\n",
            "| epoch   1 | 164200/389982 batches | lr 5.00 | ms/batch 73.55 | loss  3.49 | ppl    32.86\n",
            "| epoch   1 | 164400/389982 batches | lr 5.00 | ms/batch 69.42 | loss  3.43 | ppl    30.93\n",
            "| epoch   1 | 164600/389982 batches | lr 5.00 | ms/batch 69.16 | loss  3.39 | ppl    29.68\n",
            "| epoch   1 | 164800/389982 batches | lr 5.00 | ms/batch 68.24 | loss  3.46 | ppl    31.74\n",
            "| epoch   1 | 165000/389982 batches | lr 5.00 | ms/batch 68.39 | loss  3.43 | ppl    30.77\n",
            "| epoch   1 | 165200/389982 batches | lr 5.00 | ms/batch 68.23 | loss  3.40 | ppl    30.02\n",
            "| epoch   1 | 165400/389982 batches | lr 5.00 | ms/batch 67.30 | loss  3.54 | ppl    34.41\n",
            "| epoch   1 | 165600/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.45 | ppl    31.51\n",
            "| epoch   1 | 165800/389982 batches | lr 5.00 | ms/batch 68.09 | loss  3.55 | ppl    34.96\n",
            "| epoch   1 | 166000/389982 batches | lr 5.00 | ms/batch 69.25 | loss  3.57 | ppl    35.39\n",
            "| epoch   1 | 166200/389982 batches | lr 5.00 | ms/batch 72.80 | loss  3.58 | ppl    35.72\n",
            "| epoch   1 | 166400/389982 batches | lr 5.00 | ms/batch 68.16 | loss  3.47 | ppl    32.04\n",
            "| epoch   1 | 166600/389982 batches | lr 5.00 | ms/batch 69.23 | loss  3.42 | ppl    30.59\n",
            "| epoch   1 | 166800/389982 batches | lr 5.00 | ms/batch 68.13 | loss  3.47 | ppl    32.08\n",
            "| epoch   1 | 167000/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.49 | ppl    32.65\n",
            "| epoch   1 | 167200/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.43 | ppl    30.92\n",
            "| epoch   1 | 167400/389982 batches | lr 5.00 | ms/batch 68.06 | loss  3.44 | ppl    31.17\n",
            "| epoch   1 | 167600/389982 batches | lr 5.00 | ms/batch 68.95 | loss  3.41 | ppl    30.20\n",
            "| epoch   1 | 167800/389982 batches | lr 5.00 | ms/batch 67.20 | loss  3.52 | ppl    33.94\n",
            "| epoch   1 | 168000/389982 batches | lr 5.00 | ms/batch 68.07 | loss  3.48 | ppl    32.38\n",
            "| epoch   1 | 168200/389982 batches | lr 5.00 | ms/batch 72.22 | loss  3.52 | ppl    33.80\n",
            "| epoch   1 | 168400/389982 batches | lr 5.00 | ms/batch 67.65 | loss  3.52 | ppl    33.86\n",
            "| epoch   1 | 168600/389982 batches | lr 5.00 | ms/batch 67.57 | loss  3.73 | ppl    41.78\n",
            "| epoch   1 | 168800/389982 batches | lr 5.00 | ms/batch 68.32 | loss  3.65 | ppl    38.43\n",
            "| epoch   1 | 169000/389982 batches | lr 5.00 | ms/batch 68.21 | loss  3.46 | ppl    31.71\n",
            "| epoch   1 | 169200/389982 batches | lr 5.00 | ms/batch 68.13 | loss  3.45 | ppl    31.45\n",
            "| epoch   1 | 169400/389982 batches | lr 5.00 | ms/batch 67.79 | loss  3.51 | ppl    33.51\n",
            "| epoch   1 | 169600/389982 batches | lr 5.00 | ms/batch 68.19 | loss  3.59 | ppl    36.24\n",
            "| epoch   1 | 169800/389982 batches | lr 5.00 | ms/batch 67.67 | loss  3.69 | ppl    40.19\n",
            "| epoch   1 | 170000/389982 batches | lr 5.00 | ms/batch 67.50 | loss  3.47 | ppl    32.15\n",
            "| epoch   1 | 170200/389982 batches | lr 5.00 | ms/batch 73.44 | loss  3.45 | ppl    31.61\n",
            "| epoch   1 | 170400/389982 batches | lr 5.00 | ms/batch 67.19 | loss  3.49 | ppl    32.78\n",
            "| epoch   1 | 170600/389982 batches | lr 5.00 | ms/batch 69.00 | loss  3.50 | ppl    33.21\n",
            "| epoch   1 | 170800/389982 batches | lr 5.00 | ms/batch 67.43 | loss  3.60 | ppl    36.65\n",
            "| epoch   1 | 171000/389982 batches | lr 5.00 | ms/batch 67.50 | loss  3.48 | ppl    32.62\n",
            "| epoch   1 | 171200/389982 batches | lr 5.00 | ms/batch 68.28 | loss  3.51 | ppl    33.31\n",
            "| epoch   1 | 171400/389982 batches | lr 5.00 | ms/batch 67.98 | loss  3.45 | ppl    31.47\n",
            "| epoch   1 | 171600/389982 batches | lr 5.00 | ms/batch 69.11 | loss  3.42 | ppl    30.59\n",
            "| epoch   1 | 171800/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.48 | ppl    32.60\n",
            "| epoch   1 | 172000/389982 batches | lr 5.00 | ms/batch 66.81 | loss  3.52 | ppl    33.63\n",
            "| epoch   1 | 172200/389982 batches | lr 5.00 | ms/batch 72.73 | loss  3.47 | ppl    32.06\n",
            "| epoch   1 | 172400/389982 batches | lr 5.00 | ms/batch 68.27 | loss  3.46 | ppl    31.93\n",
            "| epoch   1 | 172600/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.46 | ppl    31.84\n",
            "| epoch   1 | 172800/389982 batches | lr 5.00 | ms/batch 67.60 | loss  3.49 | ppl    32.78\n",
            "| epoch   1 | 173000/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.49 | ppl    32.84\n",
            "| epoch   1 | 173200/389982 batches | lr 5.00 | ms/batch 68.51 | loss  3.45 | ppl    31.64\n",
            "| epoch   1 | 173400/389982 batches | lr 5.00 | ms/batch 67.01 | loss  3.50 | ppl    33.00\n",
            "| epoch   1 | 173600/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.46 | ppl    31.68\n",
            "| epoch   1 | 173800/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.45 | ppl    31.58\n",
            "| epoch   1 | 174000/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.42 | ppl    30.58\n",
            "| epoch   1 | 174200/389982 batches | lr 5.00 | ms/batch 73.06 | loss  3.44 | ppl    31.20\n",
            "| epoch   1 | 174400/389982 batches | lr 5.00 | ms/batch 68.06 | loss  3.47 | ppl    32.05\n",
            "| epoch   1 | 174600/389982 batches | lr 5.00 | ms/batch 68.92 | loss  3.46 | ppl    31.94\n",
            "| epoch   1 | 174800/389982 batches | lr 5.00 | ms/batch 67.06 | loss  3.50 | ppl    33.19\n",
            "| epoch   1 | 175000/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.45 | ppl    31.55\n",
            "| epoch   1 | 175200/389982 batches | lr 5.00 | ms/batch 68.06 | loss  3.42 | ppl    30.64\n",
            "| epoch   1 | 175400/389982 batches | lr 5.00 | ms/batch 67.75 | loss  3.47 | ppl    32.15\n",
            "| epoch   1 | 175600/389982 batches | lr 5.00 | ms/batch 67.84 | loss  3.47 | ppl    32.18\n",
            "| epoch   1 | 175800/389982 batches | lr 5.00 | ms/batch 66.79 | loss  3.51 | ppl    33.59\n",
            "| epoch   1 | 176000/389982 batches | lr 5.00 | ms/batch 68.44 | loss  3.47 | ppl    32.03\n",
            "| epoch   1 | 176200/389982 batches | lr 5.00 | ms/batch 71.26 | loss  3.54 | ppl    34.62\n",
            "| epoch   1 | 176400/389982 batches | lr 5.00 | ms/batch 68.21 | loss  3.43 | ppl    30.99\n",
            "| epoch   1 | 176600/389982 batches | lr 5.00 | ms/batch 67.27 | loss  3.47 | ppl    32.15\n",
            "| epoch   1 | 176800/389982 batches | lr 5.00 | ms/batch 67.02 | loss  3.47 | ppl    32.07\n",
            "| epoch   1 | 177000/389982 batches | lr 5.00 | ms/batch 68.34 | loss  3.53 | ppl    33.99\n",
            "| epoch   1 | 177200/389982 batches | lr 5.00 | ms/batch 69.88 | loss  3.34 | ppl    28.26\n",
            "| epoch   1 | 177400/389982 batches | lr 5.00 | ms/batch 69.49 | loss  3.40 | ppl    29.92\n",
            "| epoch   1 | 177600/389982 batches | lr 5.00 | ms/batch 68.27 | loss  3.49 | ppl    32.65\n",
            "| epoch   1 | 177800/389982 batches | lr 5.00 | ms/batch 67.47 | loss  3.63 | ppl    37.58\n",
            "| epoch   1 | 178000/389982 batches | lr 5.00 | ms/batch 68.12 | loss  3.41 | ppl    30.42\n",
            "| epoch   1 | 178200/389982 batches | lr 5.00 | ms/batch 73.94 | loss  3.51 | ppl    33.44\n",
            "| epoch   1 | 178400/389982 batches | lr 5.00 | ms/batch 68.62 | loss  3.49 | ppl    32.95\n",
            "| epoch   1 | 178600/389982 batches | lr 5.00 | ms/batch 67.21 | loss  3.62 | ppl    37.50\n",
            "| epoch   1 | 178800/389982 batches | lr 5.00 | ms/batch 68.47 | loss  3.46 | ppl    31.89\n",
            "| epoch   1 | 179000/389982 batches | lr 5.00 | ms/batch 69.30 | loss  3.38 | ppl    29.29\n",
            "| epoch   1 | 179200/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.41 | ppl    30.32\n",
            "| epoch   1 | 179400/389982 batches | lr 5.00 | ms/batch 68.07 | loss  3.46 | ppl    31.76\n",
            "| epoch   1 | 179600/389982 batches | lr 5.00 | ms/batch 68.55 | loss  3.46 | ppl    31.78\n",
            "| epoch   1 | 179800/389982 batches | lr 5.00 | ms/batch 68.62 | loss  3.43 | ppl    30.95\n",
            "| epoch   1 | 180000/389982 batches | lr 5.00 | ms/batch 67.31 | loss  3.44 | ppl    31.34\n",
            "| epoch   1 | 180200/389982 batches | lr 5.00 | ms/batch 72.38 | loss  3.51 | ppl    33.30\n",
            "| epoch   1 | 180400/389982 batches | lr 5.00 | ms/batch 66.84 | loss  3.51 | ppl    33.56\n",
            "| epoch   1 | 180600/389982 batches | lr 5.00 | ms/batch 68.71 | loss  3.39 | ppl    29.60\n",
            "| epoch   1 | 180800/389982 batches | lr 5.00 | ms/batch 65.86 | loss  3.54 | ppl    34.51\n",
            "| epoch   1 | 181000/389982 batches | lr 5.00 | ms/batch 68.86 | loss  3.42 | ppl    30.50\n",
            "| epoch   1 | 181200/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.48 | ppl    32.53\n",
            "| epoch   1 | 181400/389982 batches | lr 5.00 | ms/batch 67.06 | loss  3.49 | ppl    32.79\n",
            "| epoch   1 | 181600/389982 batches | lr 5.00 | ms/batch 66.67 | loss  3.52 | ppl    33.66\n",
            "| epoch   1 | 181800/389982 batches | lr 5.00 | ms/batch 68.44 | loss  3.48 | ppl    32.52\n",
            "| epoch   1 | 182000/389982 batches | lr 5.00 | ms/batch 68.68 | loss  3.46 | ppl    31.89\n",
            "| epoch   1 | 182200/389982 batches | lr 5.00 | ms/batch 71.80 | loss  3.46 | ppl    31.83\n",
            "| epoch   1 | 182400/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.46 | ppl    31.80\n",
            "| epoch   1 | 182600/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.49 | ppl    32.64\n",
            "| epoch   1 | 182800/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.47 | ppl    32.04\n",
            "| epoch   1 | 183000/389982 batches | lr 5.00 | ms/batch 68.09 | loss  3.45 | ppl    31.43\n",
            "| epoch   1 | 183200/389982 batches | lr 5.00 | ms/batch 66.42 | loss  3.58 | ppl    35.72\n",
            "| epoch   1 | 183400/389982 batches | lr 5.00 | ms/batch 68.75 | loss  3.41 | ppl    30.14\n",
            "| epoch   1 | 183600/389982 batches | lr 5.00 | ms/batch 70.14 | loss  3.41 | ppl    30.30\n",
            "| epoch   1 | 183800/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.47 | ppl    32.19\n",
            "| epoch   1 | 184000/389982 batches | lr 5.00 | ms/batch 67.89 | loss  3.58 | ppl    35.89\n",
            "| epoch   1 | 184200/389982 batches | lr 5.00 | ms/batch 72.44 | loss  3.55 | ppl    34.80\n",
            "| epoch   1 | 184400/389982 batches | lr 5.00 | ms/batch 68.54 | loss  3.51 | ppl    33.47\n",
            "| epoch   1 | 184600/389982 batches | lr 5.00 | ms/batch 67.84 | loss  3.47 | ppl    32.15\n",
            "| epoch   1 | 184800/389982 batches | lr 5.00 | ms/batch 69.14 | loss  3.45 | ppl    31.48\n",
            "| epoch   1 | 185000/389982 batches | lr 5.00 | ms/batch 66.15 | loss  3.56 | ppl    35.10\n",
            "| epoch   1 | 185200/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.46 | ppl    31.86\n",
            "| epoch   1 | 185400/389982 batches | lr 5.00 | ms/batch 68.45 | loss  3.44 | ppl    31.26\n",
            "| epoch   1 | 185600/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.44 | ppl    31.12\n",
            "| epoch   1 | 185800/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.46 | ppl    31.78\n",
            "| epoch   1 | 186000/389982 batches | lr 5.00 | ms/batch 67.09 | loss  3.50 | ppl    33.24\n",
            "| epoch   1 | 186200/389982 batches | lr 5.00 | ms/batch 72.36 | loss  3.49 | ppl    32.70\n",
            "| epoch   1 | 186400/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.51 | ppl    33.61\n",
            "| epoch   1 | 186600/389982 batches | lr 5.00 | ms/batch 68.40 | loss  3.42 | ppl    30.69\n",
            "| epoch   1 | 186800/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.53 | ppl    34.00\n",
            "| epoch   1 | 187000/389982 batches | lr 5.00 | ms/batch 67.72 | loss  3.50 | ppl    33.25\n",
            "| epoch   1 | 187200/389982 batches | lr 5.00 | ms/batch 66.66 | loss  3.54 | ppl    34.46\n",
            "| epoch   1 | 187400/389982 batches | lr 5.00 | ms/batch 68.05 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 187600/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.44 | ppl    31.16\n",
            "| epoch   1 | 187800/389982 batches | lr 5.00 | ms/batch 67.02 | loss  3.57 | ppl    35.57\n",
            "| epoch   1 | 188000/389982 batches | lr 5.00 | ms/batch 66.64 | loss  3.57 | ppl    35.65\n",
            "| epoch   1 | 188200/389982 batches | lr 5.00 | ms/batch 75.16 | loss  3.35 | ppl    28.44\n",
            "| epoch   1 | 188400/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.66 | ppl    39.00\n",
            "| epoch   1 | 188600/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.56 | ppl    35.24\n",
            "| epoch   1 | 188800/389982 batches | lr 5.00 | ms/batch 67.10 | loss  3.56 | ppl    35.08\n",
            "| epoch   1 | 189000/389982 batches | lr 5.00 | ms/batch 68.71 | loss  3.45 | ppl    31.45\n",
            "| epoch   1 | 189200/389982 batches | lr 5.00 | ms/batch 68.27 | loss  3.45 | ppl    31.43\n",
            "| epoch   1 | 189400/389982 batches | lr 5.00 | ms/batch 67.98 | loss  3.50 | ppl    32.95\n",
            "| epoch   1 | 189600/389982 batches | lr 5.00 | ms/batch 68.38 | loss  3.44 | ppl    31.12\n",
            "| epoch   1 | 189800/389982 batches | lr 5.00 | ms/batch 68.17 | loss  3.47 | ppl    32.23\n",
            "| epoch   1 | 190000/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.51 | ppl    33.30\n",
            "| epoch   1 | 190200/389982 batches | lr 5.00 | ms/batch 73.72 | loss  3.42 | ppl    30.62\n",
            "| epoch   1 | 190400/389982 batches | lr 5.00 | ms/batch 68.11 | loss  3.46 | ppl    31.97\n",
            "| epoch   1 | 190600/389982 batches | lr 5.00 | ms/batch 69.46 | loss  3.38 | ppl    29.30\n",
            "| epoch   1 | 190800/389982 batches | lr 5.00 | ms/batch 68.97 | loss  3.43 | ppl    30.80\n",
            "| epoch   1 | 191000/389982 batches | lr 5.00 | ms/batch 68.53 | loss  3.46 | ppl    31.74\n",
            "| epoch   1 | 191200/389982 batches | lr 5.00 | ms/batch 67.78 | loss  3.43 | ppl    30.95\n",
            "| epoch   1 | 191400/389982 batches | lr 5.00 | ms/batch 68.69 | loss  3.48 | ppl    32.54\n",
            "| epoch   1 | 191600/389982 batches | lr 5.00 | ms/batch 67.30 | loss  3.53 | ppl    34.19\n",
            "| epoch   1 | 191800/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.44 | ppl    31.03\n",
            "| epoch   1 | 192000/389982 batches | lr 5.00 | ms/batch 68.83 | loss  3.51 | ppl    33.31\n",
            "| epoch   1 | 192200/389982 batches | lr 5.00 | ms/batch 74.20 | loss  3.46 | ppl    31.82\n",
            "| epoch   1 | 192400/389982 batches | lr 5.00 | ms/batch 68.82 | loss  3.39 | ppl    29.63\n",
            "| epoch   1 | 192600/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.50 | ppl    33.05\n",
            "| epoch   1 | 192800/389982 batches | lr 5.00 | ms/batch 66.84 | loss  3.51 | ppl    33.55\n",
            "| epoch   1 | 193000/389982 batches | lr 5.00 | ms/batch 66.10 | loss  3.54 | ppl    34.58\n",
            "| epoch   1 | 193200/389982 batches | lr 5.00 | ms/batch 67.18 | loss  3.48 | ppl    32.31\n",
            "| epoch   1 | 193400/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.48 | ppl    32.39\n",
            "| epoch   1 | 193600/389982 batches | lr 5.00 | ms/batch 66.90 | loss  3.48 | ppl    32.62\n",
            "| epoch   1 | 193800/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.51 | ppl    33.55\n",
            "| epoch   1 | 194000/389982 batches | lr 5.00 | ms/batch 67.28 | loss  3.44 | ppl    31.33\n",
            "| epoch   1 | 194200/389982 batches | lr 5.00 | ms/batch 72.59 | loss  3.44 | ppl    31.05\n",
            "| epoch   1 | 194400/389982 batches | lr 5.00 | ms/batch 67.81 | loss  3.55 | ppl    34.75\n",
            "| epoch   1 | 194600/389982 batches | lr 5.00 | ms/batch 66.42 | loss  3.51 | ppl    33.33\n",
            "| epoch   1 | 194800/389982 batches | lr 5.00 | ms/batch 69.27 | loss  3.43 | ppl    30.90\n",
            "| epoch   1 | 195000/389982 batches | lr 5.00 | ms/batch 68.61 | loss  3.46 | ppl    31.92\n",
            "| epoch   1 | 195200/389982 batches | lr 5.00 | ms/batch 67.31 | loss  3.50 | ppl    33.16\n",
            "| epoch   1 | 195400/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.41 | ppl    30.29\n",
            "| epoch   1 | 195600/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.47 | ppl    32.23\n",
            "| epoch   1 | 195800/389982 batches | lr 5.00 | ms/batch 71.57 | loss  3.49 | ppl    32.80\n",
            "| epoch   1 | 196000/389982 batches | lr 5.00 | ms/batch 69.54 | loss  3.53 | ppl    34.17\n",
            "| epoch   1 | 196200/389982 batches | lr 5.00 | ms/batch 73.85 | loss  3.42 | ppl    30.67\n",
            "| epoch   1 | 196400/389982 batches | lr 5.00 | ms/batch 67.90 | loss  3.47 | ppl    32.14\n",
            "| epoch   1 | 196600/389982 batches | lr 5.00 | ms/batch 69.62 | loss  3.44 | ppl    31.15\n",
            "| epoch   1 | 196800/389982 batches | lr 5.00 | ms/batch 67.15 | loss  3.48 | ppl    32.60\n",
            "| epoch   1 | 197000/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.46 | ppl    31.87\n",
            "| epoch   1 | 197200/389982 batches | lr 5.00 | ms/batch 69.57 | loss  3.54 | ppl    34.48\n",
            "| epoch   1 | 197400/389982 batches | lr 5.00 | ms/batch 66.76 | loss  3.68 | ppl    39.84\n",
            "| epoch   1 | 197600/389982 batches | lr 5.00 | ms/batch 66.51 | loss  3.52 | ppl    33.62\n",
            "| epoch   1 | 197800/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.57 | ppl    35.41\n",
            "| epoch   1 | 198000/389982 batches | lr 5.00 | ms/batch 67.91 | loss  3.45 | ppl    31.46\n",
            "| epoch   1 | 198200/389982 batches | lr 5.00 | ms/batch 71.31 | loss  3.50 | ppl    33.11\n",
            "| epoch   1 | 198400/389982 batches | lr 5.00 | ms/batch 67.47 | loss  3.46 | ppl    31.78\n",
            "| epoch   1 | 198600/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.46 | ppl    31.66\n",
            "| epoch   1 | 198800/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.50 | ppl    33.18\n",
            "| epoch   1 | 199000/389982 batches | lr 5.00 | ms/batch 68.21 | loss  3.61 | ppl    36.86\n",
            "| epoch   1 | 199200/389982 batches | lr 5.00 | ms/batch 68.72 | loss  3.60 | ppl    36.50\n",
            "| epoch   1 | 199400/389982 batches | lr 5.00 | ms/batch 68.63 | loss  3.48 | ppl    32.53\n",
            "| epoch   1 | 199600/389982 batches | lr 5.00 | ms/batch 66.66 | loss  3.54 | ppl    34.31\n",
            "| epoch   1 | 199800/389982 batches | lr 5.00 | ms/batch 68.32 | loss  3.43 | ppl    30.75\n",
            "| epoch   1 | 200000/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.48 | ppl    32.51\n",
            "| epoch   1 | 200200/389982 batches | lr 5.00 | ms/batch 74.13 | loss  3.39 | ppl    29.62\n",
            "| epoch   1 | 200400/389982 batches | lr 5.00 | ms/batch 68.98 | loss  3.43 | ppl    30.88\n",
            "| epoch   1 | 200600/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.43 | ppl    30.82\n",
            "| epoch   1 | 200800/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.44 | ppl    31.24\n",
            "| epoch   1 | 201000/389982 batches | lr 5.00 | ms/batch 67.39 | loss  3.50 | ppl    33.22\n",
            "| epoch   1 | 201200/389982 batches | lr 5.00 | ms/batch 66.98 | loss  3.53 | ppl    34.19\n",
            "| epoch   1 | 201400/389982 batches | lr 5.00 | ms/batch 68.44 | loss  3.47 | ppl    32.21\n",
            "| epoch   1 | 201600/389982 batches | lr 5.00 | ms/batch 69.42 | loss  3.40 | ppl    29.99\n",
            "| epoch   1 | 201800/389982 batches | lr 5.00 | ms/batch 67.00 | loss  3.47 | ppl    32.03\n",
            "| epoch   1 | 202000/389982 batches | lr 5.00 | ms/batch 68.74 | loss  3.45 | ppl    31.65\n",
            "| epoch   1 | 202200/389982 batches | lr 5.00 | ms/batch 77.28 | loss  3.47 | ppl    32.27\n",
            "| epoch   1 | 202400/389982 batches | lr 5.00 | ms/batch 67.19 | loss  3.62 | ppl    37.50\n",
            "| epoch   1 | 202600/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.46 | ppl    31.80\n",
            "| epoch   1 | 202800/389982 batches | lr 5.00 | ms/batch 67.28 | loss  3.49 | ppl    32.70\n",
            "| epoch   1 | 203000/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.50 | ppl    32.96\n",
            "| epoch   1 | 203200/389982 batches | lr 5.00 | ms/batch 68.64 | loss  3.44 | ppl    31.16\n",
            "| epoch   1 | 203400/389982 batches | lr 5.00 | ms/batch 68.19 | loss  3.42 | ppl    30.58\n",
            "| epoch   1 | 203600/389982 batches | lr 5.00 | ms/batch 67.14 | loss  3.49 | ppl    32.63\n",
            "| epoch   1 | 203800/389982 batches | lr 5.00 | ms/batch 67.79 | loss  3.42 | ppl    30.54\n",
            "| epoch   1 | 204000/389982 batches | lr 5.00 | ms/batch 68.89 | loss  3.42 | ppl    30.61\n",
            "| epoch   1 | 204200/389982 batches | lr 5.00 | ms/batch 73.47 | loss  3.46 | ppl    31.94\n",
            "| epoch   1 | 204400/389982 batches | lr 5.00 | ms/batch 67.53 | loss  3.50 | ppl    33.01\n",
            "| epoch   1 | 204600/389982 batches | lr 5.00 | ms/batch 68.48 | loss  3.41 | ppl    30.31\n",
            "| epoch   1 | 204800/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.51 | ppl    33.35\n",
            "| epoch   1 | 205000/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.47 | ppl    32.29\n",
            "| epoch   1 | 205200/389982 batches | lr 5.00 | ms/batch 67.75 | loss  3.55 | ppl    34.86\n",
            "| epoch   1 | 205400/389982 batches | lr 5.00 | ms/batch 66.13 | loss  3.48 | ppl    32.36\n",
            "| epoch   1 | 205600/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.42 | ppl    30.55\n",
            "| epoch   1 | 205800/389982 batches | lr 5.00 | ms/batch 67.58 | loss  3.46 | ppl    31.88\n",
            "| epoch   1 | 206000/389982 batches | lr 5.00 | ms/batch 69.30 | loss  3.51 | ppl    33.34\n",
            "| epoch   1 | 206200/389982 batches | lr 5.00 | ms/batch 73.14 | loss  3.75 | ppl    42.69\n",
            "| epoch   1 | 206400/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.50 | ppl    33.25\n",
            "| epoch   1 | 206600/389982 batches | lr 5.00 | ms/batch 68.69 | loss  3.39 | ppl    29.72\n",
            "| epoch   1 | 206800/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.46 | ppl    31.68\n",
            "| epoch   1 | 207000/389982 batches | lr 5.00 | ms/batch 67.34 | loss  3.59 | ppl    36.41\n",
            "| epoch   1 | 207200/389982 batches | lr 5.00 | ms/batch 68.78 | loss  3.40 | ppl    30.09\n",
            "| epoch   1 | 207400/389982 batches | lr 5.00 | ms/batch 67.55 | loss  3.48 | ppl    32.50\n",
            "| epoch   1 | 207600/389982 batches | lr 5.00 | ms/batch 67.34 | loss  3.46 | ppl    31.76\n",
            "| epoch   1 | 207800/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.51 | ppl    33.58\n",
            "| epoch   1 | 208000/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.56 | ppl    35.15\n",
            "| epoch   1 | 208200/389982 batches | lr 5.00 | ms/batch 72.97 | loss  3.42 | ppl    30.67\n",
            "| epoch   1 | 208400/389982 batches | lr 5.00 | ms/batch 68.45 | loss  3.46 | ppl    31.97\n",
            "| epoch   1 | 208600/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.52 | ppl    33.67\n",
            "| epoch   1 | 208800/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.65 | ppl    38.37\n",
            "| epoch   1 | 209000/389982 batches | lr 5.00 | ms/batch 66.96 | loss  3.53 | ppl    34.02\n",
            "| epoch   1 | 209200/389982 batches | lr 5.00 | ms/batch 69.66 | loss  3.49 | ppl    32.94\n",
            "| epoch   1 | 209400/389982 batches | lr 5.00 | ms/batch 70.01 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 209600/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.52 | ppl    33.68\n",
            "| epoch   1 | 209800/389982 batches | lr 5.00 | ms/batch 67.39 | loss  3.44 | ppl    31.28\n",
            "| epoch   1 | 210000/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.43 | ppl    30.97\n",
            "| epoch   1 | 210200/389982 batches | lr 5.00 | ms/batch 71.65 | loss  3.48 | ppl    32.31\n",
            "| epoch   1 | 210400/389982 batches | lr 5.00 | ms/batch 66.58 | loss  3.54 | ppl    34.41\n",
            "| epoch   1 | 210600/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.50 | ppl    33.06\n",
            "| epoch   1 | 210800/389982 batches | lr 5.00 | ms/batch 68.06 | loss  3.52 | ppl    33.74\n",
            "| epoch   1 | 211000/389982 batches | lr 5.00 | ms/batch 69.34 | loss  3.43 | ppl    30.92\n",
            "| epoch   1 | 211200/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.47 | ppl    32.18\n",
            "| epoch   1 | 211400/389982 batches | lr 5.00 | ms/batch 68.45 | loss  3.39 | ppl    29.77\n",
            "| epoch   1 | 211600/389982 batches | lr 5.00 | ms/batch 66.77 | loss  3.47 | ppl    32.14\n",
            "| epoch   1 | 211800/389982 batches | lr 5.00 | ms/batch 67.81 | loss  3.44 | ppl    31.10\n",
            "| epoch   1 | 212000/389982 batches | lr 5.00 | ms/batch 68.80 | loss  3.41 | ppl    30.24\n",
            "| epoch   1 | 212200/389982 batches | lr 5.00 | ms/batch 72.86 | loss  3.43 | ppl    30.89\n",
            "| epoch   1 | 212400/389982 batches | lr 5.00 | ms/batch 68.90 | loss  3.64 | ppl    38.18\n",
            "| epoch   1 | 212600/389982 batches | lr 5.00 | ms/batch 68.19 | loss  3.51 | ppl    33.37\n",
            "| epoch   1 | 212800/389982 batches | lr 5.00 | ms/batch 67.85 | loss  3.46 | ppl    31.68\n",
            "| epoch   1 | 213000/389982 batches | lr 5.00 | ms/batch 67.50 | loss  3.50 | ppl    33.09\n",
            "| epoch   1 | 213200/389982 batches | lr 5.00 | ms/batch 69.08 | loss  3.45 | ppl    31.38\n",
            "| epoch   1 | 213400/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.48 | ppl    32.47\n",
            "| epoch   1 | 213600/389982 batches | lr 5.00 | ms/batch 69.42 | loss  3.46 | ppl    31.83\n",
            "| epoch   1 | 213800/389982 batches | lr 5.00 | ms/batch 66.94 | loss  3.50 | ppl    33.18\n",
            "| epoch   1 | 214000/389982 batches | lr 5.00 | ms/batch 67.87 | loss  3.51 | ppl    33.48\n",
            "| epoch   1 | 214200/389982 batches | lr 5.00 | ms/batch 71.43 | loss  3.62 | ppl    37.40\n",
            "| epoch   1 | 214400/389982 batches | lr 5.00 | ms/batch 69.29 | loss  3.45 | ppl    31.64\n",
            "| epoch   1 | 214600/389982 batches | lr 5.00 | ms/batch 68.93 | loss  3.37 | ppl    29.12\n",
            "| epoch   1 | 214800/389982 batches | lr 5.00 | ms/batch 68.49 | loss  3.50 | ppl    33.21\n",
            "| epoch   1 | 215000/389982 batches | lr 5.00 | ms/batch 68.34 | loss  3.43 | ppl    30.76\n",
            "| epoch   1 | 215200/389982 batches | lr 5.00 | ms/batch 68.72 | loss  3.42 | ppl    30.52\n",
            "| epoch   1 | 215400/389982 batches | lr 5.00 | ms/batch 68.91 | loss  3.40 | ppl    29.95\n",
            "| epoch   1 | 215600/389982 batches | lr 5.00 | ms/batch 67.16 | loss  3.47 | ppl    32.13\n",
            "| epoch   1 | 215800/389982 batches | lr 5.00 | ms/batch 67.72 | loss  3.52 | ppl    33.77\n",
            "| epoch   1 | 216000/389982 batches | lr 5.00 | ms/batch 67.89 | loss  3.43 | ppl    30.83\n",
            "| epoch   1 | 216200/389982 batches | lr 5.00 | ms/batch 73.48 | loss  3.41 | ppl    30.35\n",
            "| epoch   1 | 216400/389982 batches | lr 5.00 | ms/batch 68.96 | loss  3.40 | ppl    30.03\n",
            "| epoch   1 | 216600/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.42 | ppl    30.47\n",
            "| epoch   1 | 216800/389982 batches | lr 5.00 | ms/batch 65.93 | loss  3.58 | ppl    35.81\n",
            "| epoch   1 | 217000/389982 batches | lr 5.00 | ms/batch 67.43 | loss  3.46 | ppl    31.93\n",
            "| epoch   1 | 217200/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.45 | ppl    31.66\n",
            "| epoch   1 | 217400/389982 batches | lr 5.00 | ms/batch 66.07 | loss  3.51 | ppl    33.39\n",
            "| epoch   1 | 217600/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.43 | ppl    30.75\n",
            "| epoch   1 | 217800/389982 batches | lr 5.00 | ms/batch 68.67 | loss  3.53 | ppl    34.08\n",
            "| epoch   1 | 218000/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.43 | ppl    30.87\n",
            "| epoch   1 | 218200/389982 batches | lr 5.00 | ms/batch 74.93 | loss  3.46 | ppl    31.81\n",
            "| epoch   1 | 218400/389982 batches | lr 5.00 | ms/batch 68.68 | loss  3.41 | ppl    30.40\n",
            "| epoch   1 | 218600/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.48 | ppl    32.42\n",
            "| epoch   1 | 218800/389982 batches | lr 5.00 | ms/batch 69.13 | loss  3.45 | ppl    31.50\n",
            "| epoch   1 | 219000/389982 batches | lr 5.00 | ms/batch 66.52 | loss  3.51 | ppl    33.47\n",
            "| epoch   1 | 219200/389982 batches | lr 5.00 | ms/batch 69.08 | loss  3.39 | ppl    29.54\n",
            "| epoch   1 | 219400/389982 batches | lr 5.00 | ms/batch 68.07 | loss  3.43 | ppl    30.95\n",
            "| epoch   1 | 219600/389982 batches | lr 5.00 | ms/batch 67.96 | loss  3.42 | ppl    30.67\n",
            "| epoch   1 | 219800/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.47 | ppl    31.98\n",
            "| epoch   1 | 220000/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.43 | ppl    30.73\n",
            "| epoch   1 | 220200/389982 batches | lr 5.00 | ms/batch 71.84 | loss  3.52 | ppl    33.70\n",
            "| epoch   1 | 220400/389982 batches | lr 5.00 | ms/batch 68.66 | loss  3.39 | ppl    29.59\n",
            "| epoch   1 | 220600/389982 batches | lr 5.00 | ms/batch 67.58 | loss  3.47 | ppl    32.15\n",
            "| epoch   1 | 220800/389982 batches | lr 5.00 | ms/batch 66.51 | loss  3.50 | ppl    33.00\n",
            "| epoch   1 | 221000/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.44 | ppl    31.27\n",
            "| epoch   1 | 221200/389982 batches | lr 5.00 | ms/batch 68.47 | loss  3.47 | ppl    31.98\n",
            "| epoch   1 | 221400/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.46 | ppl    31.78\n",
            "| epoch   1 | 221600/389982 batches | lr 5.00 | ms/batch 67.64 | loss  3.41 | ppl    30.36\n",
            "| epoch   1 | 221800/389982 batches | lr 5.00 | ms/batch 68.47 | loss  3.47 | ppl    32.01\n",
            "| epoch   1 | 222000/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.44 | ppl    31.33\n",
            "| epoch   1 | 222200/389982 batches | lr 5.00 | ms/batch 73.59 | loss  3.52 | ppl    33.77\n",
            "| epoch   1 | 222400/389982 batches | lr 5.00 | ms/batch 66.91 | loss  3.49 | ppl    32.64\n",
            "| epoch   1 | 222600/389982 batches | lr 5.00 | ms/batch 69.01 | loss  3.42 | ppl    30.61\n",
            "| epoch   1 | 222800/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.43 | ppl    30.76\n",
            "| epoch   1 | 223000/389982 batches | lr 5.00 | ms/batch 69.37 | loss  3.63 | ppl    37.75\n",
            "| epoch   1 | 223200/389982 batches | lr 5.00 | ms/batch 69.93 | loss  3.45 | ppl    31.39\n",
            "| epoch   1 | 223400/389982 batches | lr 5.00 | ms/batch 67.00 | loss  3.52 | ppl    33.81\n",
            "| epoch   1 | 223600/389982 batches | lr 5.00 | ms/batch 67.24 | loss  3.48 | ppl    32.35\n",
            "| epoch   1 | 223800/389982 batches | lr 5.00 | ms/batch 66.23 | loss  3.53 | ppl    34.00\n",
            "| epoch   1 | 224000/389982 batches | lr 5.00 | ms/batch 68.38 | loss  3.40 | ppl    30.00\n",
            "| epoch   1 | 224200/389982 batches | lr 5.00 | ms/batch 72.76 | loss  3.53 | ppl    34.28\n",
            "| epoch   1 | 224400/389982 batches | lr 5.00 | ms/batch 66.84 | loss  3.48 | ppl    32.59\n",
            "| epoch   1 | 224600/389982 batches | lr 5.00 | ms/batch 69.00 | loss  3.40 | ppl    30.08\n",
            "| epoch   1 | 224800/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.46 | ppl    31.84\n",
            "| epoch   1 | 225000/389982 batches | lr 5.00 | ms/batch 68.67 | loss  3.49 | ppl    32.68\n",
            "| epoch   1 | 225200/389982 batches | lr 5.00 | ms/batch 67.92 | loss  3.45 | ppl    31.64\n",
            "| epoch   1 | 225400/389982 batches | lr 5.00 | ms/batch 66.89 | loss  3.49 | ppl    32.85\n",
            "| epoch   1 | 225600/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.45 | ppl    31.50\n",
            "| epoch   1 | 225800/389982 batches | lr 5.00 | ms/batch 67.43 | loss  3.41 | ppl    30.28\n",
            "| epoch   1 | 226000/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.48 | ppl    32.57\n",
            "| epoch   1 | 226200/389982 batches | lr 5.00 | ms/batch 72.45 | loss  3.52 | ppl    33.70\n",
            "| epoch   1 | 226400/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.45 | ppl    31.60\n",
            "| epoch   1 | 226600/389982 batches | lr 5.00 | ms/batch 66.20 | loss  3.54 | ppl    34.36\n",
            "| epoch   1 | 226800/389982 batches | lr 5.00 | ms/batch 67.53 | loss  3.54 | ppl    34.40\n",
            "| epoch   1 | 227000/389982 batches | lr 5.00 | ms/batch 67.57 | loss  3.47 | ppl    32.28\n",
            "| epoch   1 | 227200/389982 batches | lr 5.00 | ms/batch 67.57 | loss  3.44 | ppl    31.19\n",
            "| epoch   1 | 227400/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.59 | ppl    36.37\n",
            "| epoch   1 | 227600/389982 batches | lr 5.00 | ms/batch 70.44 | loss  3.34 | ppl    28.33\n",
            "| epoch   1 | 227800/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.40 | ppl    29.92\n",
            "| epoch   1 | 228000/389982 batches | lr 5.00 | ms/batch 68.13 | loss  3.48 | ppl    32.32\n",
            "| epoch   1 | 228200/389982 batches | lr 5.00 | ms/batch 73.14 | loss  3.57 | ppl    35.34\n",
            "| epoch   1 | 228400/389982 batches | lr 5.00 | ms/batch 67.98 | loss  3.52 | ppl    33.84\n",
            "| epoch   1 | 228600/389982 batches | lr 5.00 | ms/batch 66.86 | loss  3.49 | ppl    32.87\n",
            "| epoch   1 | 228800/389982 batches | lr 5.00 | ms/batch 67.62 | loss  3.52 | ppl    33.75\n",
            "| epoch   1 | 229000/389982 batches | lr 5.00 | ms/batch 67.12 | loss  3.46 | ppl    31.78\n",
            "| epoch   1 | 229200/389982 batches | lr 5.00 | ms/batch 68.35 | loss  3.45 | ppl    31.49\n",
            "| epoch   1 | 229400/389982 batches | lr 5.00 | ms/batch 66.59 | loss  3.49 | ppl    32.92\n",
            "| epoch   1 | 229600/389982 batches | lr 5.00 | ms/batch 67.54 | loss  3.43 | ppl    30.75\n",
            "| epoch   1 | 229800/389982 batches | lr 5.00 | ms/batch 66.64 | loss  3.48 | ppl    32.53\n",
            "| epoch   1 | 230000/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.50 | ppl    33.19\n",
            "| epoch   1 | 230200/389982 batches | lr 5.00 | ms/batch 72.60 | loss  3.43 | ppl    30.86\n",
            "| epoch   1 | 230400/389982 batches | lr 5.00 | ms/batch 68.69 | loss  3.40 | ppl    29.92\n",
            "| epoch   1 | 230600/389982 batches | lr 5.00 | ms/batch 67.23 | loss  3.47 | ppl    32.22\n",
            "| epoch   1 | 230800/389982 batches | lr 5.00 | ms/batch 67.81 | loss  3.44 | ppl    31.31\n",
            "| epoch   1 | 231000/389982 batches | lr 5.00 | ms/batch 66.84 | loss  3.53 | ppl    34.06\n",
            "| epoch   1 | 231200/389982 batches | lr 5.00 | ms/batch 67.06 | loss  3.46 | ppl    31.81\n",
            "| epoch   1 | 231400/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.40 | ppl    30.03\n",
            "| epoch   1 | 231600/389982 batches | lr 5.00 | ms/batch 67.08 | loss  3.46 | ppl    31.91\n",
            "| epoch   1 | 231800/389982 batches | lr 5.00 | ms/batch 67.79 | loss  3.45 | ppl    31.62\n",
            "| epoch   1 | 232000/389982 batches | lr 5.00 | ms/batch 66.76 | loss  3.50 | ppl    33.26\n",
            "| epoch   1 | 232200/389982 batches | lr 5.00 | ms/batch 71.94 | loss  3.45 | ppl    31.62\n",
            "| epoch   1 | 232400/389982 batches | lr 5.00 | ms/batch 67.02 | loss  3.51 | ppl    33.37\n",
            "| epoch   1 | 232600/389982 batches | lr 5.00 | ms/batch 70.35 | loss  3.46 | ppl    31.77\n",
            "| epoch   1 | 232800/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.46 | ppl    31.82\n",
            "| epoch   1 | 233000/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.50 | ppl    33.24\n",
            "| epoch   1 | 233200/389982 batches | lr 5.00 | ms/batch 66.40 | loss  3.54 | ppl    34.52\n",
            "| epoch   1 | 233400/389982 batches | lr 5.00 | ms/batch 67.97 | loss  3.41 | ppl    30.41\n",
            "| epoch   1 | 233600/389982 batches | lr 5.00 | ms/batch 67.87 | loss  3.43 | ppl    30.94\n",
            "| epoch   1 | 233800/389982 batches | lr 5.00 | ms/batch 67.57 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 234000/389982 batches | lr 5.00 | ms/batch 68.76 | loss  3.36 | ppl    28.68\n",
            "| epoch   1 | 234200/389982 batches | lr 5.00 | ms/batch 72.36 | loss  3.41 | ppl    30.13\n",
            "| epoch   1 | 234400/389982 batches | lr 5.00 | ms/batch 68.27 | loss  3.40 | ppl    30.05\n",
            "| epoch   1 | 234600/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.63 | ppl    37.65\n",
            "| epoch   1 | 234800/389982 batches | lr 5.00 | ms/batch 68.04 | loss  3.49 | ppl    32.73\n",
            "| epoch   1 | 235000/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.48 | ppl    32.31\n",
            "| epoch   1 | 235200/389982 batches | lr 5.00 | ms/batch 66.96 | loss  3.49 | ppl    32.81\n",
            "| epoch   1 | 235400/389982 batches | lr 5.00 | ms/batch 68.57 | loss  3.50 | ppl    32.96\n",
            "| epoch   1 | 235600/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.61 | ppl    36.85\n",
            "| epoch   1 | 235800/389982 batches | lr 5.00 | ms/batch 68.06 | loss  3.58 | ppl    35.74\n",
            "| epoch   1 | 236000/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.45 | ppl    31.50\n",
            "| epoch   1 | 236200/389982 batches | lr 5.00 | ms/batch 72.56 | loss  3.42 | ppl    30.48\n",
            "| epoch   1 | 236400/389982 batches | lr 5.00 | ms/batch 66.68 | loss  3.50 | ppl    33.10\n",
            "| epoch   1 | 236600/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.48 | ppl    32.41\n",
            "| epoch   1 | 236800/389982 batches | lr 5.00 | ms/batch 67.88 | loss  3.43 | ppl    30.96\n",
            "| epoch   1 | 237000/389982 batches | lr 5.00 | ms/batch 67.53 | loss  3.47 | ppl    32.19\n",
            "| epoch   1 | 237200/389982 batches | lr 5.00 | ms/batch 68.24 | loss  3.47 | ppl    32.09\n",
            "| epoch   1 | 237400/389982 batches | lr 5.00 | ms/batch 68.74 | loss  3.46 | ppl    31.69\n",
            "| epoch   1 | 237600/389982 batches | lr 5.00 | ms/batch 66.88 | loss  3.49 | ppl    32.69\n",
            "| epoch   1 | 237800/389982 batches | lr 5.00 | ms/batch 67.92 | loss  3.44 | ppl    31.22\n",
            "| epoch   1 | 238000/389982 batches | lr 5.00 | ms/batch 67.12 | loss  3.51 | ppl    33.46\n",
            "| epoch   1 | 238200/389982 batches | lr 5.00 | ms/batch 72.16 | loss  3.52 | ppl    33.90\n",
            "| epoch   1 | 238400/389982 batches | lr 5.00 | ms/batch 67.16 | loss  3.50 | ppl    33.08\n",
            "| epoch   1 | 238600/389982 batches | lr 5.00 | ms/batch 67.46 | loss  3.43 | ppl    30.88\n",
            "| epoch   1 | 238800/389982 batches | lr 5.00 | ms/batch 66.99 | loss  3.49 | ppl    32.65\n",
            "| epoch   1 | 239000/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.46 | ppl    31.92\n",
            "| epoch   1 | 239200/389982 batches | lr 5.00 | ms/batch 69.63 | loss  3.52 | ppl    33.85\n",
            "| epoch   1 | 239400/389982 batches | lr 5.00 | ms/batch 68.31 | loss  3.50 | ppl    33.19\n",
            "| epoch   1 | 239600/389982 batches | lr 5.00 | ms/batch 67.93 | loss  3.42 | ppl    30.48\n",
            "| epoch   1 | 239800/389982 batches | lr 5.00 | ms/batch 68.28 | loss  3.38 | ppl    29.46\n",
            "| epoch   1 | 240000/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.46 | ppl    31.86\n",
            "| epoch   1 | 240200/389982 batches | lr 5.00 | ms/batch 72.65 | loss  3.39 | ppl    29.79\n",
            "| epoch   1 | 240400/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.42 | ppl    30.57\n",
            "| epoch   1 | 240600/389982 batches | lr 5.00 | ms/batch 68.74 | loss  3.46 | ppl    31.90\n",
            "| epoch   1 | 240800/389982 batches | lr 5.00 | ms/batch 67.93 | loss  3.61 | ppl    37.11\n",
            "| epoch   1 | 241000/389982 batches | lr 5.00 | ms/batch 66.89 | loss  3.53 | ppl    34.09\n",
            "| epoch   1 | 241200/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.43 | ppl    30.76\n",
            "| epoch   1 | 241400/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.45 | ppl    31.65\n",
            "| epoch   1 | 241600/389982 batches | lr 5.00 | ms/batch 68.69 | loss  3.43 | ppl    31.00\n",
            "| epoch   1 | 241800/389982 batches | lr 5.00 | ms/batch 67.28 | loss  3.54 | ppl    34.53\n",
            "| epoch   1 | 242000/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.51 | ppl    33.42\n",
            "| epoch   1 | 242200/389982 batches | lr 5.00 | ms/batch 71.70 | loss  3.47 | ppl    32.09\n",
            "| epoch   1 | 242400/389982 batches | lr 5.00 | ms/batch 68.88 | loss  3.40 | ppl    30.01\n",
            "| epoch   1 | 242600/389982 batches | lr 5.00 | ms/batch 68.48 | loss  3.43 | ppl    30.77\n",
            "| epoch   1 | 242800/389982 batches | lr 5.00 | ms/batch 66.88 | loss  3.47 | ppl    32.19\n",
            "| epoch   1 | 243000/389982 batches | lr 5.00 | ms/batch 68.96 | loss  3.46 | ppl    31.85\n",
            "| epoch   1 | 243200/389982 batches | lr 5.00 | ms/batch 68.45 | loss  3.41 | ppl    30.15\n",
            "| epoch   1 | 243400/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.46 | ppl    31.72\n",
            "| epoch   1 | 243600/389982 batches | lr 5.00 | ms/batch 66.73 | loss  3.50 | ppl    33.28\n",
            "| epoch   1 | 243800/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.47 | ppl    32.09\n",
            "| epoch   1 | 244000/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.38 | ppl    29.47\n",
            "| epoch   1 | 244200/389982 batches | lr 5.00 | ms/batch 72.94 | loss  3.46 | ppl    31.76\n",
            "| epoch   1 | 244400/389982 batches | lr 5.00 | ms/batch 67.81 | loss  3.47 | ppl    32.04\n",
            "| epoch   1 | 244600/389982 batches | lr 5.00 | ms/batch 69.56 | loss  3.38 | ppl    29.51\n",
            "| epoch   1 | 244800/389982 batches | lr 5.00 | ms/batch 66.83 | loss  3.55 | ppl    34.89\n",
            "| epoch   1 | 245000/389982 batches | lr 5.00 | ms/batch 67.97 | loss  3.43 | ppl    30.79\n",
            "| epoch   1 | 245200/389982 batches | lr 5.00 | ms/batch 68.12 | loss  3.46 | ppl    31.79\n",
            "| epoch   1 | 245400/389982 batches | lr 5.00 | ms/batch 69.79 | loss  3.45 | ppl    31.39\n",
            "| epoch   1 | 245600/389982 batches | lr 5.00 | ms/batch 67.92 | loss  3.47 | ppl    32.08\n",
            "| epoch   1 | 245800/389982 batches | lr 5.00 | ms/batch 68.10 | loss  3.40 | ppl    30.11\n",
            "| epoch   1 | 246000/389982 batches | lr 5.00 | ms/batch 69.48 | loss  3.36 | ppl    28.81\n",
            "| epoch   1 | 246200/389982 batches | lr 5.00 | ms/batch 72.54 | loss  3.46 | ppl    31.85\n",
            "| epoch   1 | 246400/389982 batches | lr 5.00 | ms/batch 69.18 | loss  3.39 | ppl    29.80\n",
            "| epoch   1 | 246600/389982 batches | lr 5.00 | ms/batch 67.60 | loss  3.45 | ppl    31.49\n",
            "| epoch   1 | 246800/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.40 | ppl    29.95\n",
            "| epoch   1 | 247000/389982 batches | lr 5.00 | ms/batch 68.76 | loss  3.38 | ppl    29.42\n",
            "| epoch   1 | 247200/389982 batches | lr 5.00 | ms/batch 67.01 | loss  3.56 | ppl    35.06\n",
            "| epoch   1 | 247400/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.42 | ppl    30.50\n",
            "| epoch   1 | 247600/389982 batches | lr 5.00 | ms/batch 69.07 | loss  3.38 | ppl    29.49\n",
            "| epoch   1 | 247800/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.55 | ppl    34.92\n",
            "| epoch   1 | 248000/389982 batches | lr 5.00 | ms/batch 66.58 | loss  3.47 | ppl    32.24\n",
            "| epoch   1 | 248200/389982 batches | lr 5.00 | ms/batch 71.41 | loss  3.55 | ppl    34.90\n",
            "| epoch   1 | 248400/389982 batches | lr 5.00 | ms/batch 67.79 | loss  3.51 | ppl    33.61\n",
            "| epoch   1 | 248600/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.48 | ppl    32.41\n",
            "| epoch   1 | 248800/389982 batches | lr 5.00 | ms/batch 67.23 | loss  3.50 | ppl    33.16\n",
            "| epoch   1 | 249000/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.43 | ppl    30.81\n",
            "| epoch   1 | 249200/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.47 | ppl    32.18\n",
            "| epoch   1 | 249400/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.47 | ppl    32.02\n",
            "| epoch   1 | 249600/389982 batches | lr 5.00 | ms/batch 68.35 | loss  3.50 | ppl    33.26\n",
            "| epoch   1 | 249800/389982 batches | lr 5.00 | ms/batch 66.37 | loss  3.60 | ppl    36.67\n",
            "| epoch   1 | 250000/389982 batches | lr 5.00 | ms/batch 66.71 | loss  3.53 | ppl    34.00\n",
            "| epoch   1 | 250200/389982 batches | lr 5.00 | ms/batch 73.48 | loss  3.56 | ppl    35.00\n",
            "| epoch   1 | 250400/389982 batches | lr 5.00 | ms/batch 68.33 | loss  3.42 | ppl    30.67\n",
            "| epoch   1 | 250600/389982 batches | lr 5.00 | ms/batch 67.30 | loss  3.45 | ppl    31.47\n",
            "| epoch   1 | 250800/389982 batches | lr 5.00 | ms/batch 66.88 | loss  3.43 | ppl    30.88\n",
            "| epoch   1 | 251000/389982 batches | lr 5.00 | ms/batch 69.00 | loss  3.43 | ppl    30.77\n",
            "| epoch   1 | 251200/389982 batches | lr 5.00 | ms/batch 67.87 | loss  3.50 | ppl    33.18\n",
            "| epoch   1 | 251400/389982 batches | lr 5.00 | ms/batch 67.29 | loss  3.43 | ppl    30.75\n",
            "| epoch   1 | 251600/389982 batches | lr 5.00 | ms/batch 67.46 | loss  3.45 | ppl    31.54\n",
            "| epoch   1 | 251800/389982 batches | lr 5.00 | ms/batch 67.05 | loss  3.52 | ppl    33.76\n",
            "| epoch   1 | 252000/389982 batches | lr 5.00 | ms/batch 67.14 | loss  3.45 | ppl    31.42\n",
            "| epoch   1 | 252200/389982 batches | lr 5.00 | ms/batch 72.42 | loss  3.43 | ppl    30.83\n",
            "| epoch   1 | 252400/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.48 | ppl    32.43\n",
            "| epoch   1 | 252600/389982 batches | lr 5.00 | ms/batch 68.27 | loss  3.42 | ppl    30.65\n",
            "| epoch   1 | 252800/389982 batches | lr 5.00 | ms/batch 68.19 | loss  3.39 | ppl    29.63\n",
            "| epoch   1 | 253000/389982 batches | lr 5.00 | ms/batch 68.17 | loss  3.41 | ppl    30.18\n",
            "| epoch   1 | 253200/389982 batches | lr 5.00 | ms/batch 66.69 | loss  3.49 | ppl    32.74\n",
            "| epoch   1 | 253400/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.49 | ppl    32.80\n",
            "| epoch   1 | 253600/389982 batches | lr 5.00 | ms/batch 67.96 | loss  3.50 | ppl    33.08\n",
            "| epoch   1 | 253800/389982 batches | lr 5.00 | ms/batch 66.85 | loss  3.49 | ppl    32.69\n",
            "| epoch   1 | 254000/389982 batches | lr 5.00 | ms/batch 69.23 | loss  3.40 | ppl    29.91\n",
            "| epoch   1 | 254200/389982 batches | lr 5.00 | ms/batch 72.55 | loss  3.46 | ppl    31.95\n",
            "| epoch   1 | 254400/389982 batches | lr 5.00 | ms/batch 66.89 | loss  3.50 | ppl    33.04\n",
            "| epoch   1 | 254600/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.44 | ppl    31.05\n",
            "| epoch   1 | 254800/389982 batches | lr 5.00 | ms/batch 66.78 | loss  3.52 | ppl    33.81\n",
            "| epoch   1 | 255000/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.44 | ppl    31.20\n",
            "| epoch   1 | 255200/389982 batches | lr 5.00 | ms/batch 68.36 | loss  3.43 | ppl    30.86\n",
            "| epoch   1 | 255400/389982 batches | lr 5.00 | ms/batch 68.76 | loss  3.41 | ppl    30.25\n",
            "| epoch   1 | 255600/389982 batches | lr 5.00 | ms/batch 67.84 | loss  3.45 | ppl    31.58\n",
            "| epoch   1 | 255800/389982 batches | lr 5.00 | ms/batch 66.83 | loss  3.48 | ppl    32.60\n",
            "| epoch   1 | 256000/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.45 | ppl    31.53\n",
            "| epoch   1 | 256200/389982 batches | lr 5.00 | ms/batch 73.53 | loss  3.61 | ppl    37.03\n",
            "| epoch   1 | 256400/389982 batches | lr 5.00 | ms/batch 66.52 | loss  3.47 | ppl    32.16\n",
            "| epoch   1 | 256600/389982 batches | lr 5.00 | ms/batch 66.77 | loss  3.46 | ppl    31.66\n",
            "| epoch   1 | 256800/389982 batches | lr 5.00 | ms/batch 68.19 | loss  3.40 | ppl    30.11\n",
            "| epoch   1 | 257000/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.63 | ppl    37.90\n",
            "| epoch   1 | 257200/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.44 | ppl    31.20\n",
            "| epoch   1 | 257400/389982 batches | lr 5.00 | ms/batch 68.06 | loss  3.44 | ppl    31.18\n",
            "| epoch   1 | 257600/389982 batches | lr 5.00 | ms/batch 67.67 | loss  3.47 | ppl    32.10\n",
            "| epoch   1 | 257800/389982 batches | lr 5.00 | ms/batch 67.17 | loss  3.52 | ppl    33.86\n",
            "| epoch   1 | 258000/389982 batches | lr 5.00 | ms/batch 68.06 | loss  3.42 | ppl    30.44\n",
            "| epoch   1 | 258200/389982 batches | lr 5.00 | ms/batch 72.00 | loss  3.76 | ppl    42.76\n",
            "| epoch   1 | 258400/389982 batches | lr 5.00 | ms/batch 68.47 | loss  3.64 | ppl    38.04\n",
            "| epoch   1 | 258600/389982 batches | lr 5.00 | ms/batch 66.72 | loss  3.50 | ppl    33.18\n",
            "| epoch   1 | 258800/389982 batches | lr 5.00 | ms/batch 68.19 | loss  3.51 | ppl    33.39\n",
            "| epoch   1 | 259000/389982 batches | lr 5.00 | ms/batch 67.60 | loss  3.48 | ppl    32.36\n",
            "| epoch   1 | 259200/389982 batches | lr 5.00 | ms/batch 67.88 | loss  3.52 | ppl    33.69\n",
            "| epoch   1 | 259400/389982 batches | lr 5.00 | ms/batch 68.62 | loss  3.52 | ppl    33.81\n",
            "| epoch   1 | 259600/389982 batches | lr 5.00 | ms/batch 67.90 | loss  3.43 | ppl    30.99\n",
            "| epoch   1 | 259800/389982 batches | lr 5.00 | ms/batch 68.25 | loss  3.55 | ppl    34.66\n",
            "| epoch   1 | 260000/389982 batches | lr 5.00 | ms/batch 66.32 | loss  3.51 | ppl    33.44\n",
            "| epoch   1 | 260200/389982 batches | lr 5.00 | ms/batch 72.60 | loss  3.45 | ppl    31.55\n",
            "| epoch   1 | 260400/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.45 | ppl    31.35\n",
            "| epoch   1 | 260600/389982 batches | lr 5.00 | ms/batch 66.78 | loss  3.52 | ppl    33.67\n",
            "| epoch   1 | 260800/389982 batches | lr 5.00 | ms/batch 68.57 | loss  3.40 | ppl    30.03\n",
            "| epoch   1 | 261000/389982 batches | lr 5.00 | ms/batch 68.49 | loss  3.41 | ppl    30.24\n",
            "| epoch   1 | 261200/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.48 | ppl    32.60\n",
            "| epoch   1 | 261400/389982 batches | lr 5.00 | ms/batch 67.27 | loss  3.43 | ppl    30.78\n",
            "| epoch   1 | 261600/389982 batches | lr 5.00 | ms/batch 67.44 | loss  3.48 | ppl    32.38\n",
            "| epoch   1 | 261800/389982 batches | lr 5.00 | ms/batch 68.97 | loss  3.44 | ppl    31.14\n",
            "| epoch   1 | 262000/389982 batches | lr 5.00 | ms/batch 68.70 | loss  3.37 | ppl    28.99\n",
            "| epoch   1 | 262200/389982 batches | lr 5.00 | ms/batch 71.95 | loss  3.48 | ppl    32.39\n",
            "| epoch   1 | 262400/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.44 | ppl    31.27\n",
            "| epoch   1 | 262600/389982 batches | lr 5.00 | ms/batch 67.11 | loss  3.45 | ppl    31.42\n",
            "| epoch   1 | 262800/389982 batches | lr 5.00 | ms/batch 67.10 | loss  3.45 | ppl    31.60\n",
            "| epoch   1 | 263000/389982 batches | lr 5.00 | ms/batch 68.21 | loss  3.43 | ppl    31.02\n",
            "| epoch   1 | 263200/389982 batches | lr 5.00 | ms/batch 69.05 | loss  3.39 | ppl    29.58\n",
            "| epoch   1 | 263400/389982 batches | lr 5.00 | ms/batch 68.98 | loss  3.40 | ppl    29.99\n",
            "| epoch   1 | 263600/389982 batches | lr 5.00 | ms/batch 68.05 | loss  3.59 | ppl    36.32\n",
            "| epoch   1 | 263800/389982 batches | lr 5.00 | ms/batch 67.98 | loss  3.50 | ppl    33.03\n",
            "| epoch   1 | 264000/389982 batches | lr 5.00 | ms/batch 69.14 | loss  3.49 | ppl    32.92\n",
            "| epoch   1 | 264200/389982 batches | lr 5.00 | ms/batch 72.57 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 264400/389982 batches | lr 5.00 | ms/batch 66.69 | loss  3.51 | ppl    33.55\n",
            "| epoch   1 | 264600/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.41 | ppl    30.36\n",
            "| epoch   1 | 264800/389982 batches | lr 5.00 | ms/batch 66.69 | loss  3.54 | ppl    34.31\n",
            "| epoch   1 | 265000/389982 batches | lr 5.00 | ms/batch 68.24 | loss  3.50 | ppl    33.02\n",
            "| epoch   1 | 265200/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.42 | ppl    30.69\n",
            "| epoch   1 | 265400/389982 batches | lr 5.00 | ms/batch 68.32 | loss  3.42 | ppl    30.44\n",
            "| epoch   1 | 265600/389982 batches | lr 5.00 | ms/batch 69.39 | loss  3.36 | ppl    28.75\n",
            "| epoch   1 | 265800/389982 batches | lr 5.00 | ms/batch 67.97 | loss  3.56 | ppl    35.06\n",
            "| epoch   1 | 266000/389982 batches | lr 5.00 | ms/batch 66.93 | loss  3.49 | ppl    32.81\n",
            "| epoch   1 | 266200/389982 batches | lr 5.00 | ms/batch 71.19 | loss  3.50 | ppl    33.22\n",
            "| epoch   1 | 266400/389982 batches | lr 5.00 | ms/batch 67.18 | loss  3.45 | ppl    31.50\n",
            "| epoch   1 | 266600/389982 batches | lr 5.00 | ms/batch 68.84 | loss  3.49 | ppl    32.64\n",
            "| epoch   1 | 266800/389982 batches | lr 5.00 | ms/batch 66.95 | loss  3.54 | ppl    34.50\n",
            "| epoch   1 | 267000/389982 batches | lr 5.00 | ms/batch 69.30 | loss  3.41 | ppl    30.34\n",
            "| epoch   1 | 267200/389982 batches | lr 5.00 | ms/batch 69.22 | loss  3.37 | ppl    28.95\n",
            "| epoch   1 | 267400/389982 batches | lr 5.00 | ms/batch 67.67 | loss  3.45 | ppl    31.36\n",
            "| epoch   1 | 267600/389982 batches | lr 5.00 | ms/batch 67.60 | loss  3.42 | ppl    30.69\n",
            "| epoch   1 | 267800/389982 batches | lr 5.00 | ms/batch 67.16 | loss  3.45 | ppl    31.46\n",
            "| epoch   1 | 268000/389982 batches | lr 5.00 | ms/batch 68.24 | loss  3.42 | ppl    30.62\n",
            "| epoch   1 | 268200/389982 batches | lr 5.00 | ms/batch 73.47 | loss  3.39 | ppl    29.67\n",
            "| epoch   1 | 268400/389982 batches | lr 5.00 | ms/batch 66.60 | loss  3.51 | ppl    33.47\n",
            "| epoch   1 | 268600/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.55 | ppl    34.95\n",
            "| epoch   1 | 268800/389982 batches | lr 5.00 | ms/batch 67.76 | loss  3.45 | ppl    31.63\n",
            "| epoch   1 | 269000/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.47 | ppl    32.26\n",
            "| epoch   1 | 269200/389982 batches | lr 5.00 | ms/batch 65.90 | loss  3.55 | ppl    34.72\n",
            "| epoch   1 | 269400/389982 batches | lr 5.00 | ms/batch 68.55 | loss  3.38 | ppl    29.27\n",
            "| epoch   1 | 269600/389982 batches | lr 5.00 | ms/batch 67.34 | loss  3.43 | ppl    30.82\n",
            "| epoch   1 | 269800/389982 batches | lr 5.00 | ms/batch 68.80 | loss  3.44 | ppl    31.17\n",
            "| epoch   1 | 270000/389982 batches | lr 5.00 | ms/batch 69.05 | loss  3.43 | ppl    30.74\n",
            "| epoch   1 | 270200/389982 batches | lr 5.00 | ms/batch 72.30 | loss  3.43 | ppl    31.00\n",
            "| epoch   1 | 270400/389982 batches | lr 5.00 | ms/batch 68.61 | loss  3.39 | ppl    29.62\n",
            "| epoch   1 | 270600/389982 batches | lr 5.00 | ms/batch 68.43 | loss  3.47 | ppl    32.10\n",
            "| epoch   1 | 270800/389982 batches | lr 5.00 | ms/batch 66.63 | loss  3.46 | ppl    31.95\n",
            "| epoch   1 | 271000/389982 batches | lr 5.00 | ms/batch 68.67 | loss  3.46 | ppl    31.91\n",
            "| epoch   1 | 271200/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.43 | ppl    30.76\n",
            "| epoch   1 | 271400/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.44 | ppl    31.24\n",
            "| epoch   1 | 271600/389982 batches | lr 5.00 | ms/batch 67.87 | loss  3.43 | ppl    30.96\n",
            "| epoch   1 | 271800/389982 batches | lr 5.00 | ms/batch 67.62 | loss  3.48 | ppl    32.61\n",
            "| epoch   1 | 272000/389982 batches | lr 5.00 | ms/batch 67.39 | loss  3.47 | ppl    32.10\n",
            "| epoch   1 | 272200/389982 batches | lr 5.00 | ms/batch 72.86 | loss  3.57 | ppl    35.68\n",
            "| epoch   1 | 272400/389982 batches | lr 5.00 | ms/batch 68.59 | loss  3.42 | ppl    30.65\n",
            "| epoch   1 | 272600/389982 batches | lr 5.00 | ms/batch 67.72 | loss  3.46 | ppl    31.92\n",
            "| epoch   1 | 272800/389982 batches | lr 5.00 | ms/batch 68.34 | loss  3.40 | ppl    29.84\n",
            "| epoch   1 | 273000/389982 batches | lr 5.00 | ms/batch 68.23 | loss  3.45 | ppl    31.36\n",
            "| epoch   1 | 273200/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.45 | ppl    31.49\n",
            "| epoch   1 | 273400/389982 batches | lr 5.00 | ms/batch 69.09 | loss  3.38 | ppl    29.23\n",
            "| epoch   1 | 273600/389982 batches | lr 5.00 | ms/batch 67.85 | loss  3.41 | ppl    30.39\n",
            "| epoch   1 | 273800/389982 batches | lr 5.00 | ms/batch 66.92 | loss  3.50 | ppl    33.12\n",
            "| epoch   1 | 274000/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.44 | ppl    31.30\n",
            "| epoch   1 | 274200/389982 batches | lr 5.00 | ms/batch 73.10 | loss  3.43 | ppl    30.83\n",
            "| epoch   1 | 274400/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.47 | ppl    32.22\n",
            "| epoch   1 | 274600/389982 batches | lr 5.00 | ms/batch 65.62 | loss  3.53 | ppl    34.06\n",
            "| epoch   1 | 274800/389982 batches | lr 5.00 | ms/batch 66.68 | loss  3.56 | ppl    35.00\n",
            "| epoch   1 | 275000/389982 batches | lr 5.00 | ms/batch 69.24 | loss  3.38 | ppl    29.29\n",
            "| epoch   1 | 275200/389982 batches | lr 5.00 | ms/batch 68.27 | loss  3.45 | ppl    31.62\n",
            "| epoch   1 | 275400/389982 batches | lr 5.00 | ms/batch 67.32 | loss  3.51 | ppl    33.53\n",
            "| epoch   1 | 275600/389982 batches | lr 5.00 | ms/batch 67.81 | loss  3.42 | ppl    30.68\n",
            "| epoch   1 | 275800/389982 batches | lr 5.00 | ms/batch 66.68 | loss  3.48 | ppl    32.60\n",
            "| epoch   1 | 276000/389982 batches | lr 5.00 | ms/batch 68.08 | loss  3.57 | ppl    35.60\n",
            "| epoch   1 | 276200/389982 batches | lr 5.00 | ms/batch 72.50 | loss  3.43 | ppl    30.97\n",
            "| epoch   1 | 276400/389982 batches | lr 5.00 | ms/batch 67.46 | loss  3.38 | ppl    29.36\n",
            "| epoch   1 | 276600/389982 batches | lr 5.00 | ms/batch 67.83 | loss  3.44 | ppl    31.23\n",
            "| epoch   1 | 276800/389982 batches | lr 5.00 | ms/batch 67.31 | loss  3.46 | ppl    31.82\n",
            "| epoch   1 | 277000/389982 batches | lr 5.00 | ms/batch 67.07 | loss  3.46 | ppl    31.66\n",
            "| epoch   1 | 277200/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.41 | ppl    30.25\n",
            "| epoch   1 | 277400/389982 batches | lr 5.00 | ms/batch 68.48 | loss  3.42 | ppl    30.60\n",
            "| epoch   1 | 277600/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.41 | ppl    30.40\n",
            "| epoch   1 | 277800/389982 batches | lr 5.00 | ms/batch 68.87 | loss  3.41 | ppl    30.23\n",
            "| epoch   1 | 278000/389982 batches | lr 5.00 | ms/batch 66.65 | loss  3.47 | ppl    32.11\n",
            "| epoch   1 | 278200/389982 batches | lr 5.00 | ms/batch 73.56 | loss  3.45 | ppl    31.53\n",
            "| epoch   1 | 278400/389982 batches | lr 5.00 | ms/batch 68.11 | loss  3.40 | ppl    30.07\n",
            "| epoch   1 | 278600/389982 batches | lr 5.00 | ms/batch 67.80 | loss  3.43 | ppl    30.84\n",
            "| epoch   1 | 278800/389982 batches | lr 5.00 | ms/batch 67.48 | loss  3.43 | ppl    30.76\n",
            "| epoch   1 | 279000/389982 batches | lr 5.00 | ms/batch 67.49 | loss  3.42 | ppl    30.47\n",
            "| epoch   1 | 279200/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.53 | ppl    34.16\n",
            "| epoch   1 | 279400/389982 batches | lr 5.00 | ms/batch 68.39 | loss  3.40 | ppl    29.83\n",
            "| epoch   1 | 279600/389982 batches | lr 5.00 | ms/batch 69.06 | loss  3.44 | ppl    31.28\n",
            "| epoch   1 | 279800/389982 batches | lr 5.00 | ms/batch 66.76 | loss  3.47 | ppl    32.19\n",
            "| epoch   1 | 280000/389982 batches | lr 5.00 | ms/batch 67.31 | loss  3.43 | ppl    30.77\n",
            "| epoch   1 | 280200/389982 batches | lr 5.00 | ms/batch 72.91 | loss  3.42 | ppl    30.62\n",
            "| epoch   1 | 280400/389982 batches | lr 5.00 | ms/batch 66.75 | loss  3.53 | ppl    34.20\n",
            "| epoch   1 | 280600/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.46 | ppl    31.86\n",
            "| epoch   1 | 280800/389982 batches | lr 5.00 | ms/batch 66.62 | loss  3.52 | ppl    33.89\n",
            "| epoch   1 | 281000/389982 batches | lr 5.00 | ms/batch 67.13 | loss  3.48 | ppl    32.57\n",
            "| epoch   1 | 281200/389982 batches | lr 5.00 | ms/batch 66.21 | loss  3.52 | ppl    33.85\n",
            "| epoch   1 | 281400/389982 batches | lr 5.00 | ms/batch 67.50 | loss  3.40 | ppl    29.95\n",
            "| epoch   1 | 281600/389982 batches | lr 5.00 | ms/batch 68.29 | loss  3.43 | ppl    30.77\n",
            "| epoch   1 | 281800/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.41 | ppl    30.40\n",
            "| epoch   1 | 282000/389982 batches | lr 5.00 | ms/batch 67.44 | loss  3.43 | ppl    30.74\n",
            "| epoch   1 | 282200/389982 batches | lr 5.00 | ms/batch 71.13 | loss  3.51 | ppl    33.38\n",
            "| epoch   1 | 282400/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.50 | ppl    33.12\n",
            "| epoch   1 | 282600/389982 batches | lr 5.00 | ms/batch 66.62 | loss  3.52 | ppl    33.76\n",
            "| epoch   1 | 282800/389982 batches | lr 5.00 | ms/batch 67.12 | loss  3.46 | ppl    31.91\n",
            "| epoch   1 | 283000/389982 batches | lr 5.00 | ms/batch 67.46 | loss  3.46 | ppl    31.82\n",
            "| epoch   1 | 283200/389982 batches | lr 5.00 | ms/batch 66.91 | loss  3.51 | ppl    33.48\n",
            "| epoch   1 | 283400/389982 batches | lr 5.00 | ms/batch 68.38 | loss  3.39 | ppl    29.54\n",
            "| epoch   1 | 283600/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.43 | ppl    30.84\n",
            "| epoch   1 | 283800/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.42 | ppl    30.49\n",
            "| epoch   1 | 284000/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.48 | ppl    32.36\n",
            "| epoch   1 | 284200/389982 batches | lr 5.00 | ms/batch 73.10 | loss  3.68 | ppl    39.59\n",
            "| epoch   1 | 284400/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.47 | ppl    32.21\n",
            "| epoch   1 | 284600/389982 batches | lr 5.00 | ms/batch 67.15 | loss  3.45 | ppl    31.52\n",
            "| epoch   1 | 284800/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.48 | ppl    32.53\n",
            "| epoch   1 | 285000/389982 batches | lr 5.00 | ms/batch 66.86 | loss  3.56 | ppl    35.13\n",
            "| epoch   1 | 285200/389982 batches | lr 5.00 | ms/batch 68.01 | loss  3.47 | ppl    32.06\n",
            "| epoch   1 | 285400/389982 batches | lr 5.00 | ms/batch 66.68 | loss  3.46 | ppl    31.74\n",
            "| epoch   1 | 285600/389982 batches | lr 5.00 | ms/batch 66.98 | loss  3.51 | ppl    33.29\n",
            "| epoch   1 | 285800/389982 batches | lr 5.00 | ms/batch 69.37 | loss  3.39 | ppl    29.53\n",
            "| epoch   1 | 286000/389982 batches | lr 5.00 | ms/batch 67.55 | loss  3.54 | ppl    34.47\n",
            "| epoch   1 | 286200/389982 batches | lr 5.00 | ms/batch 72.23 | loss  3.53 | ppl    34.24\n",
            "| epoch   1 | 286400/389982 batches | lr 5.00 | ms/batch 67.75 | loss  3.45 | ppl    31.47\n",
            "| epoch   1 | 286600/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.40 | ppl    30.07\n",
            "| epoch   1 | 286800/389982 batches | lr 5.00 | ms/batch 69.26 | loss  3.42 | ppl    30.71\n",
            "| epoch   1 | 287000/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.42 | ppl    30.46\n",
            "| epoch   1 | 287200/389982 batches | lr 5.00 | ms/batch 68.71 | loss  3.38 | ppl    29.27\n",
            "| epoch   1 | 287400/389982 batches | lr 5.00 | ms/batch 67.17 | loss  3.45 | ppl    31.57\n",
            "| epoch   1 | 287600/389982 batches | lr 5.00 | ms/batch 69.68 | loss  3.40 | ppl    29.98\n",
            "| epoch   1 | 287800/389982 batches | lr 5.00 | ms/batch 67.57 | loss  3.56 | ppl    35.23\n",
            "| epoch   1 | 288000/389982 batches | lr 5.00 | ms/batch 68.01 | loss  3.50 | ppl    33.18\n",
            "| epoch   1 | 288200/389982 batches | lr 5.00 | ms/batch 77.76 | loss  3.42 | ppl    30.49\n",
            "| epoch   1 | 288400/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.59 | ppl    36.24\n",
            "| epoch   1 | 288600/389982 batches | lr 5.00 | ms/batch 67.65 | loss  3.64 | ppl    38.06\n",
            "| epoch   1 | 288800/389982 batches | lr 5.00 | ms/batch 69.21 | loss  3.38 | ppl    29.45\n",
            "| epoch   1 | 289000/389982 batches | lr 5.00 | ms/batch 68.33 | loss  3.39 | ppl    29.74\n",
            "| epoch   1 | 289200/389982 batches | lr 5.00 | ms/batch 69.65 | loss  3.61 | ppl    36.95\n",
            "| epoch   1 | 289400/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.64 | ppl    37.95\n",
            "| epoch   1 | 289600/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.45 | ppl    31.46\n",
            "| epoch   1 | 289800/389982 batches | lr 5.00 | ms/batch 68.23 | loss  3.47 | ppl    32.15\n",
            "| epoch   1 | 290000/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.44 | ppl    31.19\n",
            "| epoch   1 | 290200/389982 batches | lr 5.00 | ms/batch 73.45 | loss  3.40 | ppl    30.06\n",
            "| epoch   1 | 290400/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.49 | ppl    32.86\n",
            "| epoch   1 | 290600/389982 batches | lr 5.00 | ms/batch 67.11 | loss  3.46 | ppl    31.83\n",
            "| epoch   1 | 290800/389982 batches | lr 5.00 | ms/batch 68.21 | loss  3.39 | ppl    29.68\n",
            "| epoch   1 | 291000/389982 batches | lr 5.00 | ms/batch 66.52 | loss  3.51 | ppl    33.56\n",
            "| epoch   1 | 291200/389982 batches | lr 5.00 | ms/batch 68.53 | loss  3.45 | ppl    31.52\n",
            "| epoch   1 | 291400/389982 batches | lr 5.00 | ms/batch 67.33 | loss  3.45 | ppl    31.58\n",
            "| epoch   1 | 291600/389982 batches | lr 5.00 | ms/batch 68.71 | loss  3.37 | ppl    29.00\n",
            "| epoch   1 | 291800/389982 batches | lr 5.00 | ms/batch 68.94 | loss  3.41 | ppl    30.30\n",
            "| epoch   1 | 292000/389982 batches | lr 5.00 | ms/batch 66.91 | loss  3.49 | ppl    32.77\n",
            "| epoch   1 | 292200/389982 batches | lr 5.00 | ms/batch 72.83 | loss  3.43 | ppl    30.78\n",
            "| epoch   1 | 292400/389982 batches | lr 5.00 | ms/batch 67.64 | loss  3.40 | ppl    29.82\n",
            "| epoch   1 | 292600/389982 batches | lr 5.00 | ms/batch 68.29 | loss  3.38 | ppl    29.41\n",
            "| epoch   1 | 292800/389982 batches | lr 5.00 | ms/batch 69.04 | loss  3.47 | ppl    32.15\n",
            "| epoch   1 | 293000/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.46 | ppl    31.84\n",
            "| epoch   1 | 293200/389982 batches | lr 5.00 | ms/batch 69.42 | loss  3.39 | ppl    29.67\n",
            "| epoch   1 | 293400/389982 batches | lr 5.00 | ms/batch 69.39 | loss  3.32 | ppl    27.57\n",
            "| epoch   1 | 293600/389982 batches | lr 5.00 | ms/batch 66.66 | loss  3.47 | ppl    32.28\n",
            "| epoch   1 | 293800/389982 batches | lr 5.00 | ms/batch 68.05 | loss  3.45 | ppl    31.40\n",
            "| epoch   1 | 294000/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.42 | ppl    30.44\n",
            "| epoch   1 | 294200/389982 batches | lr 5.00 | ms/batch 74.67 | loss  3.32 | ppl    27.78\n",
            "| epoch   1 | 294400/389982 batches | lr 5.00 | ms/batch 68.47 | loss  3.37 | ppl    29.04\n",
            "| epoch   1 | 294600/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.45 | ppl    31.65\n",
            "| epoch   1 | 294800/389982 batches | lr 5.00 | ms/batch 68.90 | loss  3.37 | ppl    29.17\n",
            "| epoch   1 | 295000/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.44 | ppl    31.22\n",
            "| epoch   1 | 295200/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.47 | ppl    32.21\n",
            "| epoch   1 | 295400/389982 batches | lr 5.00 | ms/batch 67.98 | loss  3.47 | ppl    32.00\n",
            "| epoch   1 | 295600/389982 batches | lr 5.00 | ms/batch 69.04 | loss  3.40 | ppl    30.11\n",
            "| epoch   1 | 295800/389982 batches | lr 5.00 | ms/batch 67.20 | loss  3.43 | ppl    30.88\n",
            "| epoch   1 | 296000/389982 batches | lr 5.00 | ms/batch 67.65 | loss  3.47 | ppl    32.04\n",
            "| epoch   1 | 296200/389982 batches | lr 5.00 | ms/batch 72.73 | loss  3.48 | ppl    32.41\n",
            "| epoch   1 | 296400/389982 batches | lr 5.00 | ms/batch 68.86 | loss  3.52 | ppl    33.88\n",
            "| epoch   1 | 296600/389982 batches | lr 5.00 | ms/batch 67.97 | loss  3.49 | ppl    32.85\n",
            "| epoch   1 | 296800/389982 batches | lr 5.00 | ms/batch 68.55 | loss  3.43 | ppl    30.85\n",
            "| epoch   1 | 297000/389982 batches | lr 5.00 | ms/batch 68.11 | loss  3.42 | ppl    30.65\n",
            "| epoch   1 | 297200/389982 batches | lr 5.00 | ms/batch 68.54 | loss  3.47 | ppl    32.29\n",
            "| epoch   1 | 297400/389982 batches | lr 5.00 | ms/batch 68.60 | loss  3.42 | ppl    30.59\n",
            "| epoch   1 | 297600/389982 batches | lr 5.00 | ms/batch 66.70 | loss  3.50 | ppl    33.21\n",
            "| epoch   1 | 297800/389982 batches | lr 5.00 | ms/batch 67.88 | loss  3.45 | ppl    31.35\n",
            "| epoch   1 | 298000/389982 batches | lr 5.00 | ms/batch 67.44 | loss  3.47 | ppl    32.25\n",
            "| epoch   1 | 298200/389982 batches | lr 5.00 | ms/batch 72.64 | loss  3.43 | ppl    31.01\n",
            "| epoch   1 | 298400/389982 batches | lr 5.00 | ms/batch 68.53 | loss  3.41 | ppl    30.27\n",
            "| epoch   1 | 298600/389982 batches | lr 5.00 | ms/batch 68.93 | loss  3.33 | ppl    27.84\n",
            "| epoch   1 | 298800/389982 batches | lr 5.00 | ms/batch 68.04 | loss  3.49 | ppl    32.74\n",
            "| epoch   1 | 299000/389982 batches | lr 5.00 | ms/batch 68.52 | loss  3.40 | ppl    29.97\n",
            "| epoch   1 | 299200/389982 batches | lr 5.00 | ms/batch 67.68 | loss  3.48 | ppl    32.40\n",
            "| epoch   1 | 299400/389982 batches | lr 5.00 | ms/batch 69.77 | loss  3.32 | ppl    27.71\n",
            "| epoch   1 | 299600/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.42 | ppl    30.52\n",
            "| epoch   1 | 299800/389982 batches | lr 5.00 | ms/batch 67.38 | loss  3.43 | ppl    30.97\n",
            "| epoch   1 | 300000/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.42 | ppl    30.49\n",
            "| epoch   1 | 300200/389982 batches | lr 5.00 | ms/batch 71.69 | loss  3.43 | ppl    30.89\n",
            "| epoch   1 | 300400/389982 batches | lr 5.00 | ms/batch 67.88 | loss  3.43 | ppl    30.89\n",
            "| epoch   1 | 300600/389982 batches | lr 5.00 | ms/batch 67.17 | loss  3.46 | ppl    31.97\n",
            "| epoch   1 | 300800/389982 batches | lr 5.00 | ms/batch 66.64 | loss  3.61 | ppl    36.83\n",
            "| epoch   1 | 301000/389982 batches | lr 5.00 | ms/batch 68.39 | loss  3.39 | ppl    29.78\n",
            "| epoch   1 | 301200/389982 batches | lr 5.00 | ms/batch 68.23 | loss  3.44 | ppl    31.07\n",
            "| epoch   1 | 301400/389982 batches | lr 5.00 | ms/batch 69.57 | loss  3.36 | ppl    28.77\n",
            "| epoch   1 | 301600/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.41 | ppl    30.29\n",
            "| epoch   1 | 301800/389982 batches | lr 5.00 | ms/batch 67.48 | loss  3.43 | ppl    30.79\n",
            "| epoch   1 | 302000/389982 batches | lr 5.00 | ms/batch 66.90 | loss  3.54 | ppl    34.32\n",
            "| epoch   1 | 302200/389982 batches | lr 5.00 | ms/batch 72.27 | loss  3.44 | ppl    31.11\n",
            "| epoch   1 | 302400/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.44 | ppl    31.10\n",
            "| epoch   1 | 302600/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.53 | ppl    34.22\n",
            "| epoch   1 | 302800/389982 batches | lr 5.00 | ms/batch 66.49 | loss  3.48 | ppl    32.33\n",
            "| epoch   1 | 303000/389982 batches | lr 5.00 | ms/batch 67.76 | loss  3.45 | ppl    31.58\n",
            "| epoch   1 | 303200/389982 batches | lr 5.00 | ms/batch 68.65 | loss  3.39 | ppl    29.62\n",
            "| epoch   1 | 303400/389982 batches | lr 5.00 | ms/batch 69.07 | loss  3.39 | ppl    29.53\n",
            "| epoch   1 | 303600/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.48 | ppl    32.48\n",
            "| epoch   1 | 303800/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.44 | ppl    31.08\n",
            "| epoch   1 | 304000/389982 batches | lr 5.00 | ms/batch 67.18 | loss  3.46 | ppl    31.74\n",
            "| epoch   1 | 304200/389982 batches | lr 5.00 | ms/batch 73.84 | loss  3.46 | ppl    31.66\n",
            "| epoch   1 | 304400/389982 batches | lr 5.00 | ms/batch 68.61 | loss  3.42 | ppl    30.61\n",
            "| epoch   1 | 304600/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.44 | ppl    31.08\n",
            "| epoch   1 | 304800/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.42 | ppl    30.69\n",
            "| epoch   1 | 305000/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.44 | ppl    31.06\n",
            "| epoch   1 | 305200/389982 batches | lr 5.00 | ms/batch 68.56 | loss  3.38 | ppl    29.27\n",
            "| epoch   1 | 305400/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.46 | ppl    31.82\n",
            "| epoch   1 | 305600/389982 batches | lr 5.00 | ms/batch 67.13 | loss  3.86 | ppl    47.48\n",
            "| epoch   1 | 305800/389982 batches | lr 5.00 | ms/batch 69.64 | loss  3.55 | ppl    34.93\n",
            "| epoch   1 | 306000/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.43 | ppl    30.80\n",
            "| epoch   1 | 306200/389982 batches | lr 5.00 | ms/batch 73.27 | loss  3.45 | ppl    31.55\n",
            "| epoch   1 | 306400/389982 batches | lr 5.00 | ms/batch 68.64 | loss  3.38 | ppl    29.27\n",
            "| epoch   1 | 306600/389982 batches | lr 5.00 | ms/batch 68.93 | loss  3.40 | ppl    29.94\n",
            "| epoch   1 | 306800/389982 batches | lr 5.00 | ms/batch 68.39 | loss  3.42 | ppl    30.45\n",
            "| epoch   1 | 307000/389982 batches | lr 5.00 | ms/batch 67.06 | loss  3.51 | ppl    33.33\n",
            "| epoch   1 | 307200/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.45 | ppl    31.35\n",
            "| epoch   1 | 307400/389982 batches | lr 5.00 | ms/batch 67.92 | loss  3.44 | ppl    31.26\n",
            "| epoch   1 | 307600/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.42 | ppl    30.46\n",
            "| epoch   1 | 307800/389982 batches | lr 5.00 | ms/batch 67.87 | loss  3.42 | ppl    30.62\n",
            "| epoch   1 | 308000/389982 batches | lr 5.00 | ms/batch 67.92 | loss  3.50 | ppl    33.18\n",
            "| epoch   1 | 308200/389982 batches | lr 5.00 | ms/batch 73.40 | loss  3.40 | ppl    30.04\n",
            "| epoch   1 | 308400/389982 batches | lr 5.00 | ms/batch 66.94 | loss  3.43 | ppl    30.85\n",
            "| epoch   1 | 308600/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.44 | ppl    31.12\n",
            "| epoch   1 | 308800/389982 batches | lr 5.00 | ms/batch 67.96 | loss  3.44 | ppl    31.15\n",
            "| epoch   1 | 309000/389982 batches | lr 5.00 | ms/batch 67.76 | loss  3.43 | ppl    30.76\n",
            "| epoch   1 | 309200/389982 batches | lr 5.00 | ms/batch 68.17 | loss  3.46 | ppl    31.95\n",
            "| epoch   1 | 309400/389982 batches | lr 5.00 | ms/batch 67.02 | loss  3.45 | ppl    31.44\n",
            "| epoch   1 | 309600/389982 batches | lr 5.00 | ms/batch 66.67 | loss  3.48 | ppl    32.42\n",
            "| epoch   1 | 309800/389982 batches | lr 5.00 | ms/batch 66.75 | loss  3.51 | ppl    33.36\n",
            "| epoch   1 | 310000/389982 batches | lr 5.00 | ms/batch 66.25 | loss  3.48 | ppl    32.31\n",
            "| epoch   1 | 310200/389982 batches | lr 5.00 | ms/batch 72.70 | loss  3.44 | ppl    31.24\n",
            "| epoch   1 | 310400/389982 batches | lr 5.00 | ms/batch 68.74 | loss  3.52 | ppl    33.66\n",
            "| epoch   1 | 310600/389982 batches | lr 5.00 | ms/batch 67.80 | loss  3.45 | ppl    31.44\n",
            "| epoch   1 | 310800/389982 batches | lr 5.00 | ms/batch 68.49 | loss  3.39 | ppl    29.61\n",
            "| epoch   1 | 311000/389982 batches | lr 5.00 | ms/batch 66.13 | loss  3.54 | ppl    34.64\n",
            "| epoch   1 | 311200/389982 batches | lr 5.00 | ms/batch 66.83 | loss  3.48 | ppl    32.56\n",
            "| epoch   1 | 311400/389982 batches | lr 5.00 | ms/batch 67.23 | loss  3.50 | ppl    33.23\n",
            "| epoch   1 | 311600/389982 batches | lr 5.00 | ms/batch 66.77 | loss  3.46 | ppl    31.97\n",
            "| epoch   1 | 311800/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.45 | ppl    31.56\n",
            "| epoch   1 | 312000/389982 batches | lr 5.00 | ms/batch 67.90 | loss  3.41 | ppl    30.13\n",
            "| epoch   1 | 312200/389982 batches | lr 5.00 | ms/batch 72.03 | loss  3.46 | ppl    31.85\n",
            "| epoch   1 | 312400/389982 batches | lr 5.00 | ms/batch 67.55 | loss  3.46 | ppl    31.97\n",
            "| epoch   1 | 312600/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.45 | ppl    31.49\n",
            "| epoch   1 | 312800/389982 batches | lr 5.00 | ms/batch 67.92 | loss  3.44 | ppl    31.10\n",
            "| epoch   1 | 313000/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.44 | ppl    31.14\n",
            "| epoch   1 | 313200/389982 batches | lr 5.00 | ms/batch 69.11 | loss  3.46 | ppl    31.75\n",
            "| epoch   1 | 313400/389982 batches | lr 5.00 | ms/batch 68.79 | loss  3.42 | ppl    30.58\n",
            "| epoch   1 | 313600/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.41 | ppl    30.31\n",
            "| epoch   1 | 313800/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.46 | ppl    31.96\n",
            "| epoch   1 | 314000/389982 batches | lr 5.00 | ms/batch 67.05 | loss  3.52 | ppl    33.83\n",
            "| epoch   1 | 314200/389982 batches | lr 5.00 | ms/batch 72.62 | loss  3.44 | ppl    31.33\n",
            "| epoch   1 | 314400/389982 batches | lr 5.00 | ms/batch 68.90 | loss  3.41 | ppl    30.20\n",
            "| epoch   1 | 314600/389982 batches | lr 5.00 | ms/batch 68.18 | loss  3.37 | ppl    29.15\n",
            "| epoch   1 | 314800/389982 batches | lr 5.00 | ms/batch 67.49 | loss  3.44 | ppl    31.27\n",
            "| epoch   1 | 315000/389982 batches | lr 5.00 | ms/batch 68.83 | loss  3.32 | ppl    27.79\n",
            "| epoch   1 | 315200/389982 batches | lr 5.00 | ms/batch 68.10 | loss  3.40 | ppl    30.03\n",
            "| epoch   1 | 315400/389982 batches | lr 5.00 | ms/batch 66.77 | loss  3.50 | ppl    33.03\n",
            "| epoch   1 | 315600/389982 batches | lr 5.00 | ms/batch 66.87 | loss  3.46 | ppl    31.69\n",
            "| epoch   1 | 315800/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.44 | ppl    31.18\n",
            "| epoch   1 | 316000/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.39 | ppl    29.68\n",
            "| epoch   1 | 316200/389982 batches | lr 5.00 | ms/batch 72.87 | loss  3.40 | ppl    30.04\n",
            "| epoch   1 | 316400/389982 batches | lr 5.00 | ms/batch 68.23 | loss  3.40 | ppl    29.89\n",
            "| epoch   1 | 316600/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.45 | ppl    31.55\n",
            "| epoch   1 | 316800/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.52 | ppl    33.69\n",
            "| epoch   1 | 317000/389982 batches | lr 5.00 | ms/batch 68.59 | loss  3.57 | ppl    35.36\n",
            "| epoch   1 | 317200/389982 batches | lr 5.00 | ms/batch 68.39 | loss  3.49 | ppl    32.72\n",
            "| epoch   1 | 317400/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.43 | ppl    30.81\n",
            "| epoch   1 | 317600/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.42 | ppl    30.42\n",
            "| epoch   1 | 317800/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.45 | ppl    31.61\n",
            "| epoch   1 | 318000/389982 batches | lr 5.00 | ms/batch 67.55 | loss  3.46 | ppl    31.93\n",
            "| epoch   1 | 318200/389982 batches | lr 5.00 | ms/batch 71.77 | loss  3.47 | ppl    32.21\n",
            "| epoch   1 | 318400/389982 batches | lr 5.00 | ms/batch 66.68 | loss  3.44 | ppl    31.13\n",
            "| epoch   1 | 318600/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.46 | ppl    31.93\n",
            "| epoch   1 | 318800/389982 batches | lr 5.00 | ms/batch 68.81 | loss  3.49 | ppl    32.69\n",
            "| epoch   1 | 319000/389982 batches | lr 5.00 | ms/batch 68.41 | loss  3.41 | ppl    30.17\n",
            "| epoch   1 | 319200/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.43 | ppl    30.79\n",
            "| epoch   1 | 319400/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.41 | ppl    30.14\n",
            "| epoch   1 | 319600/389982 batches | lr 5.00 | ms/batch 68.03 | loss  3.36 | ppl    28.77\n",
            "| epoch   1 | 319800/389982 batches | lr 5.00 | ms/batch 66.94 | loss  3.46 | ppl    31.72\n",
            "| epoch   1 | 320000/389982 batches | lr 5.00 | ms/batch 67.18 | loss  3.46 | ppl    31.72\n",
            "| epoch   1 | 320200/389982 batches | lr 5.00 | ms/batch 74.79 | loss  3.43 | ppl    30.98\n",
            "| epoch   1 | 320400/389982 batches | lr 5.00 | ms/batch 66.76 | loss  3.47 | ppl    32.05\n",
            "| epoch   1 | 320600/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.43 | ppl    30.96\n",
            "| epoch   1 | 320800/389982 batches | lr 5.00 | ms/batch 68.83 | loss  3.49 | ppl    32.90\n",
            "| epoch   1 | 321000/389982 batches | lr 5.00 | ms/batch 66.36 | loss  3.49 | ppl    32.80\n",
            "| epoch   1 | 321200/389982 batches | lr 5.00 | ms/batch 67.48 | loss  3.42 | ppl    30.51\n",
            "| epoch   1 | 321400/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.47 | ppl    32.14\n",
            "| epoch   1 | 321600/389982 batches | lr 5.00 | ms/batch 67.39 | loss  3.42 | ppl    30.50\n",
            "| epoch   1 | 321800/389982 batches | lr 5.00 | ms/batch 66.33 | loss  3.48 | ppl    32.51\n",
            "| epoch   1 | 322000/389982 batches | lr 5.00 | ms/batch 69.67 | loss  3.42 | ppl    30.63\n",
            "| epoch   1 | 322200/389982 batches | lr 5.00 | ms/batch 72.15 | loss  3.45 | ppl    31.51\n",
            "| epoch   1 | 322400/389982 batches | lr 5.00 | ms/batch 69.09 | loss  3.35 | ppl    28.52\n",
            "| epoch   1 | 322600/389982 batches | lr 5.00 | ms/batch 69.38 | loss  3.59 | ppl    36.39\n",
            "| epoch   1 | 322800/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.51 | ppl    33.38\n",
            "| epoch   1 | 323000/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.48 | ppl    32.31\n",
            "| epoch   1 | 323200/389982 batches | lr 5.00 | ms/batch 67.11 | loss  3.46 | ppl    31.97\n",
            "| epoch   1 | 323400/389982 batches | lr 5.00 | ms/batch 67.60 | loss  3.39 | ppl    29.58\n",
            "| epoch   1 | 323600/389982 batches | lr 5.00 | ms/batch 68.89 | loss  3.43 | ppl    30.76\n",
            "| epoch   1 | 323800/389982 batches | lr 5.00 | ms/batch 68.92 | loss  3.38 | ppl    29.39\n",
            "| epoch   1 | 324000/389982 batches | lr 5.00 | ms/batch 66.83 | loss  3.46 | ppl    31.81\n",
            "| epoch   1 | 324200/389982 batches | lr 5.00 | ms/batch 72.82 | loss  3.42 | ppl    30.66\n",
            "| epoch   1 | 324400/389982 batches | lr 5.00 | ms/batch 66.58 | loss  3.54 | ppl    34.61\n",
            "| epoch   1 | 324600/389982 batches | lr 5.00 | ms/batch 68.06 | loss  3.47 | ppl    32.20\n",
            "| epoch   1 | 324800/389982 batches | lr 5.00 | ms/batch 69.02 | loss  3.49 | ppl    32.84\n",
            "| epoch   1 | 325000/389982 batches | lr 5.00 | ms/batch 66.10 | loss  3.55 | ppl    34.68\n",
            "| epoch   1 | 325200/389982 batches | lr 5.00 | ms/batch 66.22 | loss  3.51 | ppl    33.40\n",
            "| epoch   1 | 325400/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.45 | ppl    31.60\n",
            "| epoch   1 | 325600/389982 batches | lr 5.00 | ms/batch 67.49 | loss  3.47 | ppl    32.14\n",
            "| epoch   1 | 325800/389982 batches | lr 5.00 | ms/batch 66.94 | loss  3.45 | ppl    31.48\n",
            "| epoch   1 | 326000/389982 batches | lr 5.00 | ms/batch 68.14 | loss  3.47 | ppl    32.22\n",
            "| epoch   1 | 326200/389982 batches | lr 5.00 | ms/batch 72.56 | loss  3.45 | ppl    31.63\n",
            "| epoch   1 | 326400/389982 batches | lr 5.00 | ms/batch 67.18 | loss  3.46 | ppl    31.84\n",
            "| epoch   1 | 326600/389982 batches | lr 5.00 | ms/batch 68.04 | loss  3.49 | ppl    32.84\n",
            "| epoch   1 | 326800/389982 batches | lr 5.00 | ms/batch 66.53 | loss  3.47 | ppl    32.07\n",
            "| epoch   1 | 327000/389982 batches | lr 5.00 | ms/batch 69.71 | loss  3.34 | ppl    28.14\n",
            "| epoch   1 | 327200/389982 batches | lr 5.00 | ms/batch 67.62 | loss  3.44 | ppl    31.30\n",
            "| epoch   1 | 327400/389982 batches | lr 5.00 | ms/batch 66.47 | loss  3.45 | ppl    31.57\n",
            "| epoch   1 | 327600/389982 batches | lr 5.00 | ms/batch 69.52 | loss  3.41 | ppl    30.23\n",
            "| epoch   1 | 327800/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.57 | ppl    35.62\n",
            "| epoch   1 | 328000/389982 batches | lr 5.00 | ms/batch 68.79 | loss  3.39 | ppl    29.63\n",
            "| epoch   1 | 328200/389982 batches | lr 5.00 | ms/batch 72.96 | loss  3.36 | ppl    28.88\n",
            "| epoch   1 | 328400/389982 batches | lr 5.00 | ms/batch 67.20 | loss  3.42 | ppl    30.62\n",
            "| epoch   1 | 328600/389982 batches | lr 5.00 | ms/batch 67.30 | loss  3.40 | ppl    30.07\n",
            "| epoch   1 | 328800/389982 batches | lr 5.00 | ms/batch 68.35 | loss  3.39 | ppl    29.53\n",
            "| epoch   1 | 329000/389982 batches | lr 5.00 | ms/batch 66.38 | loss  3.48 | ppl    32.38\n",
            "| epoch   1 | 329200/389982 batches | lr 5.00 | ms/batch 67.62 | loss  3.46 | ppl    31.83\n",
            "| epoch   1 | 329400/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.42 | ppl    30.72\n",
            "| epoch   1 | 329600/389982 batches | lr 5.00 | ms/batch 68.77 | loss  3.40 | ppl    29.91\n",
            "| epoch   1 | 329800/389982 batches | lr 5.00 | ms/batch 67.51 | loss  3.42 | ppl    30.57\n",
            "| epoch   1 | 330000/389982 batches | lr 5.00 | ms/batch 67.50 | loss  3.44 | ppl    31.13\n",
            "| epoch   1 | 330200/389982 batches | lr 5.00 | ms/batch 72.09 | loss  3.44 | ppl    31.23\n",
            "| epoch   1 | 330400/389982 batches | lr 5.00 | ms/batch 66.69 | loss  3.46 | ppl    31.91\n",
            "| epoch   1 | 330600/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.40 | ppl    29.94\n",
            "| epoch   1 | 330800/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.40 | ppl    30.10\n",
            "| epoch   1 | 331000/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.38 | ppl    29.30\n",
            "| epoch   1 | 331200/389982 batches | lr 5.00 | ms/batch 67.58 | loss  3.51 | ppl    33.58\n",
            "| epoch   1 | 331400/389982 batches | lr 5.00 | ms/batch 68.33 | loss  3.42 | ppl    30.67\n",
            "| epoch   1 | 331600/389982 batches | lr 5.00 | ms/batch 66.71 | loss  3.57 | ppl    35.42\n",
            "| epoch   1 | 331800/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.38 | ppl    29.41\n",
            "| epoch   1 | 332000/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.42 | ppl    30.72\n",
            "| epoch   1 | 332200/389982 batches | lr 5.00 | ms/batch 73.49 | loss  3.36 | ppl    28.74\n",
            "| epoch   1 | 332400/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.50 | ppl    33.12\n",
            "| epoch   1 | 332600/389982 batches | lr 5.00 | ms/batch 70.01 | loss  3.37 | ppl    29.11\n",
            "| epoch   1 | 332800/389982 batches | lr 5.00 | ms/batch 68.04 | loss  3.42 | ppl    30.57\n",
            "| epoch   1 | 333000/389982 batches | lr 5.00 | ms/batch 67.38 | loss  3.40 | ppl    30.00\n",
            "| epoch   1 | 333200/389982 batches | lr 5.00 | ms/batch 67.93 | loss  3.41 | ppl    30.34\n",
            "| epoch   1 | 333400/389982 batches | lr 5.00 | ms/batch 67.30 | loss  3.42 | ppl    30.46\n",
            "| epoch   1 | 333600/389982 batches | lr 5.00 | ms/batch 66.14 | loss  3.46 | ppl    31.96\n",
            "| epoch   1 | 333800/389982 batches | lr 5.00 | ms/batch 68.70 | loss  3.39 | ppl    29.73\n",
            "| epoch   1 | 334000/389982 batches | lr 5.00 | ms/batch 66.73 | loss  3.49 | ppl    32.83\n",
            "| epoch   1 | 334200/389982 batches | lr 5.00 | ms/batch 71.66 | loss  3.48 | ppl    32.48\n",
            "| epoch   1 | 334400/389982 batches | lr 5.00 | ms/batch 67.34 | loss  3.43 | ppl    30.92\n",
            "| epoch   1 | 334600/389982 batches | lr 5.00 | ms/batch 67.08 | loss  3.42 | ppl    30.64\n",
            "| epoch   1 | 334800/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.46 | ppl    31.94\n",
            "| epoch   1 | 335000/389982 batches | lr 5.00 | ms/batch 67.93 | loss  3.42 | ppl    30.50\n",
            "| epoch   1 | 335200/389982 batches | lr 5.00 | ms/batch 68.91 | loss  3.35 | ppl    28.53\n",
            "| epoch   1 | 335400/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.45 | ppl    31.65\n",
            "| epoch   1 | 335600/389982 batches | lr 5.00 | ms/batch 66.85 | loss  3.48 | ppl    32.32\n",
            "| epoch   1 | 335800/389982 batches | lr 5.00 | ms/batch 67.30 | loss  3.41 | ppl    30.28\n",
            "| epoch   1 | 336000/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.42 | ppl    30.60\n",
            "| epoch   1 | 336200/389982 batches | lr 5.00 | ms/batch 72.33 | loss  3.52 | ppl    33.86\n",
            "| epoch   1 | 336400/389982 batches | lr 5.00 | ms/batch 67.59 | loss  3.40 | ppl    29.92\n",
            "| epoch   1 | 336600/389982 batches | lr 5.00 | ms/batch 68.02 | loss  3.44 | ppl    31.14\n",
            "| epoch   1 | 336800/389982 batches | lr 5.00 | ms/batch 67.96 | loss  3.47 | ppl    32.01\n",
            "| epoch   1 | 337000/389982 batches | lr 5.00 | ms/batch 66.84 | loss  3.46 | ppl    31.74\n",
            "| epoch   1 | 337200/389982 batches | lr 5.00 | ms/batch 66.55 | loss  3.46 | ppl    31.74\n",
            "| epoch   1 | 337400/389982 batches | lr 5.00 | ms/batch 67.68 | loss  3.42 | ppl    30.56\n",
            "| epoch   1 | 337600/389982 batches | lr 5.00 | ms/batch 67.34 | loss  3.42 | ppl    30.62\n",
            "| epoch   1 | 337800/389982 batches | lr 5.00 | ms/batch 67.18 | loss  3.42 | ppl    30.57\n",
            "| epoch   1 | 338000/389982 batches | lr 5.00 | ms/batch 68.77 | loss  3.39 | ppl    29.72\n",
            "| epoch   1 | 338200/389982 batches | lr 5.00 | ms/batch 73.47 | loss  3.33 | ppl    27.95\n",
            "| epoch   1 | 338400/389982 batches | lr 5.00 | ms/batch 66.06 | loss  3.47 | ppl    32.06\n",
            "| epoch   1 | 338600/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.40 | ppl    29.88\n",
            "| epoch   1 | 338800/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.45 | ppl    31.49\n",
            "| epoch   1 | 339000/389982 batches | lr 5.00 | ms/batch 67.76 | loss  3.42 | ppl    30.49\n",
            "| epoch   1 | 339200/389982 batches | lr 5.00 | ms/batch 67.33 | loss  3.42 | ppl    30.43\n",
            "| epoch   1 | 339400/389982 batches | lr 5.00 | ms/batch 67.59 | loss  3.50 | ppl    33.18\n",
            "| epoch   1 | 339600/389982 batches | lr 5.00 | ms/batch 67.03 | loss  3.42 | ppl    30.60\n",
            "| epoch   1 | 339800/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.40 | ppl    30.08\n",
            "| epoch   1 | 340000/389982 batches | lr 5.00 | ms/batch 68.17 | loss  3.38 | ppl    29.26\n",
            "| epoch   1 | 340200/389982 batches | lr 5.00 | ms/batch 71.65 | loss  3.44 | ppl    31.08\n",
            "| epoch   1 | 340400/389982 batches | lr 5.00 | ms/batch 66.86 | loss  3.46 | ppl    31.79\n",
            "| epoch   1 | 340600/389982 batches | lr 5.00 | ms/batch 66.59 | loss  3.50 | ppl    33.05\n",
            "| epoch   1 | 340800/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.42 | ppl    30.62\n",
            "| epoch   1 | 341000/389982 batches | lr 5.00 | ms/batch 67.41 | loss  3.41 | ppl    30.25\n",
            "| epoch   1 | 341200/389982 batches | lr 5.00 | ms/batch 67.54 | loss  3.46 | ppl    31.68\n",
            "| epoch   1 | 341400/389982 batches | lr 5.00 | ms/batch 67.87 | loss  3.41 | ppl    30.33\n",
            "| epoch   1 | 341600/389982 batches | lr 5.00 | ms/batch 68.71 | loss  3.37 | ppl    29.01\n",
            "| epoch   1 | 341800/389982 batches | lr 5.00 | ms/batch 66.96 | loss  3.43 | ppl    30.95\n",
            "| epoch   1 | 342000/389982 batches | lr 5.00 | ms/batch 66.77 | loss  3.45 | ppl    31.60\n",
            "| epoch   1 | 342200/389982 batches | lr 5.00 | ms/batch 72.19 | loss  3.45 | ppl    31.43\n",
            "| epoch   1 | 342400/389982 batches | lr 5.00 | ms/batch 68.55 | loss  3.37 | ppl    28.97\n",
            "| epoch   1 | 342600/389982 batches | lr 5.00 | ms/batch 67.45 | loss  3.47 | ppl    32.12\n",
            "| epoch   1 | 342800/389982 batches | lr 5.00 | ms/batch 67.28 | loss  3.48 | ppl    32.33\n",
            "| epoch   1 | 343000/389982 batches | lr 5.00 | ms/batch 67.98 | loss  3.48 | ppl    32.46\n",
            "| epoch   1 | 343200/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.47 | ppl    32.29\n",
            "| epoch   1 | 343400/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.45 | ppl    31.57\n",
            "| epoch   1 | 343600/389982 batches | lr 5.00 | ms/batch 68.88 | loss  3.40 | ppl    29.89\n",
            "| epoch   1 | 343800/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.45 | ppl    31.58\n",
            "| epoch   1 | 344000/389982 batches | lr 5.00 | ms/batch 67.78 | loss  3.46 | ppl    31.70\n",
            "| epoch   1 | 344200/389982 batches | lr 5.00 | ms/batch 72.74 | loss  3.38 | ppl    29.51\n",
            "| epoch   1 | 344400/389982 batches | lr 5.00 | ms/batch 67.78 | loss  3.45 | ppl    31.52\n",
            "| epoch   1 | 344600/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.46 | ppl    31.83\n",
            "| epoch   1 | 344800/389982 batches | lr 5.00 | ms/batch 66.68 | loss  3.55 | ppl    34.78\n",
            "| epoch   1 | 345000/389982 batches | lr 5.00 | ms/batch 69.50 | loss  3.39 | ppl    29.59\n",
            "| epoch   1 | 345200/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.44 | ppl    31.09\n",
            "| epoch   1 | 345400/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.53 | ppl    34.13\n",
            "| epoch   1 | 345600/389982 batches | lr 5.00 | ms/batch 67.68 | loss  3.43 | ppl    30.89\n",
            "| epoch   1 | 345800/389982 batches | lr 5.00 | ms/batch 66.62 | loss  3.52 | ppl    33.74\n",
            "| epoch   1 | 346000/389982 batches | lr 5.00 | ms/batch 68.24 | loss  3.49 | ppl    32.88\n",
            "| epoch   1 | 346200/389982 batches | lr 5.00 | ms/batch 72.14 | loss  3.45 | ppl    31.38\n",
            "| epoch   1 | 346400/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.39 | ppl    29.77\n",
            "| epoch   1 | 346600/389982 batches | lr 5.00 | ms/batch 67.35 | loss  3.42 | ppl    30.45\n",
            "| epoch   1 | 346800/389982 batches | lr 5.00 | ms/batch 67.47 | loss  3.45 | ppl    31.44\n",
            "| epoch   1 | 347000/389982 batches | lr 5.00 | ms/batch 67.57 | loss  3.44 | ppl    31.07\n",
            "| epoch   1 | 347200/389982 batches | lr 5.00 | ms/batch 69.05 | loss  3.42 | ppl    30.64\n",
            "| epoch   1 | 347400/389982 batches | lr 5.00 | ms/batch 67.61 | loss  3.39 | ppl    29.72\n",
            "| epoch   1 | 347600/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.41 | ppl    30.13\n",
            "| epoch   1 | 347800/389982 batches | lr 5.00 | ms/batch 66.92 | loss  3.43 | ppl    30.92\n",
            "| epoch   1 | 348000/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.42 | ppl    30.51\n",
            "| epoch   1 | 348200/389982 batches | lr 5.00 | ms/batch 71.50 | loss  3.53 | ppl    34.16\n",
            "| epoch   1 | 348400/389982 batches | lr 5.00 | ms/batch 67.07 | loss  3.48 | ppl    32.41\n",
            "| epoch   1 | 348600/389982 batches | lr 5.00 | ms/batch 67.50 | loss  3.41 | ppl    30.29\n",
            "| epoch   1 | 348800/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.44 | ppl    31.18\n",
            "| epoch   1 | 349000/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.46 | ppl    31.85\n",
            "| epoch   1 | 349200/389982 batches | lr 5.00 | ms/batch 67.89 | loss  3.41 | ppl    30.41\n",
            "| epoch   1 | 349400/389982 batches | lr 5.00 | ms/batch 68.97 | loss  3.45 | ppl    31.56\n",
            "| epoch   1 | 349600/389982 batches | lr 5.00 | ms/batch 67.29 | loss  3.43 | ppl    30.80\n",
            "| epoch   1 | 349800/389982 batches | lr 5.00 | ms/batch 68.07 | loss  3.49 | ppl    32.91\n",
            "| epoch   1 | 350000/389982 batches | lr 5.00 | ms/batch 67.13 | loss  3.48 | ppl    32.39\n",
            "| epoch   1 | 350200/389982 batches | lr 5.00 | ms/batch 72.47 | loss  3.44 | ppl    31.08\n",
            "| epoch   1 | 350400/389982 batches | lr 5.00 | ms/batch 67.22 | loss  3.45 | ppl    31.35\n",
            "| epoch   1 | 350600/389982 batches | lr 5.00 | ms/batch 68.46 | loss  3.40 | ppl    30.08\n",
            "| epoch   1 | 350800/389982 batches | lr 5.00 | ms/batch 68.08 | loss  3.38 | ppl    29.23\n",
            "| epoch   1 | 351000/389982 batches | lr 5.00 | ms/batch 66.82 | loss  3.47 | ppl    32.25\n",
            "| epoch   1 | 351200/389982 batches | lr 5.00 | ms/batch 66.57 | loss  3.50 | ppl    33.21\n",
            "| epoch   1 | 351400/389982 batches | lr 5.00 | ms/batch 66.73 | loss  3.45 | ppl    31.56\n",
            "| epoch   1 | 351600/389982 batches | lr 5.00 | ms/batch 67.58 | loss  3.43 | ppl    30.95\n",
            "| epoch   1 | 351800/389982 batches | lr 5.00 | ms/batch 66.24 | loss  3.53 | ppl    34.04\n",
            "| epoch   1 | 352000/389982 batches | lr 5.00 | ms/batch 66.94 | loss  3.53 | ppl    34.07\n",
            "| epoch   1 | 352200/389982 batches | lr 5.00 | ms/batch 72.19 | loss  3.49 | ppl    32.64\n",
            "| epoch   1 | 352400/389982 batches | lr 5.00 | ms/batch 67.86 | loss  3.44 | ppl    31.33\n",
            "| epoch   1 | 352600/389982 batches | lr 5.00 | ms/batch 69.58 | loss  3.44 | ppl    31.06\n",
            "| epoch   1 | 352800/389982 batches | lr 5.00 | ms/batch 66.97 | loss  3.45 | ppl    31.58\n",
            "| epoch   1 | 353000/389982 batches | lr 5.00 | ms/batch 66.59 | loss  3.45 | ppl    31.48\n",
            "| epoch   1 | 353200/389982 batches | lr 5.00 | ms/batch 68.77 | loss  3.38 | ppl    29.25\n",
            "| epoch   1 | 353400/389982 batches | lr 5.00 | ms/batch 66.68 | loss  3.49 | ppl    32.77\n",
            "| epoch   1 | 353600/389982 batches | lr 5.00 | ms/batch 68.29 | loss  3.41 | ppl    30.13\n",
            "| epoch   1 | 353800/389982 batches | lr 5.00 | ms/batch 67.20 | loss  3.45 | ppl    31.59\n",
            "| epoch   1 | 354000/389982 batches | lr 5.00 | ms/batch 66.63 | loss  3.60 | ppl    36.63\n",
            "| epoch   1 | 354200/389982 batches | lr 5.00 | ms/batch 73.15 | loss  3.41 | ppl    30.26\n",
            "| epoch   1 | 354400/389982 batches | lr 5.00 | ms/batch 67.48 | loss  3.52 | ppl    33.65\n",
            "| epoch   1 | 354600/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.49 | ppl    32.91\n",
            "| epoch   1 | 354800/389982 batches | lr 5.00 | ms/batch 66.90 | loss  3.51 | ppl    33.56\n",
            "| epoch   1 | 355000/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.48 | ppl    32.39\n",
            "| epoch   1 | 355200/389982 batches | lr 5.00 | ms/batch 67.99 | loss  3.40 | ppl    29.88\n",
            "| epoch   1 | 355400/389982 batches | lr 5.00 | ms/batch 66.50 | loss  3.47 | ppl    32.06\n",
            "| epoch   1 | 355600/389982 batches | lr 5.00 | ms/batch 68.07 | loss  3.41 | ppl    30.27\n",
            "| epoch   1 | 355800/389982 batches | lr 5.00 | ms/batch 69.87 | loss  3.32 | ppl    27.65\n",
            "| epoch   1 | 356000/389982 batches | lr 5.00 | ms/batch 68.52 | loss  3.35 | ppl    28.50\n",
            "| epoch   1 | 356200/389982 batches | lr 5.00 | ms/batch 72.24 | loss  3.50 | ppl    33.24\n",
            "| epoch   1 | 356400/389982 batches | lr 5.00 | ms/batch 68.12 | loss  3.39 | ppl    29.77\n",
            "| epoch   1 | 356600/389982 batches | lr 5.00 | ms/batch 68.69 | loss  3.39 | ppl    29.67\n",
            "| epoch   1 | 356800/389982 batches | lr 5.00 | ms/batch 67.00 | loss  3.44 | ppl    31.19\n",
            "| epoch   1 | 357000/389982 batches | lr 5.00 | ms/batch 67.55 | loss  3.42 | ppl    30.50\n",
            "| epoch   1 | 357200/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.43 | ppl    31.02\n",
            "| epoch   1 | 357400/389982 batches | lr 5.00 | ms/batch 67.80 | loss  3.44 | ppl    31.23\n",
            "| epoch   1 | 357600/389982 batches | lr 5.00 | ms/batch 67.65 | loss  3.41 | ppl    30.20\n",
            "| epoch   1 | 357800/389982 batches | lr 5.00 | ms/batch 68.15 | loss  3.37 | ppl    29.17\n",
            "| epoch   1 | 358000/389982 batches | lr 5.00 | ms/batch 68.32 | loss  3.39 | ppl    29.76\n",
            "| epoch   1 | 358200/389982 batches | lr 5.00 | ms/batch 72.25 | loss  3.58 | ppl    35.95\n",
            "| epoch   1 | 358400/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.45 | ppl    31.35\n",
            "| epoch   1 | 358600/389982 batches | lr 5.00 | ms/batch 66.90 | loss  3.49 | ppl    32.90\n",
            "| epoch   1 | 358800/389982 batches | lr 5.00 | ms/batch 66.95 | loss  3.48 | ppl    32.35\n",
            "| epoch   1 | 359000/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.44 | ppl    31.10\n",
            "| epoch   1 | 359200/389982 batches | lr 5.00 | ms/batch 67.11 | loss  3.42 | ppl    30.60\n",
            "| epoch   1 | 359400/389982 batches | lr 5.00 | ms/batch 66.89 | loss  3.45 | ppl    31.40\n",
            "| epoch   1 | 359600/389982 batches | lr 5.00 | ms/batch 66.93 | loss  3.47 | ppl    32.19\n",
            "| epoch   1 | 359800/389982 batches | lr 5.00 | ms/batch 67.43 | loss  3.46 | ppl    31.67\n",
            "| epoch   1 | 360000/389982 batches | lr 5.00 | ms/batch 66.90 | loss  3.46 | ppl    31.86\n",
            "| epoch   1 | 360200/389982 batches | lr 5.00 | ms/batch 71.30 | loss  3.46 | ppl    31.97\n",
            "| epoch   1 | 360400/389982 batches | lr 5.00 | ms/batch 66.25 | loss  3.57 | ppl    35.53\n",
            "| epoch   1 | 360600/389982 batches | lr 5.00 | ms/batch 67.30 | loss  3.43 | ppl    30.85\n",
            "| epoch   1 | 360800/389982 batches | lr 5.00 | ms/batch 65.90 | loss  3.49 | ppl    32.88\n",
            "| epoch   1 | 361000/389982 batches | lr 5.00 | ms/batch 66.79 | loss  3.54 | ppl    34.47\n",
            "| epoch   1 | 361200/389982 batches | lr 5.00 | ms/batch 67.85 | loss  3.40 | ppl    30.06\n",
            "| epoch   1 | 361400/389982 batches | lr 5.00 | ms/batch 67.91 | loss  3.40 | ppl    30.08\n",
            "| epoch   1 | 361600/389982 batches | lr 5.00 | ms/batch 66.82 | loss  3.45 | ppl    31.43\n",
            "| epoch   1 | 361800/389982 batches | lr 5.00 | ms/batch 67.90 | loss  3.45 | ppl    31.39\n",
            "| epoch   1 | 362000/389982 batches | lr 5.00 | ms/batch 70.28 | loss  3.35 | ppl    28.53\n",
            "| epoch   1 | 362200/389982 batches | lr 5.00 | ms/batch 72.85 | loss  3.41 | ppl    30.39\n",
            "| epoch   1 | 362400/389982 batches | lr 5.00 | ms/batch 67.74 | loss  3.41 | ppl    30.12\n",
            "| epoch   1 | 362600/389982 batches | lr 5.00 | ms/batch 67.09 | loss  3.50 | ppl    33.02\n",
            "| epoch   1 | 362800/389982 batches | lr 5.00 | ms/batch 66.99 | loss  3.46 | ppl    31.75\n",
            "| epoch   1 | 363000/389982 batches | lr 5.00 | ms/batch 67.31 | loss  3.48 | ppl    32.36\n",
            "| epoch   1 | 363200/389982 batches | lr 5.00 | ms/batch 67.49 | loss  3.44 | ppl    31.10\n",
            "| epoch   1 | 363400/389982 batches | lr 5.00 | ms/batch 68.08 | loss  3.43 | ppl    30.93\n",
            "| epoch   1 | 363600/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.47 | ppl    32.17\n",
            "| epoch   1 | 363800/389982 batches | lr 5.00 | ms/batch 67.20 | loss  3.57 | ppl    35.40\n",
            "| epoch   1 | 364000/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.43 | ppl    30.91\n",
            "| epoch   1 | 364200/389982 batches | lr 5.00 | ms/batch 72.57 | loss  3.43 | ppl    30.98\n",
            "| epoch   1 | 364400/389982 batches | lr 5.00 | ms/batch 68.55 | loss  3.35 | ppl    28.37\n",
            "| epoch   1 | 364600/389982 batches | lr 5.00 | ms/batch 66.84 | loss  3.49 | ppl    32.87\n",
            "| epoch   1 | 364800/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.45 | ppl    31.45\n",
            "| epoch   1 | 365000/389982 batches | lr 5.00 | ms/batch 66.80 | loss  3.47 | ppl    32.06\n",
            "| epoch   1 | 365200/389982 batches | lr 5.00 | ms/batch 66.56 | loss  3.48 | ppl    32.54\n",
            "| epoch   1 | 365400/389982 batches | lr 5.00 | ms/batch 67.25 | loss  3.44 | ppl    31.06\n",
            "| epoch   1 | 365600/389982 batches | lr 5.00 | ms/batch 68.70 | loss  3.41 | ppl    30.19\n",
            "| epoch   1 | 365800/389982 batches | lr 5.00 | ms/batch 66.54 | loss  3.50 | ppl    32.98\n",
            "| epoch   1 | 366000/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.45 | ppl    31.60\n",
            "| epoch   1 | 366200/389982 batches | lr 5.00 | ms/batch 73.84 | loss  3.38 | ppl    29.30\n",
            "| epoch   1 | 366400/389982 batches | lr 5.00 | ms/batch 67.68 | loss  3.41 | ppl    30.25\n",
            "| epoch   1 | 366600/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.39 | ppl    29.75\n",
            "| epoch   1 | 366800/389982 batches | lr 5.00 | ms/batch 66.84 | loss  3.56 | ppl    35.31\n",
            "| epoch   1 | 367000/389982 batches | lr 5.00 | ms/batch 65.85 | loss  3.50 | ppl    33.17\n",
            "| epoch   1 | 367200/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.44 | ppl    31.30\n",
            "| epoch   1 | 367400/389982 batches | lr 5.00 | ms/batch 67.33 | loss  3.42 | ppl    30.54\n",
            "| epoch   1 | 367600/389982 batches | lr 5.00 | ms/batch 67.57 | loss  3.43 | ppl    30.75\n",
            "| epoch   1 | 367800/389982 batches | lr 5.00 | ms/batch 67.37 | loss  3.46 | ppl    31.77\n",
            "| epoch   1 | 368000/389982 batches | lr 5.00 | ms/batch 68.85 | loss  3.40 | ppl    29.90\n",
            "| epoch   1 | 368200/389982 batches | lr 5.00 | ms/batch 72.02 | loss  3.46 | ppl    31.84\n",
            "| epoch   1 | 368400/389982 batches | lr 5.00 | ms/batch 67.33 | loss  3.46 | ppl    31.75\n",
            "| epoch   1 | 368600/389982 batches | lr 5.00 | ms/batch 68.42 | loss  3.43 | ppl    30.90\n",
            "| epoch   1 | 368800/389982 batches | lr 5.00 | ms/batch 67.88 | loss  3.39 | ppl    29.72\n",
            "| epoch   1 | 369000/389982 batches | lr 5.00 | ms/batch 68.01 | loss  3.43 | ppl    30.75\n",
            "| epoch   1 | 369200/389982 batches | lr 5.00 | ms/batch 68.98 | loss  3.41 | ppl    30.14\n",
            "| epoch   1 | 369400/389982 batches | lr 5.00 | ms/batch 68.48 | loss  3.40 | ppl    29.86\n",
            "| epoch   1 | 369600/389982 batches | lr 5.00 | ms/batch 67.04 | loss  3.40 | ppl    29.95\n",
            "| epoch   1 | 369800/389982 batches | lr 5.00 | ms/batch 67.56 | loss  3.42 | ppl    30.55\n",
            "| epoch   1 | 370000/389982 batches | lr 5.00 | ms/batch 66.13 | loss  3.48 | ppl    32.53\n",
            "| epoch   1 | 370200/389982 batches | lr 5.00 | ms/batch 72.68 | loss  3.44 | ppl    31.33\n",
            "| epoch   1 | 370400/389982 batches | lr 5.00 | ms/batch 67.73 | loss  3.43 | ppl    30.97\n",
            "| epoch   1 | 370600/389982 batches | lr 5.00 | ms/batch 66.95 | loss  3.45 | ppl    31.46\n",
            "| epoch   1 | 370800/389982 batches | lr 5.00 | ms/batch 67.94 | loss  3.39 | ppl    29.77\n",
            "| epoch   1 | 371000/389982 batches | lr 5.00 | ms/batch 68.33 | loss  3.55 | ppl    34.94\n",
            "| epoch   1 | 371200/389982 batches | lr 5.00 | ms/batch 68.13 | loss  3.37 | ppl    29.20\n",
            "| epoch   1 | 371400/389982 batches | lr 5.00 | ms/batch 67.06 | loss  3.44 | ppl    31.21\n",
            "| epoch   1 | 371600/389982 batches | lr 5.00 | ms/batch 70.04 | loss  3.44 | ppl    31.10\n",
            "| epoch   1 | 371800/389982 batches | lr 5.00 | ms/batch 68.10 | loss  3.42 | ppl    30.61\n",
            "| epoch   1 | 372000/389982 batches | lr 5.00 | ms/batch 68.71 | loss  3.35 | ppl    28.60\n",
            "| epoch   1 | 372200/389982 batches | lr 5.00 | ms/batch 72.05 | loss  3.48 | ppl    32.30\n",
            "| epoch   1 | 372400/389982 batches | lr 5.00 | ms/batch 68.31 | loss  3.39 | ppl    29.70\n",
            "| epoch   1 | 372600/389982 batches | lr 5.00 | ms/batch 68.56 | loss  3.40 | ppl    29.83\n",
            "| epoch   1 | 372800/389982 batches | lr 5.00 | ms/batch 66.99 | loss  3.46 | ppl    31.90\n",
            "| epoch   1 | 373000/389982 batches | lr 5.00 | ms/batch 68.30 | loss  3.41 | ppl    30.40\n",
            "| epoch   1 | 373200/389982 batches | lr 5.00 | ms/batch 67.95 | loss  3.48 | ppl    32.35\n",
            "| epoch   1 | 373400/389982 batches | lr 5.00 | ms/batch 68.73 | loss  3.40 | ppl    29.95\n",
            "| epoch   1 | 373600/389982 batches | lr 5.00 | ms/batch 67.52 | loss  3.43 | ppl    30.77\n",
            "| epoch   1 | 373800/389982 batches | lr 5.00 | ms/batch 67.29 | loss  3.49 | ppl    32.63\n",
            "| epoch   1 | 374000/389982 batches | lr 5.00 | ms/batch 67.11 | loss  3.46 | ppl    31.74\n",
            "| epoch   1 | 374200/389982 batches | lr 5.00 | ms/batch 73.27 | loss  3.46 | ppl    31.90\n",
            "| epoch   1 | 374400/389982 batches | lr 5.00 | ms/batch 68.66 | loss  3.42 | ppl    30.59\n",
            "| epoch   1 | 374600/389982 batches | lr 5.00 | ms/batch 67.82 | loss  3.47 | ppl    32.08\n",
            "| epoch   1 | 374800/389982 batches | lr 5.00 | ms/batch 68.90 | loss  3.42 | ppl    30.46\n",
            "| epoch   1 | 375000/389982 batches | lr 5.00 | ms/batch 67.92 | loss  3.41 | ppl    30.12\n",
            "| epoch   1 | 375200/389982 batches | lr 5.00 | ms/batch 65.76 | loss  3.50 | ppl    33.27\n",
            "| epoch   1 | 375400/389982 batches | lr 5.00 | ms/batch 66.82 | loss  3.42 | ppl    30.68\n",
            "| epoch   1 | 375600/389982 batches | lr 5.00 | ms/batch 67.77 | loss  3.42 | ppl    30.55\n",
            "| epoch   1 | 375800/389982 batches | lr 5.00 | ms/batch 67.44 | loss  3.42 | ppl    30.42\n",
            "| epoch   1 | 376000/389982 batches | lr 5.00 | ms/batch 68.45 | loss  3.42 | ppl    30.57\n",
            "| epoch   1 | 376200/389982 batches | lr 5.00 | ms/batch 73.74 | loss  3.40 | ppl    30.09\n",
            "| epoch   1 | 376400/389982 batches | lr 5.00 | ms/batch 66.62 | loss  3.49 | ppl    32.93\n",
            "| epoch   1 | 376600/389982 batches | lr 5.00 | ms/batch 67.70 | loss  3.45 | ppl    31.51\n",
            "| epoch   1 | 376800/389982 batches | lr 5.00 | ms/batch 66.28 | loss  3.49 | ppl    32.73\n",
            "| epoch   1 | 377000/389982 batches | lr 5.00 | ms/batch 67.12 | loss  3.44 | ppl    31.07\n",
            "| epoch   1 | 377200/389982 batches | lr 5.00 | ms/batch 66.37 | loss  3.56 | ppl    35.00\n",
            "| epoch   1 | 377400/389982 batches | lr 5.00 | ms/batch 67.88 | loss  3.43 | ppl    30.89\n",
            "| epoch   1 | 377600/389982 batches | lr 5.00 | ms/batch 68.06 | loss  3.40 | ppl    29.82\n",
            "| epoch   1 | 377800/389982 batches | lr 5.00 | ms/batch 69.15 | loss  3.39 | ppl    29.59\n",
            "| epoch   1 | 378000/389982 batches | lr 5.00 | ms/batch 66.55 | loss  3.52 | ppl    33.75\n",
            "| epoch   1 | 378200/389982 batches | lr 5.00 | ms/batch 73.28 | loss  3.37 | ppl    29.17\n",
            "| epoch   1 | 378400/389982 batches | lr 5.00 | ms/batch 67.80 | loss  3.40 | ppl    30.03\n",
            "| epoch   1 | 378600/389982 batches | lr 5.00 | ms/batch 67.19 | loss  3.46 | ppl    31.87\n",
            "| epoch   1 | 378800/389982 batches | lr 5.00 | ms/batch 68.05 | loss  3.42 | ppl    30.55\n",
            "| epoch   1 | 379000/389982 batches | lr 5.00 | ms/batch 65.97 | loss  3.51 | ppl    33.30\n",
            "| epoch   1 | 379200/389982 batches | lr 5.00 | ms/batch 68.92 | loss  3.39 | ppl    29.62\n",
            "| epoch   1 | 379400/389982 batches | lr 5.00 | ms/batch 67.32 | loss  3.61 | ppl    36.83\n",
            "| epoch   1 | 379600/389982 batches | lr 5.00 | ms/batch 66.25 | loss  3.46 | ppl    31.88\n",
            "| epoch   1 | 379800/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.36 | ppl    28.65\n",
            "| epoch   1 | 380000/389982 batches | lr 5.00 | ms/batch 66.98 | loss  3.54 | ppl    34.54\n",
            "| epoch   1 | 380200/389982 batches | lr 5.00 | ms/batch 71.62 | loss  3.46 | ppl    31.73\n",
            "| epoch   1 | 380400/389982 batches | lr 5.00 | ms/batch 68.42 | loss  3.37 | ppl    29.07\n",
            "| epoch   1 | 380600/389982 batches | lr 5.00 | ms/batch 67.34 | loss  3.41 | ppl    30.39\n",
            "| epoch   1 | 380800/389982 batches | lr 5.00 | ms/batch 69.03 | loss  3.38 | ppl    29.46\n",
            "| epoch   1 | 381000/389982 batches | lr 5.00 | ms/batch 67.66 | loss  3.42 | ppl    30.66\n",
            "| epoch   1 | 381200/389982 batches | lr 5.00 | ms/batch 67.46 | loss  3.46 | ppl    31.87\n",
            "| epoch   1 | 381400/389982 batches | lr 5.00 | ms/batch 68.22 | loss  3.41 | ppl    30.27\n",
            "| epoch   1 | 381600/389982 batches | lr 5.00 | ms/batch 68.58 | loss  3.38 | ppl    29.52\n",
            "| epoch   1 | 381800/389982 batches | lr 5.00 | ms/batch 67.42 | loss  3.45 | ppl    31.38\n",
            "| epoch   1 | 382000/389982 batches | lr 5.00 | ms/batch 68.71 | loss  3.41 | ppl    30.17\n",
            "| epoch   1 | 382200/389982 batches | lr 5.00 | ms/batch 71.93 | loss  3.47 | ppl    32.25\n",
            "| epoch   1 | 382400/389982 batches | lr 5.00 | ms/batch 67.26 | loss  3.43 | ppl    31.01\n",
            "| epoch   1 | 382600/389982 batches | lr 5.00 | ms/batch 69.35 | loss  3.51 | ppl    33.30\n",
            "| epoch   1 | 382800/389982 batches | lr 5.00 | ms/batch 68.42 | loss  3.43 | ppl    30.79\n",
            "| epoch   1 | 383000/389982 batches | lr 5.00 | ms/batch 69.40 | loss  3.35 | ppl    28.45\n",
            "| epoch   1 | 383200/389982 batches | lr 5.00 | ms/batch 66.19 | loss  3.47 | ppl    32.14\n",
            "| epoch   1 | 383400/389982 batches | lr 5.00 | ms/batch 68.89 | loss  3.38 | ppl    29.25\n",
            "| epoch   1 | 383600/389982 batches | lr 5.00 | ms/batch 67.62 | loss  3.43 | ppl    30.90\n",
            "| epoch   1 | 383800/389982 batches | lr 5.00 | ms/batch 67.31 | loss  3.42 | ppl    30.45\n",
            "| epoch   1 | 384000/389982 batches | lr 5.00 | ms/batch 67.69 | loss  3.44 | ppl    31.27\n",
            "| epoch   1 | 384200/389982 batches | lr 5.00 | ms/batch 72.63 | loss  3.39 | ppl    29.73\n",
            "| epoch   1 | 384400/389982 batches | lr 5.00 | ms/batch 66.89 | loss  3.46 | ppl    31.88\n",
            "| epoch   1 | 384600/389982 batches | lr 5.00 | ms/batch 67.16 | loss  3.61 | ppl    37.05\n",
            "| epoch   1 | 384800/389982 batches | lr 5.00 | ms/batch 68.00 | loss  3.43 | ppl    30.93\n",
            "| epoch   1 | 385000/389982 batches | lr 5.00 | ms/batch 68.56 | loss  3.48 | ppl    32.54\n",
            "| epoch   1 | 385200/389982 batches | lr 5.00 | ms/batch 67.40 | loss  3.41 | ppl    30.36\n",
            "| epoch   1 | 385400/389982 batches | lr 5.00 | ms/batch 68.38 | loss  3.38 | ppl    29.43\n",
            "| epoch   1 | 385600/389982 batches | lr 5.00 | ms/batch 68.20 | loss  3.35 | ppl    28.60\n",
            "| epoch   1 | 385800/389982 batches | lr 5.00 | ms/batch 67.85 | loss  3.46 | ppl    31.77\n",
            "| epoch   1 | 386000/389982 batches | lr 5.00 | ms/batch 66.57 | loss  3.50 | ppl    33.24\n",
            "| epoch   1 | 386200/389982 batches | lr 5.00 | ms/batch 73.03 | loss  3.46 | ppl    31.89\n",
            "| epoch   1 | 386400/389982 batches | lr 5.00 | ms/batch 68.86 | loss  3.42 | ppl    30.55\n",
            "| epoch   1 | 386600/389982 batches | lr 5.00 | ms/batch 67.81 | loss  3.43 | ppl    30.76\n",
            "| epoch   1 | 386800/389982 batches | lr 5.00 | ms/batch 68.26 | loss  3.38 | ppl    29.40\n",
            "| epoch   1 | 387000/389982 batches | lr 5.00 | ms/batch 66.92 | loss  3.45 | ppl    31.44\n",
            "| epoch   1 | 387200/389982 batches | lr 5.00 | ms/batch 68.37 | loss  3.43 | ppl    30.78\n",
            "| epoch   1 | 387400/389982 batches | lr 5.00 | ms/batch 67.33 | loss  3.44 | ppl    31.08\n",
            "| epoch   1 | 387600/389982 batches | lr 5.00 | ms/batch 67.71 | loss  3.52 | ppl    33.74\n",
            "| epoch   1 | 387800/389982 batches | lr 5.00 | ms/batch 66.83 | loss  3.49 | ppl    32.62\n",
            "| epoch   1 | 388000/389982 batches | lr 5.00 | ms/batch 67.44 | loss  3.41 | ppl    30.36\n",
            "| epoch   1 | 388200/389982 batches | lr 5.00 | ms/batch 73.20 | loss  3.47 | ppl    32.11\n",
            "| epoch   1 | 388400/389982 batches | lr 5.00 | ms/batch 66.52 | loss  3.52 | ppl    33.64\n",
            "| epoch   1 | 388600/389982 batches | lr 5.00 | ms/batch 68.77 | loss  3.40 | ppl    30.11\n",
            "| epoch   1 | 388800/389982 batches | lr 5.00 | ms/batch 67.63 | loss  3.46 | ppl    31.90\n",
            "| epoch   1 | 389000/389982 batches | lr 5.00 | ms/batch 67.90 | loss  3.42 | ppl    30.62\n",
            "| epoch   1 | 389200/389982 batches | lr 5.00 | ms/batch 67.18 | loss  3.46 | ppl    31.90\n",
            "| epoch   1 | 389400/389982 batches | lr 5.00 | ms/batch 68.29 | loss  3.39 | ppl    29.52\n",
            "| epoch   1 | 389600/389982 batches | lr 5.00 | ms/batch 66.24 | loss  3.48 | ppl    32.57\n",
            "| epoch   1 | 389800/389982 batches | lr 5.00 | ms/batch 66.82 | loss  3.52 | ppl    33.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 29561.71s | valid loss  3.47 | valid ppl    32.09\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTU1EsrOeYA_"
      },
      "source": [
        "Evaluate the best model on the test dataset\n",
        "-------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtLvVpeleYA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c594c98f-5f01-449a-8e28-3e3374a82a31"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  3.47 | test ppl    32.15\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GktJJN0RmIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40274283-05e1-4b10-9bce-553478774a4c"
      },
      "source": [
        "#torch.save(best_model.state_dict(), \"/content/drive/MyDrive/Project2/transformer3.pt\")\n",
        "#best_model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "#best_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Project2/transformer2.pt\", map_location=torch.device(device)))\n",
        "sentence = [\"left of the political spectrum [MASK]\"]\n",
        "src_mask = generate_square_subsequent_mask(1).to(device)\n",
        "sentence_data = tokenizer(\n",
        "                      sentence,\n",
        "                      padding=True, \n",
        "                      truncation=True,\n",
        "                      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "                      return_token_type_ids=False,\n",
        "                      return_attention_mask=False,\n",
        "                      return_tensors='pt',  # Return PyTorch tensors\n",
        "                    )[\"input_ids\"].to(device)\n",
        "best_model.eval()\n",
        "with torch.no_grad():\n",
        "  print(\"input size\", sentence_data.shape)\n",
        "  output = best_model(sentence_data, src_mask)\n",
        "  output_flat = output.view(-1, ntokens)\n",
        "  print(\"output size\", output.shape)\n",
        "  print(output)\n",
        "  print(\"output flat size\", output_flat.shape)\n",
        "  print(output_flat)\n",
        "  print(nn.Softmax(dim=1)(output_flat))\n",
        "  result_index = torch.argmax(nn.Softmax(dim=1)(output_flat), dim=1)\n",
        "  print(result_index)\n",
        "  print(sentence)\n",
        "  print(tokenizer.convert_ids_to_tokens(sentence_data[0]))\n",
        "  print(tokenizer.convert_ids_to_tokens(result_index))\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([1, 8])\n",
            "output size torch.Size([1, 8, 30592])\n",
            "tensor([[[ 9.2725, -0.3353, -0.7416,  ..., -1.4121, -1.6973, -0.5973],\n",
            "         [ 5.1012, -1.2107, -1.6178,  ..., -2.3843, -2.1508, -1.4338],\n",
            "         [ 6.2856, -2.0802, -2.0036,  ..., -2.0090, -2.3015, -2.2718],\n",
            "         ...,\n",
            "         [ 9.6759, -1.5315, -1.5371,  ..., -1.6680, -2.2948, -1.4584],\n",
            "         [ 7.5608, -1.7483, -1.7033,  ..., -2.0602, -1.9165, -1.7665],\n",
            "         [14.6608, -0.6735, -1.0620,  ..., -0.8586, -0.9394, -1.2344]]],\n",
            "       device='cuda:0')\n",
            "output flat size torch.Size([8, 30592])\n",
            "tensor([[ 9.2725, -0.3353, -0.7416,  ..., -1.4121, -1.6973, -0.5973],\n",
            "        [ 5.1012, -1.2107, -1.6178,  ..., -2.3843, -2.1508, -1.4338],\n",
            "        [ 6.2856, -2.0802, -2.0036,  ..., -2.0090, -2.3015, -2.2718],\n",
            "        ...,\n",
            "        [ 9.6759, -1.5315, -1.5371,  ..., -1.6680, -2.2948, -1.4584],\n",
            "        [ 7.5608, -1.7483, -1.7033,  ..., -2.0602, -1.9165, -1.7665],\n",
            "        [14.6608, -0.6735, -1.0620,  ..., -0.8586, -0.9394, -1.2344]],\n",
            "       device='cuda:0')\n",
            "tensor([[6.8183e-11, 4.5820e-15, 3.0519e-15,  ..., 1.5610e-15, 1.1736e-15,\n",
            "         3.5259e-15],\n",
            "        [2.6766e-04, 4.8570e-07, 3.2324e-07,  ..., 1.5019e-07, 1.8970e-07,\n",
            "         3.8858e-07],\n",
            "        [1.6251e-03, 3.7815e-07, 4.0826e-07,  ..., 4.0606e-07, 3.0311e-07,\n",
            "         3.1222e-07],\n",
            "        ...,\n",
            "        [3.6981e-02, 5.0196e-07, 4.9918e-07,  ..., 4.3794e-07, 2.3399e-07,\n",
            "         5.4007e-07],\n",
            "        [5.0409e-03, 4.5670e-07, 4.7770e-07,  ..., 3.3434e-07, 3.8598e-07,\n",
            "         4.4843e-07],\n",
            "        [7.6817e-01, 1.6822e-07, 1.1406e-07,  ..., 1.3980e-07, 1.2894e-07,\n",
            "         9.6000e-08]], device='cuda:0')\n",
            "tensor([ 101, 1996, 1996, 1000, 2283, 1012, 2015,    0], device='cuda:0')\n",
            "['left of the political spectrum [MASK]']\n",
            "['[CLS]', 'left', 'of', 'the', 'political', 'spectrum', '[MASK]', '[SEP]']\n",
            "['[CLS]', 'the', 'the', '\"', 'party', '.', '##s', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uu03EDAC1SY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8eb1128-c6d7-44a8-92ea-828179c4aa14"
      },
      "source": [
        "lista = []\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "for item in train_iter:\n",
        "  lista.append(item)\n",
        "print(lista[0:5])\n",
        "\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "print(train_data.size())\n",
        "train_data = batchify(train_data, batch_size)\n",
        "print(train_data.size())\n",
        "print(vocab.lookup_tokens(train_data[0].numpy()))\n",
        "bptt = 35\n",
        "num_batches = len(train_data) // bptt\n",
        "for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "  data, targets = get_batch(train_data, i)\n",
        "  print(data.shape, targets.shape)\n",
        "  print(vocab.lookup_tokens(data[0].numpy()))\n",
        "  print(vocab.lookup_tokens(targets.numpy()[0:20]))\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' \\n', ' = Valkyria Chronicles III = \\n', ' \\n', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]\n",
            "torch.Size([2049990])\n",
            "torch.Size([102499, 20])\n",
            "['=', 'all', 'party', 'major', 'modern', 'polygamist', ',', 'present', 'arrangement', 'trying', '<unk>', 'military', 'offering', 'the', 'and', 'or', 'while', 'escaping', 'receive', 'the']\n",
            "torch.Size([35, 20]) torch.Size([700])\n",
            "['=', 'all', 'party', 'major', 'modern', 'polygamist', ',', 'present', 'arrangement', 'trying', '<unk>', 'military', 'offering', 'the', 'and', 'or', 'while', 'escaping', 'receive', 'the']\n",
            "['valkyria', '@-@', 'led', 'intersections', 'celtic', 'families', 'kramer', 'building', 'of', 'to', ',', 'authorities', 'odds', 'score', 'viewed', 'killed', 'the', 'or', 'an', 'truth']\n"
          ]
        }
      ]
    }
  ]
}